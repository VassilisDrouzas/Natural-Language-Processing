\documentclass[11pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[greek, english]{babel}
\usepackage{alphabeta}
\usepackage{libertine}
\usepackage{graphicx}
\usepackage[backend=biber]{biblatex}
\usepackage[table]{xcolor}
\usepackage{mathptmx} % Times New Roman
\usepackage{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{subcaption} %for subfigures
\usepackage{hyperref}
\usepackage{amsmath}


\hypersetup{
	colorlinks=true,
	linkcolor=black,
	filecolor=magenta,      
	urlcolor=blue
}


\pagenumbering{arabic}
\graphicspath{ {./output/}{./images/} }
\addbibresource{refs.bib}

\def\code#1{\texttt{#1}}

\title{\Huge Text Analytics: 2nd Assignment}

\author{\LARGE Tsirmpas Dimitris\\\LARGE Drouzas Vasilis}


\begin{document}
	
	\begin{titlepage}
		\maketitle
		\begin{center}
			
			\large Athens University of Economics and Business
			
			\large MSc in Data Science
			
		\end{center}
		
	\end{titlepage}
	
	\tableofcontents
	\newpage	
	
	\section{Introduction}
	
	This report will briefly discuss the theoretical background, implementation details and decisions taken for the construction of MLP models for sentiment analysis and POS tagging tasks.
	
	This report and its associated code, analysis and results were conducted by the two authors. Specifically, the sentiment analysis task was performed by Drouzas Vasilis, and the POS-tagging task by Tsirmpas Dimitris. This report was written by both authors.
		
	Note that due to the relative custom code complexity, most of the code used in this section was developed and imported from python source files located in the `tasks` module. In-depth documentation and implementation details can be found in these files. 
	
	
	\section{POS Tagging}
	
	POS tagging is a language processing task where words in a given text are assigned specific grammatical categories, such as nouns, verbs, or adjectives. The objective is to analyze sentence structure. 
	
	In this section we describe how we can leverage pre-trained word embeddings to create a context-aware MLP classifier.
	
	
	\subsection{Dataset}
	
	Acquiring and preprocessing our data with the goal of eventually acquiring a sufficient representation of our text is the most difficult and time-consuming task. We thus split it in distinct phases:
	
	\begin{itemize}
		\item Original dataset acquisition and parsing
		\item Qualitative analysis and preprocessing
		\item Transformation necessary for the NLP task
	\end{itemize}
	
	Each of these distinct steps are individually analyzed below.
	
	
	\subsubsection{Acquisition}
	
	We select the \href{https://universaldependencies.org/treebanks/en_ewt/index.html}{English EWT-UD} tree, which is the largest currently supported collection for POS tagging tasks for the English language.
	
	This corpus contains 16622 sentences, 251492 tokens and 254820 syntactic words, as well as 926 types of words that contain both letters and punctuation, such as 's, n't, e-mail, Mr., â€™s, etc). This is markedly a much higher occurrence than its siblings, and therefore may lead to a slightly more difficult task.
	
	The dataset is made available in \texttt{conllu} format, which we parse using the recommended \texttt{conllu} python library. We create a dataframe for every word and its corresponding POS tag and link words belonging to the same sentences by a unique sentence ID. The data are already split to training, validation and test sets, thus our own sets correspond to the respective split files.
	
	We are interested in the UPOS (Universal Part of Speech) tags for English words.
	
	\subsubsection{Qualitative Analysis}
	
	Our training vocabulary is comprised of $16654$ words. We include qualitative statistics on the sentences of our dataset in Tables \ref{tab::ex-10-sent-stats} and \ref{tab::ex-10-stats}. The splits are explicitly mentioned separately because the splitting was performed by the dataset authors and not by random sampling. We would therefore like to confirm at a glance whether their data are similar.
	
	\begin{table}
		\begin{tabular}{|l|l|l|l|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{Set} & \cellcolor{blue!25}\textbf{Mean} & \cellcolor{blue!25}\textbf{Std} &
			\cellcolor{blue!25}\textbf{Min} & \cellcolor{blue!25}\textbf{25\%} & \cellcolor{blue!25}\textbf{50\%} &
			\cellcolor{blue!25}\textbf{75\%} &
			\cellcolor{blue!25}\textbf{Max} \\
			\hline
			Training & 16.31 & 12.4 & 1  & 7 & 14 & 23 & 159 \\\hline
			Validation  & 12.56 & 10.41 & 1 & 5 & 10 & 17 & 75  \\\hline
			Test & 12.08 & 10.6  & 1 & 4 & 9 & 17 & 81
			\\\hline
		\end{tabular}
	\centering
	\caption{Summary and order statistics for the number of words in the sentences of each data split.}
	\label{tab::ex-10-sent-stats}
	\end{table}

	\begin{table}
		\begin{tabular}{|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{Set} & \cellcolor{blue!25}\textbf{Total Word Count} &
			\cellcolor{blue!25}\textbf{Total Sentence Count}\\
			\hline
			Training & 204614 & 12544 \\\hline
			Validation & 25152  & 2001 \\\hline
			Test & 25096 & 2077 \\\hline
		\end{tabular}
		\centering
		\caption{Total text volume of each data split.}
		\label{tab::ex-10-stats}
	\end{table}
	
	
	\subsubsection{Preprocessing}
	
	Given the nature of our task we can not implement preprocessing steps such as removing punctuation marks, stopwords or augmenting the dataset. Thus, the only meaningful preprocessing at this stage would be converting the words to lowercase. We believe that the context of each word will carry enough information to distinguish its POS tag regardless of case. 
	
	Another issue we need to address before continuing is that of words being part of (depending on) other words for their POS tag. Those would be words such as "don't", "couldn't" or "you're". In the standard UPOS schema these are defined as two or more separate words, where the first is represented by its standard POS tag, and the rest as part of that tag (UPOS tag "PART"). For instance, "don't" would be split into "do" and "n't" with "AUX" and "PART" tags respectively. In our dataset, these words are represented both in the manner described above followed by the full word ("don't") tagged with the pseudo-tag "\_". We remove the latter representation from the working dataset.
	
	The general algorithm to calculate the window embeddings on our dataset can be found in Algorithm \ref{al::window-embeds}. The algorithm uses a few external functions which are not described here for the sake of brevity. \texttt{get\_window()} returns the context of the word inside a sentence, including padding where needed, \texttt{embedding()} returns the word embedding for a single word and \texttt{concatenate} returns a single element from a list of elements. The rest of the functions should be self-explanatory.  Note that this algorithm does not represent the actual python implementation.
	
	\begin{algorithm}
		\caption{Window Embedding creation algorithm from raw-text sentences.} 
		\label{al::window-embeds}
		
		\hspace*{\algorithmicindent} \textbf{Input} sentences, window\_lim: a list of sentences and an upper bound of windows to be computed\\
		\hspace*{\algorithmicindent} \textbf{Output} tuple(windows, targets): the window embeddings and the POS tag corresponding to the median word of each window
		\begin{algorithmic}[1]	
			\State windows = list()
			\State targets = list()
			
			\State % whitespace
			
			\For {sentence in sentences}
			
				\For {word in sentence}
					\State window = get\_window(word, sentence)
					\State target = get\_tag(word)
					
					\State windows.add(window)
					\State targets.add(target)
				\EndFor
			
			\EndFor
			
			\State % whitespace
			
			\State window\_embeddings = list()
			\For{window in windows}
			
				\If {window\_embeddings.size $\geq$ window\_lim}
					\State break
				\EndIf
				
				\State % whitespace
				
				\State word\_embeddings = list()
				
				
				\For{word in window}
				
					\If{word is PAD\_TOKEN}
						\State word\_embeddings.add(zeros(embedding\_size))
					\Else
						\State word\_embeddings.add(embedding(word))
					\EndIf
					
				\EndFor
					
				\State window\_embedding = concatenate(word\_embeddings)
				\State window\_embeddings.add(window\_embedding)
			\EndFor
			
			\State % whitespace
			
			\State targets\_vec = list()
			\For{target in targets}
				\State targets\_vec.add(one\_hot(target))
			\EndFor
			
			\State % whitespace
			
			\State \Return window\_embeddings, targets\_vec
			
		\end{algorithmic} 
	\end{algorithm}


	\subsection{Baseline Classifier}
	
	We create our own classifier which classifies each token by the majority label associated with it. The classifier is defined as a subclass of sklearn's classifier superclass and thus can seamlessly use it in most sklearn-provided functions such as \texttt{classification\_report()} and its implementation can be found in the \texttt{tasks.models} module.
	
	The results of the classifier can be found in Figure TODO. These results make intuitive sense, since most words in the English language can be classified in a single label, irrespective of context. For example, "is" will always be classified as "AUX", and all punctuation marks will be classified as "PUNCT".
	
	Thus, besides quantitative statistics such as categorical accuracy and f1-score, we should pay close attention to the precision and recall statistics for the more variable POS tags such as "NOUN" or "VERB" in order to properly evaluate our MLP classifier.
	

	\subsection{MLP Classifier}
	
	\subsubsection{Hyper-parameter tuning}
	
	We use the \texttt{keras\_tuner} library to automatically perform random search over various hyper-parameters of our model.
	
	The parameter search consists of:
	\begin{itemize}
		\item The depth of the model (the number of layers)
		\item The height of the model (the number of parameters by layer) 
		\item The learning rate
	\end{itemize}

	
	The parameter search does NOT consist of:
	\begin{itemize}
		\item Dropout rate, since dropout rarely changes the final result of a neural network, but rather tunes the trade-off between training time and overfit avoidance
		\item Activation functions, since they rarely significantly influence the model's performance
	\end{itemize}
	
	With this scheme we hope to maximize the area and granularity of our search to the hyper-parameters that are most likely to significantly influence the final results.
	
	We implement early stopping and set a maximum iteration limit of $70$. We assume that if a model needs to go over that limit, it may be computationally inefficient, and thus less desirable compared to a slightly worse, but much more efficient model. Additionally, we use a relatively large batch size to improve training times since this operation is very computationally heavy. We don not yet aim to create the best classifier, so slightly suboptimal weights are not a problem for the purposes of the hyperparameter search.
	We use a relatively very large batch size to improve training times since this operation is very computationally heavy. We don't yet aim to create the best classifier, so slightly suboptimal weights are not a problem for the purposes of the hyperparameter search.
	
	
	\subsubsection{Training}
		
	We now re-train our model with a much smaller batch size and keep track of the training history and best weights by validation loss. 
	
	Unfortunately, the different batch size means we can not rely on the hyper parameter search to get an estimation of training epochs. Thus, we rely on early-stopping on the validation data to ensure our model does not overfit as a result of training time. 
	
	We use the categorical accuracy stopping criterion instead of loss. This may lead to situations where validation loss increases, but so does accuracy \cite{loss-accuracy}. This represents a trade-off between our model being more confidently incorrect about already-misclassified instances, but better at edge cases where the classification is more ambiguous. We previously discussed how the strength of a context-aware classifier lies in these kinds of distinctions, which justifies our choice of favoring correct edge-case classifications in the expense of more confidently incorrect misclassifications.
	
	This phenomenon is demonstrated in Figure \ref{fig::ex_10_fit}.
	
	\begin{figure}
		\centering
		\includegraphics[width=10cm]{"ex_10_fit.png"}
		\caption{Loss and accuracy on the training and validation sets depending on the number of epochs.}
		\label{fig::ex_10_fit}
	\end{figure}
	
	\subsubsection{Results}
	
	The results of our MLP classifier compared to the baseline model          mentioned above can be found in Tables \ref{tab::ex_10_train}, \    \ref{tab::ex_10_valid} and \ref{tab::ex_10_test}. We include precision, recall and F1 scores for each individual tag, as well as their macro average denoted by the "MACRO" tag in the tables.
	
	We note an increase in all metrics for our MLP classifier, especially in tags such as "SCONJ" (subordinating conjunction), and VERB and NOUN (which we hypothesized at the beginning of the report). The only notable exception is the "X" tag (other) which is attributed to unintelligible material, foreign words and word fragments. This is an acceptable drawback, since these are easily caught by preprocessing or weak classifiers anyway.

	\input{output/ex_10_train.tex}
	
	\input{output/ex_10_valid.tex}
	
	\input{output/ex_10_test.tex}



        \section{ Sentiment Analysis}
        
    Sentiment analysis, also known as opinion mining, is the process of analyzing text to determine the sentiment or emotional tone expressed within it. The goal of sentiment analysis is to understand the attitudes, opinions, and emotions conveyed by the text. 

    \subsection{Dataset}
    Here we will be working with the Cornell Movie Review dataset, which consists of 2000 movie reviews, split equally in 1000 positive and 1000 negative ones. The goal here will be to develop classifiers that will effectively understand whether a review is a positive or negative one, based on the data it has been trained on.We begin by taking a brief look into our dataset.
    \subsubsection{Average Document Length (Before pre-processing)}

    The average document length in words and characters before the processing can be checked in Figure \ref{fig::len_before_preprocess}.

        \begin{figure}
	    \centering
	    \includegraphics[width=11cm]{"lengths_before_preprocessing".png}
	    \caption{Average Document length in words and characters (before the pre-processing).}
	    \label{fig::len_before_preprocess}
	\end{figure}


    \subsubsection{Pre-processing}
    
     For demonstration reasons, we start by printing the 20 most frequent words in the text, in Figure \ref{fig::20_common}.

        \begin{figure}
	    \centering
            \includegraphics[width=4cm]{"20_most_common_words".png}
	    \caption{Average Document length in words and characters (before the pre-processing).}
	    \label{fig::20_common}
	\end{figure}



     Most of these words are actually stop words. As in most text classification problems, we would typically need to remove the stop words of the text.
     
     The english stopwords is a package of 179 words that in general, would not help in a sentiment analysis problem. But, since they include terms that are negative, removing them could prove harmful for our case, since we are dealing with a sentiment analysis problem.

    e.g. imagine the phrase "I didn't like the film" to end up "like film". Disastrous, right?
    
    So, the plan is to remove all the stop words that include negative meaning before the preprocessing.
    The stop words that we decided to keep in the text are shown in Figure \ref{fig::to_keep} :

    \begin{figure}
	    \centering
            \includegraphics[width=4cm]{"to_keep_words".png}
	    \caption{Average Document length in words and characters (before the pre-processing).}
	    \label{fig::to_keep}
    \end{figure}

    Moving on to the pre-processing task, the steps performed are the following:
    \begin{itemize}
        \item{ Combination to a single document.}
        \item{ Convertion to lowercase.}
        \item{Lemmatization and stop words extraction.}
        \item{ Punctuation removal.}
        \item{ Number removal.}
         \item{Single characters removal.}
        \item{ Converting multiple spaces to single ones.}
    \end{itemize}

    \subsubsection{Average Document Length (after pre-processing)}
    The final average document length is shown in Figure \ref{fig::after_preprocess}.

    \begin{figure}
	    \centering
            \includegraphics[width=11cm]{"lengths_after_preprocessing".png}
	    \caption{Average Document length in words and characters (before the pre-processing).}
	    \label{fig::after_preprocess}
    \end{figure}

    \subsubsection{Splitting the dataset}
    We decided to split the (processed) dataset into the training set (70\%), development set (15\%) and test set (15\%). The sizes of each set are shown in Table \ref{tab::ex-9-stats}.

    \begin{table}
		\begin{tabular}{|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{Set} & \cellcolor{blue!25}\textbf{Total Word Count} &
			\cellcolor{blue!25}\textbf{Total Document Count}\\
			\hline
			Training & 36624 & 1400 \\\hline
			Validation & 16948  & 300 \\\hline
			Test & 16780 & 300 \\\hline
		\end{tabular}
		\centering
		\caption{Total text volume of each data split.}
		\label{tab::ex-9-stats}
	\end{table}
    

    \subsubsection{TF-IDF}
    We used the unigram and bi-gram TF-IDF features, defining the maximum number of features to 5000. The shapes of the data are shown in Figure \ref{fig::tf_idf}.

    \begin{figure}
	    \centering
            \includegraphics[width=11cm]{"shape_tfidf".png}
	    \caption{The shapes of the TF-IDF vectors.}
	    \label{fig::tf_idf}
    \end{figure}

    \subsubsection{Feature selection with SVD}
    We performed dimensionality reduction with the TruncatedSVD() method, reducing the number of features from 5000 to 500. The new shapes can be found in Figure \ref{fig::svd}.

    \begin{figure}
	    \centering
            \includegraphics[width=11cm]{"shape_after_svd".png}
	    \caption{The new shapes after doing SVD analysis.}
	    \label{fig::svd}
    \end{figure}

    \subsection{Classifiers}
    \subsubsection{DummyClassifier}
    DummyClassifier makes predictions that ignore the input features. This classifier serves as a simple baseline to compare against other more complex classifiers.The strategy to generate predictions was set to â€˜most\_frequentâ€™,  meaning that the predict method always returns the most frequent class label. The results of this classifier are demonstrated in Figure \ref{fig::dummy_metrics}.

    \begin{figure}
	    \centering
            \includegraphics[width=8cm]{"DummyClf_metrics".png}
	    \caption{Classification results of DummyClassifier for training,test and validation sets.}
	    \label{fig::dummy_metrics}
    \end{figure}


    As expected, the results are poor since the decision of the classifier depends exclusively only the majority class.

    \subsubsection{Logistic Regression}
    Logistic Regression is a statistical method used for binary classification tasks, where the output variable takes only two possible outcomes.
    Before applying Logistic Regression, we will perform a grid search to find the optimal parameters to run the classifier. The parameters we tried are the following:
    \begin{itemize}
    \item{Solver: We tested â€˜liblinearâ€™ and â€˜sagaâ€™ solvers}
    \item{Penalty: We tested â€˜l1â€™, â€˜l2â€™, â€˜elasticnetâ€™ reguralization penalties}
    \item{C: We tested values of 0.001, 0.01, 0.1, 1 and 10 (inverse of regularization strength)}
    \end{itemize}

    The best hyperparameters were the following:
    C= 10, penalty= â€˜l1â€™, solver = â€˜liblinearâ€™.
    
    \ 

    \ 
    Now, it is time to fit the Logistic Regression using these parameters. The results we got are shown in Figure \ref{fig::LR_metrics} .

    \begin{figure}
	    \centering
            \includegraphics[width=8cm]{"LRClf_metrics".png}
	    \caption{Metrics of the Logistic Regression on the training, test and development sets.}
	    \label{fig::LR_metrics}
     \end{figure}


    \subsection{Our custom MLP classifier}

    First of all, we define the y\_train\_1\_hot and y\_dev\_1\_hot vectors using the LabelBinarizer and applying fit\_transform() and transform() to the training and development 1-hot vectors respectively.
    Now, itâ€™s time to define our MLP model. We followed the same method as before, i.e. we used the keras\_tuner library to perform random search with parameters the depth of the model, the height and the learning rate.
    Early stopping is implemented , with main criterion the validation loss. The number of epochs was defined to 100. We experimented with a variety of different hyperparameter combinations. The first experiments examined the quantity of the hidden layers, keeping other parameters constant. The results tended to optimize using only one hidden layer, meaning that the problem can be solved with a small network. After defining the number of hidden layers to one, we examined other hyperparameter combinations:

  
    \begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{Model} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6*} & \textbf{7*} \\
    \hline
    \#Hidden layers & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
    \hline
    Batch size & 256 & 256 & 256 & 128 & 512 & 256 & 256 \\
    \hline
    Min\_hid\_size & 64 & 32 & 16 & 32 & 32 & 32 & 32 \\
    \hline
    Max\_hid\_size & 1024 & 512 & 206 & 512 & 512 & 512 & 512 \\
    \hline
    
    
    
    \end{tabular}
    \end{center}

    \begin{itemize}
        \item{Model 1: Good loss, mediocre accuracy.}
        \item{Model 2: Optimal loss \& accuracy.}
        \item{Model 3: Worse results vs Model (2) in terms of loss.}
        \item{Model 4: Data tend to overfit.}
        \item{Model 5: Worse results compared to Model (2) in both accuracy,loss.}
        \item{Model 6 (*removing the subsequent layer): Similar results to model (2).}
        \item{Model 7(*Learning rate set to 1e-5): Worse results compared to model(2).}


    \end{itemize}
    The model we chose to keep is Model 2, which seems to be the optimal one after testing the hyperparameters. The results we gain are shown in Figures \ref{fig::mlp_accuracy}, \ref{fig::mlp_loss}.

    \begin{figure}
	    \centering
            \includegraphics[width=11cm]{"MLP_accuracy".png}
	    \caption{MLP accuracy as a function of epochs.}
	    \label{fig::mlp_accuracy}
    \end{figure}

    \begin{figure}
	    \centering
            \includegraphics[width=11cm]{"MLP_loss".png}
	    \caption{MLP loss as a function of epochs.}
	    \label{fig::mlp_loss}
	\end{figure}



    At a first glance, the curves showing the model accuracy do seem a bit weird. Specifically, we would more like expect the training accuracy to start at a lower rate and increase over time. But what we see is that it actually starts already pretty high (96.71\%) and ends up slightly higher (98.36\%). It means our model very accurately predicts the training data even from the beginning.

    
    As far as the validation data are concerned, we see that no significant improvements are made while the number of epochs increases.

    
    In general, the binary accuracies for both training and validation sets are relatively high and show similar trends of improvement over epochs. Additionally, there is no significant gap between the training and validation accuracies. This suggests that the model is not overfitting to the training data. 

    Our prediction is that the data are already quite simple to distinguish, so the parameters are already too many to learn to distinguish. This is why the development accuracy does not show actually an improvement over time. 
    Now regarding the training/development losses, here we see that both training and validation losses decrease together, which suggests that the model is generalizing well.

    Next, we will provide the metrics (Precision,Recall, F1 score and the AUC scores) for training, development and test subsets in Figure \ref{fig::mlp_metrics}.

    \begin{figure}
	    \centering
            \includegraphics[width=11cm]{"MLP_metrics".png}
	    \caption{Metrics for the MLP classifier for both classes for the training, development and test sets.}
	    \label{fig::mlp_metrics}
    \end{figure}

    



    Finally, letâ€™s take a look at the Macro-averaged metrics (averaging the corresponding scores of the previous bullet over the classes) for the training, development and test subsets, as shown in Figure \ref{fig::mlp_macro_metrics}.

    \begin{figure}
	    \centering
            \includegraphics[width=4cm]{"MLP_Macro_metrics".png}
	    \caption{Macro-Metrics for the MLP classifier for both classes for the training, development and test sets.}
	    \label{fig::mlp_macro_metrics}
    \end{figure}

    

	\printbibliography
	
\end{document}