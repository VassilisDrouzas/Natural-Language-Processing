{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6543a2dd-779a-4643-a6bd-ffa5c1ac4f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Drogias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Drogias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6abedf4-87d7-4d30-b058-2a4a4f542aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.spell_correction import BigramSpellCorrector, TrigramSpellCorrector\n",
    "from src.autocomplete import BigramModel, START_TOKEN, END_TOKEN, UNKNOWN_TOKEN, TrigramModel, BaseNgramModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b74da53-b6ee-490f-8ebc-6256aea039b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg_corpus = nltk.corpus.gutenberg.fileids()                                 #Get all the files\n",
    "gutenberg_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b16673-a350-459d-8fd3-0c3c990d9c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died t\n"
     ]
    }
   ],
   "source": [
    "combined_text = \"\"             \n",
    "for file_id in gutenberg_corpus:                                        # Combine the text from all files\n",
    "    combined_text += nltk.corpus.gutenberg.raw(file_id)\n",
    "\n",
    "print(combined_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9cd0930-d3ab-4bbc-82e8-71de27c63468",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = combined_text.lower()                              #Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c6b392-464d-4c08-b16b-acaabbea6e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[emma by jane austen 1816]\\n\\nvolume i\\n\\nchapter i\\n\\n\\nemma woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.\\n\\nshe was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.  her mother\\nhad died t\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e4f3c82-7a8e-42d5-b04b-4160fe689891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_chars(text):\n",
    "   text = text.replace('[', '')\n",
    "   text = text.replace(']', '')\n",
    "   text = text.replace('\\n', ' ')\n",
    "   text = re.sub(r'[^a-zA-z.?!\\']', ' ', text)                     #Remove these characters   \n",
    "\n",
    "   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb081710-1d7d-4c53-b85d-acd0a9528c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"emma by jane austen       volume i  chapter i   emma woodhouse  handsome  clever  and rich  with a comfortable home and happy disposition  seemed to unite some of the best blessings of existence  and had lived nearly twenty one years in the world with very little to distress or vex her.  she was the youngest of the two daughters of a most affectionate  indulgent father  and had  in consequence of her sister's marriage  been mistress of his house from a very early period.  her mother had died too\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_text = remove_special_chars(combined_text)\n",
    "combined_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f25b00-f68a-4ad1-893d-f4e27e93698f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2119883"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_text.split())                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7fcb110-bdda-41dc-8850-fae413272efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11793056"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_text)                             # How many characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4da70b8-ff85-4fb0-b23f-df115f397805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"n the sea  the universe  the stars there in the     heavens   urging slowly  surely forward  forming endless  and waiting ever more  forever more behind.       good bye my fancy!  good bye my fancy! farewell dear mate  dear love! i'm going away  i know not where  or to what fortune  or whether i may ever see you again  so good bye my fancy.  now for my last  let me look back a moment  the slower fainter ticking of the clock is in me  exit  nightfall  and soon the heart thud stopping.  long have we lived  joy'd  caress'd together  delightful!  now separation  good bye my fancy.  yet let me not be too hasty  long indeed have we lived  slept  filter'd  become really blended     into one  then if we die we die together   yes  we'll remain one   if we go anywhere we'll go together to meet what happens  may be we'll be better off and blither  and learn something  may be it is yourself now really ushering me to the true songs   who     knows?  may be it is you the mortal knob really undoing  turning  so now finally  good bye  and hail! my fancy.  \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_text[11792000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14d6b0a3-9064-49b3-a16d-d527cddefe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(''.join(text))                  #Get the sentences\n",
    "    return sentences     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f90469b6-7286-4434-a88c-2668f0ccd4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96282\n",
      "her mother had died too long ago for her to have more than an indistinct remembrance of her caresses  and her place had been supplied by an excellent woman as governess  who had fallen little short of a mother in affection.\n",
      "i hardly understand you   replied the scientist  with a cold intensity of manner.\n"
     ]
    }
   ],
   "source": [
    "sentences = tokenize_sentences(combined_text) \n",
    "print(len(sentences))    \n",
    "print(sentences[2])  \n",
    "print(sentences[57649])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaf01dde-77eb-44b7-a099-fffcab33cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3036efe1-6ba2-4a96-9b1d-fae699e453d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2235498\n",
      "austen\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "words = tokenize_words(combined_text)\n",
    "print(len(words))\n",
    "print(words[3])\n",
    "print(words[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f56cfa79-f981-43fb-b6c1-1cef34040840",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = []                                    #list of all the words of sentences\n",
    "for f in sentences:\n",
    "    words_list.append(tokenize_words(f))                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01930b98-39d1-4eec-9275-5b154687c41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96282"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ba378be-6fa5-440d-be37-b8ef51dfbab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she\n",
      "was\n",
      "the\n",
      "youngest\n",
      "of\n",
      "the\n",
      "two\n",
      "daughters\n",
      "of\n",
      "a\n",
      "most\n",
      "affectionate\n",
      "indulgent\n",
      "father\n",
      "and\n",
      "had\n",
      "in\n",
      "consequence\n",
      "of\n",
      "her\n",
      "sister\n",
      "'s\n",
      "marriage\n",
      "been\n",
      "mistress\n",
      "of\n",
      "his\n",
      "house\n",
      "from\n",
      "a\n",
      "very\n",
      "early\n",
      "period\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in words_list[1]:                     # all the words of the second sentence\n",
    "    print(word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "013c921e-362d-40b9-aabb-474c4b489be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "random.shuffle(words_list)\n",
    "train_len = math.floor(0.6 * len(words_list))                      #Training set length(60%)\n",
    "dev_len = math.floor(0.2 * len(words_list))                        #Development set length (20%)\n",
    "test_len = math.floor(0.2 * len(words_list))                       #Test set length (20%)\n",
    "\n",
    "training_set = []\n",
    "development_set = []\n",
    "test_set = []\n",
    "\n",
    "for content in words_list[0:train_len]:\n",
    "    training_set.append(content)\n",
    "    \n",
    "for content in words_list[train_len: train_len + dev_len]:\n",
    "    development_set.append(content)\n",
    "\n",
    "for content in words_list[train_len + dev_len:]:\n",
    "    test_set.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "941fd203-6132-40db-a596-5b8ae9dd43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "def _calc_ngrams(all_corpus: list[str], ngram: int) -> Counter:\n",
    "    \"\"\"\n",
    "    Process a tokenized sentence into a list of ngrams.\n",
    "    :param all_corpus: a list of all the corpus words\n",
    "    :param ngram: whether the ngrams will be unigrams, bigrams etc\n",
    "    :return: the counter of either unigram, bigram or trigram\n",
    "    \"\"\"\n",
    "    unigram_counter = Counter()\n",
    "    bigram_counter = Counter()\n",
    "    trigram_counter = Counter()\n",
    "     \n",
    "    \n",
    "\n",
    "    if ngram == 1 :\n",
    "        for sentence in all_corpus:\n",
    "             grams = [gram for gram in ngrams(sentence, ngram, pad_left=True, pad_right=True,\n",
    "                                    left_pad_symbol=START_TOKEN, right_pad_symbol=END_TOKEN)]\n",
    "             unigram_counter.update(grams)\n",
    "        return unigram_counter\n",
    "        \n",
    "    elif ngram == 2:\n",
    "        for sentence in all_corpus:\n",
    "             grams = [gram for gram in ngrams(sentence, ngram, pad_left=True, pad_right=True,\n",
    "                                    left_pad_symbol=START_TOKEN, right_pad_symbol=END_TOKEN)]\n",
    "             bigram_counter.update(grams)\n",
    "        return bigram_counter\n",
    "        \n",
    "    elif ngram == 3:\n",
    "        for sentence in all_corpus:\n",
    "             grams = [gram for gram in ngrams(sentence, ngram, pad_left=True, pad_right=True,\n",
    "                                    left_pad_symbol=START_TOKEN, right_pad_symbol=END_TOKEN)]\n",
    "             trigram_counter.update(grams)\n",
    "        return trigram_counter\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dd7c6b6-b1d5-44c1-9308-867c7eb13fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_OOV_words_train(all_corpus):\n",
    "    unigram_counter = _calc_ngrams(all_corpus,1)\n",
    "    OOV_words = {}\n",
    "\n",
    "    for k, v in unigram_counter.items():\n",
    "        if v < 10:\n",
    "            key = k[0]\n",
    "            # README: Use the symbol UNKOWN_TOKEN else the model will think it's a word\n",
    "            OOV_words[key] = UNKNOWN_TOKEN                 #set the word to \"UNK\"\n",
    "\n",
    "    replaced_corpus = []                          #the original corpus having the OOV words replaced by 'UNK'\n",
    "    for sentence in all_corpus:\n",
    "        clean_sentence = []\n",
    "    \n",
    "        for word in sentence:\n",
    "            clean_sentence.append(OOV_words.get(word, word))\n",
    "    \n",
    "        replaced_corpus.append(clean_sentence)\n",
    "\n",
    "\n",
    "    vocabulary = []\n",
    "\n",
    "    for key in unigram_counter.keys():        #Iterate the unigram counter\n",
    "        word = key[0]                         #get the word\n",
    "        if word not in OOV_words:\n",
    "            vocabulary.append(word)\n",
    "\n",
    "    vocabulary = set(vocabulary)              #Keep unique words\n",
    "    return vocabulary, replaced_corpus, OOV_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92f61c51-81f4-4a80-81b1-769dbb9bd058",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, train_corpus, OOV_words = replace_OOV_words_train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "323891dd-f0bc-4454-9d24-bac8427d276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_OOV_words_test(all_corpus, vocabulary, oov_words):\n",
    "    \n",
    "    replaced_corpus = []\n",
    "    for sentence in all_corpus:\n",
    "        updated_sent = []\n",
    "\n",
    "        for word in sentence:\n",
    "            if (word not in vocabulary) or (word in oov_words):\n",
    "                updated_sent.append(UNKNOWN_TOKEN)\n",
    "            else:\n",
    "                updated_sent.append(word)\n",
    "                \n",
    "    replaced_corpus.append(updated_sent)\n",
    "    return replaced_corpus   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0eab7ab-2171-43a8-90ef-ef49e5335b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "development_set = replace_OOV_words_test(development_set, vocabulary, OOV_words)\n",
    "test_set = replace_OOV_words_test(test_set, vocabulary, OOV_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5de1392-390e-47ed-8202-93ad4f1248e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length:  7389\n",
      "Unigram's 20 most common words:\n",
      "(('the',), 80988)\n",
      "(('<UNK>',), 67987)\n",
      "(('and',), 57316)\n",
      "(('.',), 46354)\n",
      "(('of',), 43171)\n",
      "(('to',), 28921)\n",
      "(('a',), 20441)\n",
      "(('in',), 20275)\n",
      "(('i',), 18027)\n",
      "(('that',), 17278)\n",
      "(('he',), 15508)\n",
      "(('it',), 13331)\n",
      "(('his',), 12699)\n",
      "(('for',), 11897)\n",
      "(('was',), 11376)\n",
      "(('not',), 10952)\n",
      "(('with',), 10619)\n",
      "(('is',), 9871)\n",
      "(('you',), 9770)\n",
      "(('be',), 9614)\n",
      "\n",
      "\n",
      "Bigram's 20 most common words:\n",
      "(('.', '<end>'), 46110)\n",
      "(('of', 'the'), 11799)\n",
      "(('the', '<UNK>'), 8187)\n",
      "(('<start>', 'and'), 8032)\n",
      "(('in', 'the'), 6223)\n",
      "(('<UNK>', 'and'), 6178)\n",
      "(('?', '<end>'), 5991)\n",
      "(('and', 'the'), 5362)\n",
      "(('<UNK>', '<UNK>'), 5266)\n",
      "(('!', '<end>'), 5077)\n",
      "(('<UNK>', '.'), 4877)\n",
      "(('and', '<UNK>'), 4680)\n",
      "(('the', 'lord'), 4223)\n",
      "(('<UNK>', 'of'), 4105)\n",
      "(('<start>', 'i'), 3401)\n",
      "(('of', '<UNK>'), 3397)\n",
      "(('to', 'the'), 3269)\n",
      "(('<UNK>', 'the'), 3255)\n",
      "(('<start>', 'the'), 3066)\n",
      "(('a', '<UNK>'), 2527)\n",
      "\n",
      "\n",
      "Trigram's 20 most common words:\n",
      "(('.', '<end>', '<end>'), 46110)\n",
      "(('<start>', '<start>', 'and'), 8032)\n",
      "(('?', '<end>', '<end>'), 5991)\n",
      "(('!', '<end>', '<end>'), 5077)\n",
      "(('<UNK>', '.', '<end>'), 4849)\n",
      "(('<start>', '<start>', 'i'), 3401)\n",
      "(('<start>', '<start>', 'the'), 3066)\n",
      "(('<start>', '<start>', 'but'), 2434)\n",
      "(('<start>', '<start>', 'he'), 2074)\n",
      "(('<UNK>', 'and', '<UNK>'), 1683)\n",
      "(('the', '<UNK>', 'of'), 1467)\n",
      "(('<start>', '<start>', 'for'), 1395)\n",
      "(('<start>', '<start>', '<UNK>'), 1384)\n",
      "(('<start>', '<start>', 'it'), 1362)\n",
      "(('<start>', 'and', 'the'), 1305)\n",
      "(('<start>', '<start>', 'then'), 1152)\n",
      "(('of', 'the', 'lord'), 1081)\n",
      "(('<UNK>', 'of', 'the'), 1015)\n",
      "(('of', 'the', '<UNK>'), 1007)\n",
      "(('<start>', 'and', 'he'), 994)\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(vocabulary)\n",
    "print (\"Vocabulary length: \", vocab_len)\n",
    "\n",
    "print(\"Unigram's 20 most common words:\")\n",
    "unigram_top_20 = _calc_ngrams(train_corpus, 1).most_common(20)\n",
    "for gram in unigram_top_20:\n",
    "    print(gram)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Bigram's 20 most common words:\")\n",
    "bigram_top_20 = _calc_ngrams(train_corpus,2).most_common(20)\n",
    "for gram in bigram_top_20:\n",
    "    print(gram)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Trigram's 20 most common words:\")\n",
    "trigram_top_20 = _calc_ngrams(train_corpus,3).most_common(20)\n",
    "for gram in trigram_top_20:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361a4ed-2f57-4f4b-a61f-6227992bacf8",
   "metadata": {},
   "source": [
    "(ii). First step: Tune Î± (alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f8740d-4f21-49fc-abca-319b07e97926",
   "metadata": {},
   "source": [
    "##  Calculate bi-gram probability\n",
    "\n",
    "### $ P(w_2|w_1) = \\frac{C(w_1,w_2) + \\alpha}{C(w_1) + \\alpha \\cdot|V|} $\n",
    "\n",
    "* $ C(w_1,w_2) $ : bigram count\n",
    "* $ C(w_1) $ : unigram count\n",
    "* $ 0 \\leq\\alpha \\leq1 $ :  smoothing hyper-parameter\n",
    "* |V|: vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c2fbe4-0a4b-4a7d-bb1b-5daad8b369c6",
   "metadata": {},
   "source": [
    "## Bi-gram LM Cross entropy & perplexity\n",
    "\n",
    "* $ CrossEntropy = -\\frac{1}{N}\\sum^{bigrams}{log_2(P(w_2|w_1))} $\n",
    " * N: Number of bigrams\n",
    "* $ Perplexity = 2^{H(p)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea916643-39d0-4b00-830d-e294299450fe",
   "metadata": {},
   "source": [
    "## Tri-gram LM Cross entropy & perplexity\n",
    "\n",
    "### $ P(w_3|w_1,w_2) = \\frac{C(w_1,w_2,w_3) + \\alpha}{C(w_1,w_2) + \\alpha \\cdot |V|} $\n",
    "\n",
    "* $ C(w_1,w_2,w_3) $ : trigram count\n",
    "* $ C(w_1,w_2) $ : bigram count\n",
    "* $ 0 \\leq\\alpha \\leq1 $ :  smoothing hyper-parameter\n",
    "* |V|: vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97dd4ed9-b159-469c-8686-ce887789cc58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_entropy(model: BaseNgramModel, dataset: list[list[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy of a language model on a given dataset.\n",
    "    \n",
    "    Cross-entropy measures how well the language model predicts the given dataset.\n",
    "    Lower cross-entropy indicates better model performance.\n",
    "    \n",
    "    :param model: The n-gram language model for which cross-entropy is calculated.\n",
    "    :param dataset: The dataset as a list of tokenized sentences, where each sentence is a list of strings.\n",
    "    :return: The cross-entropy score as a float.\n",
    "             Lower values indicate better performance in predicting the dataset.\n",
    "    \"\"\"\n",
    "    # since la place smoothing is only involved during inference\n",
    "    # we don't need to refit the model\n",
    "    sum_prob = 0\n",
    "    word_count = 0\n",
    "    \n",
    "    for sentence in dataset:       \n",
    "        # since this is a full sentence we manually append the end token\n",
    "        sentence += [END_TOKEN]\n",
    "        \n",
    "        # take into account only the END_TOKEN since START token probs are not computed\n",
    "        word_count += len(sentence)\n",
    "        \n",
    "        # get sentence probability\n",
    "        sum_prob += bi_model.sentence_proba(sentence) \n",
    "    \n",
    "    # do we need to logarithmize this again?\n",
    "    return - sum_prob / word_count\n",
    "\n",
    "\n",
    "def perplexity(cross_entropy: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate perplexity from cross-entropy.\n",
    "    \n",
    "    Perplexity is a measure of how well the language model predicts the given dataset.\n",
    "    A model with a perplexity of k, has approximately a 1/k chance of correctly predicting the next word in a sentence.\n",
    "    \n",
    "    :param cross_entropy: The cross-entropy score calculated for a language model on a dataset.\n",
    "    :return: The perplexity score as a float.\n",
    "             Lower values indicate better performance in predicting the dataset.\n",
    "    \"\"\"\n",
    "    return 2**cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e4809e1-9596-4e76-8094-0b4c241f57ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ngram_model_alpha_search(fitted_model: BaseNgramModel, \n",
    "                             alpha_values: np.ndarray, \n",
    "                             validation_dataset: list[list[str]]) -> float:\n",
    "    entropy_arr = np.full_like(alpha_values, np.inf)\n",
    "    \n",
    "    for i in range(len(alpha_values)):\n",
    "        fitted_model.alpha = alpha_values[i]\n",
    "        entropy_arr[i] = cross_entropy(fitted_model, validation_dataset)\n",
    "        \n",
    "    best_index = np.argmin(entropy_arr)\n",
    "    return alpha_values[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b3cc2dd-e3f0-4f93-80ee-bf3a8205383b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_model = BigramModel(alpha=0.001)\n",
    "bi_model.fit(train_corpus)\n",
    "\n",
    "ngram_model_alpha_search(bi_model, np.linspace(0.001, 1, 100), development_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "871e5b71-306b-46f9-b96d-b2d2cb1d71d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_model = TrigramModel(alpha=0.001)\n",
    "tri_model.fit(train_corpus)\n",
    "\n",
    "ngram_model_alpha_search(tri_model, np.linspace(0.001, 1, 100), development_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c6816-e22b-4be7-bc77-752bd6960c06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: validate these alpha values and then use them below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17f41bba-770b-40a2-8f26-85695431ab9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-gram model Cross Entropy:  6.218499528998154\n",
      "Bi-gram model Perplexity:  74.46546164724224\n"
     ]
    }
   ],
   "source": [
    "bi_model = BigramModel(alpha=0.001)\n",
    "bi_model.fit(train_corpus)\n",
    "\n",
    "bi_hc = cross_entropy(bi_model, train_corpus)\n",
    "print(\"Bi-gram model Cross Entropy: \", bi_hc)\n",
    "print(\"Bi-gram model Perplexity: \", perplexity(bi_hc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19423c60-91e1-42d6-98a2-2a902ae7a156",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tri-gram model Cross Entropy:  6.992052745057244\n",
      "Tri-gram model Perplexity:  127.296835488569\n"
     ]
    }
   ],
   "source": [
    "tri_model = TrigramModel(alpha=0.001)\n",
    "tri_model.fit(train_corpus)\n",
    "\n",
    "tri_hc = cross_entropy(tri_model, train_corpus)\n",
    "print(\"Tri-gram model Cross Entropy: \", tri_hc)\n",
    "print(\"Tri-gram model Perplexity: \", perplexity(tri_hc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f62e2d9-1ea3-4935-9d0b-735e7c37c21e",
   "metadata": {},
   "source": [
    "Now, let's test the performance in the test set, after having defined the optimal alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75603bb8-21ee-469b-bf12-85665e9094b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 5\n",
    "beam_width = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a8c72-e1a8-44dc-8338-266bbf419ee2",
   "metadata": {},
   "source": [
    "v. Create a fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcf7846c-676f-42ed-8edb-5ef8cc6a557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "UNK_sentences = [sent_tokenize(' '.join(sentence)) for sentence in train_corpus]       #get the sentences that include UNK values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5489ff0f-2e72-4587-bdf6-e75e561b83fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['sir ?', '<end> <end>']\n",
      "Corrupted: ['sir ?', '<amd> <emd>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['princes shall come out of egypt ethiopia shall soon stretch out her hands unto god .', '<end> <end>']\n",
      "Corrupted: ['princes chall cone out of egvpt efhiopia shall soon sttefch out her hands unto god .', '<end> <enb>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['aye aye men !', '<end> <end>']\n",
      "Corrupted: ['aye aye men !', '<enb> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['the singers the children of asaph an hundred forty and eight .', '<end> <end>']\n",
      "Corrupted: ['the singers tha children of asapn an hondtad forfy end aijht .', '<end> <anb>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['then the eleven disciples went away into galilee into a mountain where jesus had appointed them .', '<end> <end>']\n",
      "Corrupted: ['tham the alevem disclpies went ewey lmtu galilee into e mounfaim wnete jesus had appointad then .', '<end> <enb>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['who would <UNK> <UNK> to land !', '<end> <end>']\n",
      "Corrupted: ['who would <UNK> <UNK> to land !', '<end> <emd>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['now therefore present yourselves before the lord by your tribes and by your thousands .', '<end> <end>']\n",
      "Corrupted: ['now tneretute present yourcelves bafora fhe lord pv yout fribes and by your thuusanbc .', '<anb> <and>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['behold he taketh away who can hinder him ?', '<end> <end>']\n",
      "Corrupted: ['behoib ne taketh awav whu can hinder him ?', '<and> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['the <UNK> stared for some seconds as if at a <UNK> and vanished .', '<end> <end>']\n",
      "Corrupted: ['the <UNK> stared for sume sesomds es if et e <UNK> end venished .', '<end> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['chapter i. waked as her custom was before the day to do the <UNK> due to <UNK> may .', '<end> <end>']\n",
      "Corrupted: ['chapter i. waked as her costom was batore the dey to du fha <UNK> due to <UNK> mey .', '<end> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['a prayer of <UNK> the prophet upon <UNK> .', '<end> <end>']\n",
      "Corrupted: ['a pteyer ot <UNK> tne btophef upon <UNK> .', '<end> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['such a <UNK> and mysterious monster roused all my curiosity .', '<end> <end>']\n",
      "Corrupted: ['cuch a <UNK> enb mystetious momster ruused all my curlusity .', '<emd> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['being pretty well aware of what sort of joy you must both be feeling i have been in no hurry with my <UNK> but i hope it all went off tolerably well .', '<end> <end>']\n",
      "Corrupted: ['belng breffy weil awere of vhat sort of jov yuu must poth ba feeiing i nave baen ln mo hurrv vlth ny <UNK> but i hope it all went otf folerebly well .', '<emd> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['and they took dagon and set him in his place again .', '<end> <end>']\n",
      "Corrupted: ['and thay fook dagon end set hlm im nis piace again .', '<end> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['and jesus answering them said have ye not read so much as this what david did when himself was an <UNK> and they which were with him how he went into the house of god and did take and eat the shewbread and gave also to them that were with him which it is not lawful to eat but for the priests alone ?', '<end> <end>']\n",
      "Corrupted: ['anb jecuc answatimg them caib hava ya mot reeb so much as this what dawid did wham himself was an <UNK> and they which were with him how he wemf lntu tne hoose of god and dld taxa anb eet fhe shewbtaed amd gava elco fo fhem thaf weta with hln whlsh lf ic nof lawfui to aet pof tor tne ptiastc alome ?', '<end> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['she expected from other people the same opinions and feelings as her own and she judged of their motives by the immediate effect of their actions on herself .', '<end> <end>']\n",
      "Corrupted: ['she akpectad ftom othar people fhe sema opinionc anb feelingc es har ovn and sne gudgeb of thelr motlvec by the immediate effect uf thait actions on harcelf .', '<end> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['evil to some is always good to others .', '<end> <end>']\n",
      "Corrupted: ['evil to cone is aiways goud to othats .', '<end> <amb>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['though such a movement on the part of the whale struck under such circumstances is in no wise <UNK> and indeed is almost always more or less anticipated yet does it present one of the more perilous <UNK> of the fishery .', '<end> <end>']\n",
      "Corrupted: ['tnough soch a muwenent on fhe part uf the whaie struck unbar such sircumsfanses is in no wise <UNK> amd imdeed is alnust aiways note or less anticipated vaf dues it btecemt one uf the nore perlluus <UNK> of the flshety .', '<and> <amb>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: [\"how do you know it 's false ?\", '<end> <end>']\n",
      "Corrupted: [\"how do yuu know it 's felse ?\", '<emd> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: [\"' i am a hebrew ' he cries and then ' i fear the lord the god of heaven who hath made the sea and the dry land ! '\", '<end> <end>']\n",
      "Corrupted: [\"' i am a hebrew ' ha stiac amd than ' i fear the iurd the god uf heawam who heth mede the see and the dry lamd ! '\", '<and> <enb>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['the professor with scarcely a pause in his <UNK> <UNK> back then sit on it .', '<end> <end>']\n",
      "Corrupted: ['the brofassor vith scarceiy a pause in his <UNK> <UNK> pacx them sit un lf .', '<anb> <amd>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['and i saw a great white throne and him that sat on it from whose face the earth and the heaven fled away and there was found no place for them .', '<end> <end>']\n",
      "Corrupted: ['amd i sew a gteef vhite thtona enb him fhat sat on it trom wnose face fhe eerfh anb the heewen tled eway and there wes fuonb nu place for tnem .', '<end> <emd>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['it was natural to have some civil hopes on the subject and he said i hope you did not venture far miss fairfax this morning or i am sure you must have been wet .', '<end> <end>']\n",
      "Corrupted: ['it wes natutal fo have some civil hopes on tha supjact and he said i nope yuu dld nof venfure far miss faitfax fhis morning or i em cute vou must hawa peen vet .', '<end> <and>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: [\"let 's have a <UNK> or two before we ride to anchor in blanket bay .\", '<end> <end>']\n",
      "Corrupted: [\"let 's hava a <UNK> or fwo befura we ride to enshor in blanket bey .\", '<end> <anb>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['he was a melancholy spectacle .', '<end> <end>']\n",
      "Corrupted: ['he vas e melanchuly cpectecle .', '<enb> <emd>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['and ye shall compass the city all ye men of war and go round about the city once .', '<end> <end>']\n",
      "Corrupted: ['amd ye chall cumbass fna cify ail va nen of war and go round about fha citv once .', '<end> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['i will be glad and rejoice in thee i will sing praise to thy name o thou most high .', '<end> <end>']\n",
      "Corrupted: ['l will pe glad enb rejoice in thee i vill sing pralsa to fhy neme o thou most hlgh .', '<end> <emd>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['praise ye the lord .', '<end> <end>']\n",
      "Corrupted: ['praise ye tna lord .', '<end> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['yes yes !', '<end> <end>']\n",
      "Corrupted: ['yas yes !', '<end> <enb>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['the are lost !', '<end> <end>']\n",
      "Corrupted: ['fhe are iost !', '<enb> <emd>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['your own judgment must direct you .', '<end> <end>']\n",
      "Corrupted: ['yoor owm judgment must direst you .', '<end> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['the fire though it had dropped in one or two places was upon the whole higher and more <UNK> than ever .', '<end> <end>']\n",
      "Corrupted: ['tna fire fhuogh it nad droppad in une ut two places vec upon tha wnole higner anb more <UNK> than avet .', '<end> <emd>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['they were too much like joy <UNK> joy !', '<end> <end>']\n",
      "Corrupted: ['they were foo much iike jov <UNK> joy !', '<emd> <emd>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['and when he had opened the fifth seal i saw under the altar the souls of them that were slain for the word of god and for the testimony which they held and they cried with a loud voice saying how long o lord holy and true dost thou not judge and avenge our blood on them that dwell on the earth ?', '<end> <end>']\n",
      "Corrupted: ['amd vhen ne hed upeneb tne fiftn seel l saw under the altar tha souis ot fnem fhat vere slalm fur the votd of jod anb fut tne festimuny which thay held and fney stied with a loud voisa seying how lonj o lurd holy and froe dost thou nof jodge amb evemge our pluod om then tnat dweli on the eatth ?', '<end> <and>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['and the lord hath given me knowledge of it and i know it then thou <UNK> me their doings .', '<end> <end>']\n",
      "Corrupted: ['anb the lord hafh giwen me xnowladge of if and i xmow if than tnou <UNK> na thelr dolngs .', '<and> <emd>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['now advise and see what answer i shall return to him that sent me .', '<end> <end>']\n",
      "Corrupted: ['now advlse amd see what enswer i shall return to nin thaf sent ma .', '<end> <and>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['and the second came saying lord thy pound hath gained five pounds .', '<end> <end>']\n",
      "Corrupted: ['and tha sasund came saying iorb fny bound hath gained tlve puunds .', '<end> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['every thing that openeth the <UNK> in all flesh which they bring unto the lord whether it be of men or beasts shall be thine nevertheless the firstborn of man shalt thou surely redeem and the firstling of unclean beasts shalt thou redeem .', '<end> <end>']\n",
      "Corrupted: ['every thing tnet opematn the <UNK> in ali fiesn wnich they bring untu the iord vhather if be of nen ot beastc shall be thine nawertnelesc fne firsfborn uf man snalt thuu soraly tedeem anb fhe firstling uf onclean baestc cnalt thou redeam .', '<enb> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['passage to india !', '<end> <end>']\n",
      "Corrupted: ['pacsage to lndia !', '<end> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: [\"in a word it was queequeg 's conceit that if a man made up his mind to live mere sickness could not kill him nothing but a whale or a gale or some violent <UNK> <UNK> <UNK> of that sort .\", '<end> <end>']\n",
      "Corrupted: [\"in a word lt wes quaaqueg 's comseit that if a man meda up nis mind tu llve mere sickmess couid mot kiil him mothing but a whele or a gaie or some violent <UNK> <UNK> <UNK> uf thet sott .\", '<enb> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: [\"sing on there in the <UNK> o singer <UNK> and tender i hear your notes i hear your call i hear i come presently i understand you but a moment i <UNK> for the <UNK> star has detain 'd me the star my departing comrade holds and <UNK> me .\", '<end> <end>']\n",
      "Corrupted: [\"cimg om tnete im the <UNK> u singer <UNK> amb tender i heet yuur motec i heer yoor call i naar i some prasently i understanb vou but a momemt l <UNK> fot tne <UNK> stat nes betain 'd ma the ctar my dapartimg comrebe holdc and <UNK> me .\", '<end> <emd>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['enter king .', '<end> <end>']\n",
      "Corrupted: ['emtet kimg .', '<end> <anb>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['but his doom reserved him to more wrath for now the thought both of lost happiness and lasting pain <UNK> him round he <UNK> his <UNK> eyes that <UNK> huge affliction and dismay mixed with <UNK> pride and <UNK> hate .', '<end> <end>']\n",
      "Corrupted: ['buf nis doon recerved nim to more wrath for now the thoojht buth of lost nabpinecs and lastimg paln <UNK> him round he <UNK> his <UNK> eyac that <UNK> hoge atfllctlon anb dlsmay miked wifh <UNK> pride emd <UNK> hate .', '<end> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['and in the morning it will be foul weather to day for the sky is red and lowering .', '<end> <end>']\n",
      "Corrupted: ['and in the morning lf will ba foui veafhar tu day for tha skv is rad and lovaring .', '<enb> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['if the people of the barrel organ could keep their old world <UNK> so could he .', '<end> <end>']\n",
      "Corrupted: ['lf tne peuple ot tha barrel orgem coold xaap fheir uib world <UNK> su could he .', '<and> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['because though he cast a long evening shadow he still stood in warm sunlight .', '<end> <end>']\n",
      "Corrupted: ['becauce fhuogh he cact a lomj evaning shebow ha stlli stood ln verm cumlighf .', '<emd> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['what dat do <UNK> de <UNK> said the old black <UNK> .', '<end> <end>']\n",
      "Corrupted: ['vnat dat do <UNK> de <UNK> caid the oid black <UNK> .', '<end> <end>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['macian she is uncommonly like what !', '<end> <end>']\n",
      "Corrupted: ['neclam sne is uncommuniy like vhat !', '<end> <emd>']\n",
      "\n",
      "---\n",
      "\n",
      "Original: ['messa .', '<end> <end>']\n",
      "Corrupted: ['mesca .', '<end> <enb>']\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def corrupt_sentence(sentence, probability):\n",
    "    corrupted_sentence = \"\"\n",
    "    for char in sentence:\n",
    "        if char != ' ' and random.random() < probability:\n",
    "            \n",
    "            corrupted_sentence += get_similar_char(char)                 #replace with a similar character\n",
    "        else:\n",
    "            corrupted_sentence += char\n",
    "    return corrupted_sentence\n",
    "\n",
    "def get_similar_char(char):\n",
    "    \n",
    "    # later on maybe use the nlpaug library here \n",
    "    similar_chars = {\n",
    "        'a': 'e',\n",
    "        'b': 'p',\n",
    "        'c': 's',\n",
    "        'd': 'b',\n",
    "        'e': 'a',\n",
    "        'f': 't',\n",
    "        'g': 'j',\n",
    "        'h': 'n',\n",
    "        'i': 'l',\n",
    "        'j': 'g',\n",
    "        'k': 'x',\n",
    "        'l': 'i',\n",
    "        'm': 'n',\n",
    "        'n': 'm',\n",
    "        'o': 'u',\n",
    "        'p': 'b',\n",
    "        'q': 'g',\n",
    "        'r': 't',\n",
    "        's': 'c',\n",
    "        't': 'f',\n",
    "        'u': 'o',\n",
    "        'v': 'w',\n",
    "        'w': 'v',\n",
    "        'x': 'k',\n",
    "        'y': 'v',\n",
    "        'z': 's',\n",
    "    }\n",
    "\n",
    "    \n",
    "    return similar_chars.get(char, char)                            #return a randomly chosen character\n",
    "\n",
    "'''\n",
    "test_corpus = [\"he plays football\",\n",
    "               \"he plais footbal\",\n",
    "               \"she enjoys good football\",\n",
    "               \"she plays good music\",\n",
    "               \"he prays to god\",\n",
    "               \"please buy me the other ball\",\n",
    "               \"he pleases the other players by playing good football\",\n",
    "               \"he plys god ftball\"]\n",
    "\n",
    "'''\n",
    "probability = 0.2                                        #probability of character replacement\n",
    "\n",
    "\n",
    "     \n",
    "corrupted_corpus = [[corrupt_sentence(word, probability) for word in sentence] for sentence in UNK_sentences[1:50]]   #generate the corrupted corpus\n",
    "\n",
    "for original, corrupted in zip(UNK_sentences[1:50], corrupted_corpus):\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Corrupted: {corrupted}\")\n",
    "    print(\"\\n---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7bde09d0-4ad6-4a34-a17a-46e1a6e0bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "tweet_wt = TweetTokenizer()\n",
    "tokenized = [tweet_wt.tokenize(' '.join(sentence)) for sentence in UNK_sentences[:50]]  # Get the first 50 sentences\n",
    "\n",
    "model = BigramModel(alpha=0.01)\n",
    "model.fit(tokenized)                                # model is fitted with the correct and tokenized words\n",
    "\n",
    "corrupted_tokenized = [tweet_wt.tokenize(sentence) for sentence_list in corrupted_corpus for sentence in sentence_list] #tokenize the corrupted sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6779be16-f60e-4fbf-a336-c0e2dad91458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentences: [['who would <UNK> <UNK> to land !', '<end> <end>'], ['now therefore present yourselves before the lord by your tribes and by your thousands .', '<end> <end>'], ['behold he taketh away who can hinder him ?', '<end> <end>'], ['the <UNK> stared for some seconds as if at a <UNK> and vanished .', '<end> <end>'], ['chapter i. waked as her custom was before the day to do the <UNK> due to <UNK> may .', '<end> <end>']]\n",
      "\n",
      "\n",
      "Corrupted(wrong) sentences: [['who would <UNK> <UNK> to land !', '<end> <emd>'], ['now tneretute present yourcelves bafora fhe lord pv yout fribes and by your thuusanbc .', '<anb> <and>'], ['behoib ne taketh awav whu can hinder him ?', '<and> <end>'], ['the <UNK> stared for sume sesomds es if et e <UNK> end venished .', '<end> <end>'], ['chapter i. waked as her costom was batore the dey to du fha <UNK> due to <UNK> mey .', '<end> <end>']]\n",
      "\n",
      "\n",
      "Final result (corrected sentences): [['<start>', 'now', 'therefore', 'present', 'yourselves', 'before', 'the', 'first', 'shout', 'for', 'tarlton', \"'\", 's', 'triumph', 'was', 'over', 'everybody', 'exclaimed', 'poor', 'loveit', '!'], ['<start>', 'because', 'though', 'such', 'a', 'mountain', 'where', 'jesus', 'answering', 'them', 'that', 'were', 'slain', 'for', 'tarlton', \"'\", 's', 'triumph', 'was', 'over', 'everybody'], ['<start>', 'now', 'therefore', 'present', 'yourselves', 'before', 'the', 'first', 'shout', 'for', 'tarlton', \"'\", 's', 'triumph', 'was', 'over', 'everybody', 'exclaimed', 'poor', 'loveit', '!'], ['<start>', 'now', 'therefore', 'present', 'one', 'or', 'two', 'places', 'was', 'queequeg', \"'\", 's', 'triumph', 'was', 'queequeg', \"'\", 's', 'triumph', 'was', 'over', 'everybody'], ['<start>', 'now', 'therefore', 'present', 'yourselves', 'before', 'the', 'first', 'shout', 'for', 'tarlton', \"'\", 's', 'triumph', 'was', 'over', 'everybody', 'exclaimed', 'poor', 'loveit', '!']]\n"
     ]
    }
   ],
   "source": [
    "corrected = []\n",
    "corrector = BigramSpellCorrector(model, lamda1=0.5, lamda2=-0.5)\n",
    "for sent in corrupted_tokenized:\n",
    "  output_seq = corrector.spell_correct(original_tokenized_sentence=sent, max_depth = 20, beam_width = 3)  #give the corrupt sentences to spell correct\n",
    "  corrected.append(output_seq)\n",
    "    \n",
    "print('Original sentences:', UNK_sentences[6:11])\n",
    "print('\\n')\n",
    "print('Corrupted(wrong) sentences:', corrupted_corpus[5:10])\n",
    "print('\\n')\n",
    "print('Final result (corrected sentences):', corrected[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9aadeb1d-ed6d-4c29-9dbe-73cd4f88f7cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence ['after', 'the', 'first', 'shout', 'for', 'tarlton', \"'\", 's', 'triumph', 'was', 'over', 'everybody', 'exclaimed', 'poor', 'loveit', '!', '<end>', '<end>']\n",
      "Original sentence:  ['after']\n",
      "Prediction:  the\n",
      "\n",
      "\n",
      "Original sentence:  ['after', 'the']\n",
      "Prediction:  lord\n",
      "\n",
      "\n",
      "Original sentence:  ['after', 'the', 'first']\n",
      "Prediction:  shout\n",
      "\n",
      "\n",
      "Original sentence:  ['after', 'the', 'first', 'shout']\n",
      "Prediction:  for\n",
      "\n",
      "\n",
      "Original sentence:  ['after', 'the', 'first', 'shout', 'for']\n",
      "Prediction:  tarlton\n",
      "\n",
      "\n",
      "Original sentence:  ['after', 'the', 'first', 'shout', 'for', 'tarlton']\n",
      "Prediction:  '\n",
      "\n",
      "\n",
      "Original sentence:  ['after', 'the', 'first', 'shout', 'for', 'tarlton', \"'\"]\n",
      "Prediction:  s\n",
      "\n",
      "\n",
      "Original sentence:  ['after', 'the', 'first', 'shout', 'for', 'tarlton', \"'\", 's']\n",
      "Prediction:  triumph\n",
      "\n",
      "\n",
      "Original sentence:  ['after', 'the', 'first', 'shout', 'for', 'tarlton', \"'\", 's', 'triumph']\n",
      "Prediction:  was\n",
      "\n",
      "\n",
      "Original sentence:  ['after', 'the', 'first', 'shout', 'for', 'tarlton', \"'\", 's', 'triumph', 'was']\n",
      "Prediction:  over\n",
      "\n",
      "\n",
      "Original sentence:  ['after', 'the', 'first', 'shout', 'for', 'tarlton', \"'\", 's', 'triumph', 'was', 'over']\n",
      "Prediction:  everybody\n",
      "\n",
      "\n",
      "Original sentence:  ['after', 'the', 'first', 'shout', 'for', 'tarlton', \"'\", 's', 'triumph', 'was', 'over', 'everybody']\n",
      "Prediction:  exclaimed\n",
      "\n",
      "\n",
      "Original sentence:  ['after', 'the', 'first', 'shout', 'for', 'tarlton', \"'\", 's', 'triumph', 'was', 'over', 'everybody', 'exclaimed']\n",
      "Prediction:  poor\n",
      "\n",
      "\n",
      "Original sentence:  ['after', 'the', 'first', 'shout', 'for', 'tarlton', \"'\", 's', 'triumph', 'was', 'over', 'everybody', 'exclaimed', 'poor']\n",
      "Prediction:  loveit\n",
      "\n",
      "\n",
      "Original sentence:  ['after', 'the', 'first', 'shout', 'for', 'tarlton', \"'\", 's', 'triumph', 'was', 'over', 'everybody', 'exclaimed', 'poor', 'loveit']\n",
      "Prediction:  !\n",
      "\n",
      "\n",
      "Original sentence:  ['after', 'the', 'first', 'shout', 'for', 'tarlton', \"'\", 's', 'triumph', 'was', 'over', 'everybody', 'exclaimed', 'poor', 'loveit', '!']\n",
      "Prediction:  '\n",
      "\n",
      "\n",
      "Original sentence ['sir', '?', '<end>', '<end>']\n",
      "Original sentence:  ['sir']\n",
      "Prediction:  ?\n",
      "\n",
      "\n",
      "Original sentence:  ['sir', '?']\n",
      "Prediction:  <end>\n",
      "\n",
      "\n",
      "Original sentence ['princes', 'shall', 'come', 'out', 'of', 'egypt', 'ethiopia', 'shall', 'soon', 'stretch', 'out', 'her', 'hands', 'unto', 'god', '.', '<end>', '<end>']\n",
      "Original sentence:  ['princes']\n",
      "Prediction:  shall\n",
      "\n",
      "\n",
      "Original sentence:  ['princes', 'shall']\n",
      "Prediction:  soon\n",
      "\n",
      "\n",
      "Original sentence:  ['princes', 'shall', 'come']\n",
      "Prediction:  presently\n",
      "\n",
      "\n",
      "Original sentence:  ['princes', 'shall', 'come', 'out']\n",
      "Prediction:  her\n",
      "\n",
      "\n",
      "Original sentence:  ['princes', 'shall', 'come', 'out', 'of']\n",
      "Prediction:  god\n",
      "\n",
      "\n",
      "Original sentence:  ['princes', 'shall', 'come', 'out', 'of', 'egypt']\n",
      "Prediction:  ethiopia\n",
      "\n",
      "\n",
      "Original sentence:  ['princes', 'shall', 'come', 'out', 'of', 'egypt', 'ethiopia']\n",
      "Prediction:  shall\n",
      "\n",
      "\n",
      "Original sentence:  ['princes', 'shall', 'come', 'out', 'of', 'egypt', 'ethiopia', 'shall']\n",
      "Prediction:  soon\n",
      "\n",
      "\n",
      "Original sentence:  ['princes', 'shall', 'come', 'out', 'of', 'egypt', 'ethiopia', 'shall', 'soon']\n",
      "Prediction:  stretch\n",
      "\n",
      "\n",
      "Original sentence:  ['princes', 'shall', 'come', 'out', 'of', 'egypt', 'ethiopia', 'shall', 'soon', 'stretch']\n",
      "Prediction:  out\n",
      "\n",
      "\n",
      "Original sentence:  ['princes', 'shall', 'come', 'out', 'of', 'egypt', 'ethiopia', 'shall', 'soon', 'stretch', 'out']\n",
      "Prediction:  her\n",
      "\n",
      "\n",
      "Original sentence:  ['princes', 'shall', 'come', 'out', 'of', 'egypt', 'ethiopia', 'shall', 'soon', 'stretch', 'out', 'her']\n",
      "Prediction:  hands\n",
      "\n",
      "\n",
      "Original sentence:  ['princes', 'shall', 'come', 'out', 'of', 'egypt', 'ethiopia', 'shall', 'soon', 'stretch', 'out', 'her', 'hands']\n",
      "Prediction:  unto\n",
      "\n",
      "\n",
      "Original sentence:  ['princes', 'shall', 'come', 'out', 'of', 'egypt', 'ethiopia', 'shall', 'soon', 'stretch', 'out', 'her', 'hands', 'unto']\n",
      "Prediction:  god\n",
      "\n",
      "\n",
      "Original sentence:  ['princes', 'shall', 'come', 'out', 'of', 'egypt', 'ethiopia', 'shall', 'soon', 'stretch', 'out', 'her', 'hands', 'unto', 'god']\n",
      "Prediction:  and\n",
      "\n",
      "\n",
      "Original sentence:  ['princes', 'shall', 'come', 'out', 'of', 'egypt', 'ethiopia', 'shall', 'soon', 'stretch', 'out', 'her', 'hands', 'unto', 'god', '.']\n",
      "Prediction:  <end>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You need to give incomplete sentence for autocorrect. Right now it just predicts the end of the sentences after the '.'\n",
    "# Try a while loop until END_TOKEN is the output maybe\n",
    "\n",
    "#Check how the auto correction works\n",
    "predicted = []\n",
    "for sent in tokenized[:3]:\n",
    "    print(\"Original sentence\", sent)\n",
    "    for i, token in enumerate(sent):                     #try all possible combinations within the sentence\n",
    "        partial_sent = sent[:i+1]\n",
    "        if END_TOKEN in partial_sent:\n",
    "            break\n",
    "        pred = model.predict(tokenized_sentence = partial_sent)\n",
    "        print(\"Original sentence: \", partial_sent)\n",
    "        print(\"Prediction: \", pred)\n",
    "        predicted.append(pred)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e399e286-72a6-4202-9b68-a5ccd53ec5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import cer, wer\n",
    "\n",
    "\n",
    "count = 0\n",
    "sum_cer = 0\n",
    "sum_wer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef2059fe-2d2d-4be3-a6f9-8c8c722a7eaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m corrected_sentence \u001b[38;5;129;01min\u001b[39;00m corrected:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(corrected_sentence)):\n\u001b[1;32m----> 3\u001b[0m         token_k \u001b[38;5;241m=\u001b[39m START_TOKEN \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtokenized\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m         token_j \u001b[38;5;241m=\u001b[39m corrected_sentence[i]\n\u001b[0;32m      5\u001b[0m         sum_cer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cer(token_k, token_j)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for corrected_sentence in corrected:\n",
    "    for i in range(len(corrected_sentence)):\n",
    "        token_k = START_TOKEN if i == 0 else tokenized[i][i - 1]\n",
    "        token_j = corrected_sentence[i]\n",
    "        sum_cer += cer(token_k, token_j)\n",
    "        sum_wer += wer(token_k, token_j)\n",
    "        count += 1\n",
    "        \n",
    "avg_cer = sum_cer/count\n",
    "avg_wer = sum_wer/count\n",
    "\n",
    "print(f'Avg cer = {avg_cer}')\n",
    "print(f'Avg wer = {avg_wer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40890cc-c615-4a65-9f3a-d7ef513fdf66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
