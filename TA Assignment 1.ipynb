{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6543a2dd-779a-4643-a6bd-ffa5c1ac4f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Drogias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Drogias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6abedf4-87d7-4d30-b058-2a4a4f542aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.spell_correction import BigramSpellCorrector, TrigramSpellCorrector\n",
    "from src.autocomplete import BigramModel, START_TOKEN, END_TOKEN, TrigramModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b74da53-b6ee-490f-8ebc-6256aea039b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg_corpus = nltk.corpus.gutenberg.fileids()                                 #Get all the files\n",
    "gutenberg_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b16673-a350-459d-8fd3-0c3c990d9c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died t\n"
     ]
    }
   ],
   "source": [
    "combined_text = \"\"             \n",
    "for file_id in gutenberg_corpus:                                        # Combine the text from all files\n",
    "    combined_text += nltk.corpus.gutenberg.raw(file_id)\n",
    "\n",
    "print(combined_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9cd0930-d3ab-4bbc-82e8-71de27c63468",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = combined_text.lower()                              #Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c6b392-464d-4c08-b16b-acaabbea6e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[emma by jane austen 1816]\\n\\nvolume i\\n\\nchapter i\\n\\n\\nemma woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.\\n\\nshe was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.  her mother\\nhad died t\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e4f3c82-7a8e-42d5-b04b-4160fe689891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_chars(text):\n",
    "   text = text.replace('[', '')\n",
    "   text = text.replace(']', '')\n",
    "   text = text.replace('\\n', ' ')\n",
    "   text = re.sub(r'[^a-zA-z.?!\\']', ' ', text)                     #Remove these characters   \n",
    "\n",
    "   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb081710-1d7d-4c53-b85d-acd0a9528c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"emma by jane austen       volume i  chapter i   emma woodhouse  handsome  clever  and rich  with a comfortable home and happy disposition  seemed to unite some of the best blessings of existence  and had lived nearly twenty one years in the world with very little to distress or vex her.  she was the youngest of the two daughters of a most affectionate  indulgent father  and had  in consequence of her sister's marriage  been mistress of his house from a very early period.  her mother had died too\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_text = remove_special_chars(combined_text)\n",
    "combined_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f25b00-f68a-4ad1-893d-f4e27e93698f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2119883"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_text.split())                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7fcb110-bdda-41dc-8850-fae413272efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11793056"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_text)                             # How many characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4da70b8-ff85-4fb0-b23f-df115f397805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"n the sea  the universe  the stars there in the     heavens   urging slowly  surely forward  forming endless  and waiting ever more  forever more behind.       good bye my fancy!  good bye my fancy! farewell dear mate  dear love! i'm going away  i know not where  or to what fortune  or whether i may ever see you again  so good bye my fancy.  now for my last  let me look back a moment  the slower fainter ticking of the clock is in me  exit  nightfall  and soon the heart thud stopping.  long have we lived  joy'd  caress'd together  delightful!  now separation  good bye my fancy.  yet let me not be too hasty  long indeed have we lived  slept  filter'd  become really blended     into one  then if we die we die together   yes  we'll remain one   if we go anywhere we'll go together to meet what happens  may be we'll be better off and blither  and learn something  may be it is yourself now really ushering me to the true songs   who     knows?  may be it is you the mortal knob really undoing  turning  so now finally  good bye  and hail! my fancy.  \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_text[11792000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14d6b0a3-9064-49b3-a16d-d527cddefe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(''.join(text))                  #Get the sentences\n",
    "    return sentences     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f90469b6-7286-4434-a88c-2668f0ccd4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96282\n",
      "her mother had died too long ago for her to have more than an indistinct remembrance of her caresses  and her place had been supplied by an excellent woman as governess  who had fallen little short of a mother in affection.\n",
      "i hardly understand you   replied the scientist  with a cold intensity of manner.\n"
     ]
    }
   ],
   "source": [
    "sentences = tokenize_sentences(combined_text) \n",
    "print(len(sentences))    \n",
    "print(sentences[2])  \n",
    "print(sentences[57649])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaf01dde-77eb-44b7-a099-fffcab33cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3036efe1-6ba2-4a96-9b1d-fae699e453d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2235498\n",
      "austen\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "words = tokenize_words(combined_text)\n",
    "print(len(words))\n",
    "print(words[3])\n",
    "print(words[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f56cfa79-f981-43fb-b6c1-1cef34040840",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = []                                    #list of all the words of sentences\n",
    "for f in sentences:\n",
    "    words_list.append(tokenize_words(f))                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01930b98-39d1-4eec-9275-5b154687c41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96282"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ba378be-6fa5-440d-be37-b8ef51dfbab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she\n",
      "was\n",
      "the\n",
      "youngest\n",
      "of\n",
      "the\n",
      "two\n",
      "daughters\n",
      "of\n",
      "a\n",
      "most\n",
      "affectionate\n",
      "indulgent\n",
      "father\n",
      "and\n",
      "had\n",
      "in\n",
      "consequence\n",
      "of\n",
      "her\n",
      "sister\n",
      "'s\n",
      "marriage\n",
      "been\n",
      "mistress\n",
      "of\n",
      "his\n",
      "house\n",
      "from\n",
      "a\n",
      "very\n",
      "early\n",
      "period\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in words_list[1]:                     # all the words of the second sentence\n",
    "    print(word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "013c921e-362d-40b9-aabb-474c4b489be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "random.shuffle(words_list)\n",
    "train_len = math.floor(0.6 * len(words_list))                      #Training set length(60%)\n",
    "dev_len = math.floor(0.2 * len(words_list))                        #Development set length (20%)\n",
    "test_len = math.floor(0.2 * len(words_list))                       #Test set length (20%)\n",
    "\n",
    "training_set = []\n",
    "development_set = []\n",
    "test_set = []\n",
    "\n",
    "for content in words_list[0:train_len]:\n",
    "    training_set.append(content)\n",
    "    \n",
    "for content in words_list[train_len: train_len + dev_len]:\n",
    "    development_set.append(content)\n",
    "\n",
    "for content in words_list[train_len + dev_len:]:\n",
    "    test_set.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "941fd203-6132-40db-a596-5b8ae9dd43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "def _calc_ngrams(all_corpus: list[str], ngram: int) -> Counter:\n",
    "    \"\"\"\n",
    "    Process a tokenized sentence into a list of ngrams.\n",
    "    :param all_corpus: a list of all the corpus words\n",
    "    :param ngram: whether the ngrams will be unigrams, bigrams etc\n",
    "    :return: the counter of either unigram, bigram or trigram\n",
    "    \"\"\"\n",
    "    unigram_counter = Counter()\n",
    "    bigram_counter = Counter()\n",
    "    trigram_counter = Counter()\n",
    "     \n",
    "    \n",
    "\n",
    "    if ngram == 1 :\n",
    "        for sentence in all_corpus:\n",
    "             grams = [gram for gram in ngrams(sentence, ngram, pad_left=True, pad_right=True,\n",
    "                                    left_pad_symbol=START_TOKEN, right_pad_symbol=END_TOKEN)]\n",
    "             unigram_counter.update(grams)\n",
    "        return unigram_counter\n",
    "        \n",
    "    elif ngram == 2:\n",
    "        for sentence in all_corpus:\n",
    "             grams = [gram for gram in ngrams(sentence, ngram, pad_left=True, pad_right=True,\n",
    "                                    left_pad_symbol=START_TOKEN, right_pad_symbol=END_TOKEN)]\n",
    "             bigram_counter.update(grams)\n",
    "        return bigram_counter\n",
    "        \n",
    "    elif ngram == 3:\n",
    "        for sentence in all_corpus:\n",
    "             grams = [gram for gram in ngrams(sentence, ngram, pad_left=True, pad_right=True,\n",
    "                                    left_pad_symbol=START_TOKEN, right_pad_symbol=END_TOKEN)]\n",
    "             trigram_counter.update(grams)\n",
    "        return trigram_counter\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dd7c6b6-b1d5-44c1-9308-867c7eb13fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_OOV_words_train(all_corpus):\n",
    "    unigram_counter = _calc_ngrams(all_corpus,1)\n",
    "    OOV_words = {}\n",
    "\n",
    "    for k, v in unigram_counter.items():\n",
    "        if v < 10:\n",
    "            key = k[0]\n",
    "            OOV_words[key] = \"UNK\"                 #set the word to \"UNK\"\n",
    "\n",
    "    replaced_corpus = []                          #the original corpus having the OOV words replaced by 'UNK'\n",
    "    for sentence in all_corpus:\n",
    "        clean_sentence = []\n",
    "    \n",
    "        for word in sentence:\n",
    "            clean_sentence.append(OOV_words.get(word, word))\n",
    "    \n",
    "        replaced_corpus.append(clean_sentence)\n",
    "\n",
    "\n",
    "    vocabulary = []\n",
    "\n",
    "    for key in unigram_counter.keys():        #Iterate the unigram counter\n",
    "        word = key[0]                         #get the word\n",
    "        if word not in OOV_words:\n",
    "            vocabulary.append(word)\n",
    "\n",
    "    vocabulary = set(vocabulary)              #Keep unique words\n",
    "    return vocabulary, replaced_corpus, OOV_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92f61c51-81f4-4a80-81b1-769dbb9bd058",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, new_corpus, OOV_words = replace_OOV_words_train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "323891dd-f0bc-4454-9d24-bac8427d276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_OOV_words_test(all_corpus, vocabulary, oov_words):\n",
    "    \n",
    "    replaced_corpus = []\n",
    "    for sentence in all_corpus:\n",
    "        updated_sent = []\n",
    "\n",
    "        for word in sentence:\n",
    "            if (word not in vocabulary) or (word in oov_words):\n",
    "                updated_sent.append('UNK')\n",
    "            else:\n",
    "                updated_sent.append(word)\n",
    "                \n",
    "    replaced_corpus.append(updated_sent)\n",
    "    return replaced_corpus   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0eab7ab-2171-43a8-90ef-ef49e5335b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "development_set = replace_OOV_words_test(development_set, vocabulary, OOV_words)\n",
    "test_set = replace_OOV_words_test(test_set, vocabulary, OOV_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5de1392-390e-47ed-8202-93ad4f1248e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length:  7282\n",
      "Unigram's 20 most common words:\n",
      "(('the',), 79562)\n",
      "(('UNK',), 68344)\n",
      "(('and',), 56745)\n",
      "(('.',), 46531)\n",
      "(('of',), 42462)\n",
      "(('to',), 28609)\n",
      "(('a',), 20229)\n",
      "(('in',), 20175)\n",
      "(('i',), 18198)\n",
      "(('that',), 17234)\n",
      "(('he',), 15518)\n",
      "(('it',), 13316)\n",
      "(('his',), 12798)\n",
      "(('for',), 11625)\n",
      "(('was',), 11291)\n",
      "(('not',), 10977)\n",
      "(('with',), 10558)\n",
      "(('is',), 10058)\n",
      "(('you',), 9874)\n",
      "(('be',), 9637)\n",
      "\n",
      "\n",
      "Bigram's 20 most common words:\n",
      "(('.', '<end>'), 46309)\n",
      "(('of', 'the'), 11330)\n",
      "(('the', 'UNK'), 8090)\n",
      "(('<start>', 'and'), 8003)\n",
      "(('UNK', 'and'), 6296)\n",
      "(('in', 'the'), 6168)\n",
      "(('?', '<end>'), 5950)\n",
      "(('and', 'the'), 5240)\n",
      "(('UNK', 'UNK'), 5203)\n",
      "(('UNK', '.'), 4980)\n",
      "(('!', '<end>'), 4959)\n",
      "(('and', 'UNK'), 4674)\n",
      "(('the', 'lord'), 4195)\n",
      "(('UNK', 'of'), 4098)\n",
      "(('<start>', 'i'), 3532)\n",
      "(('of', 'UNK'), 3374)\n",
      "(('to', 'the'), 3164)\n",
      "(('UNK', 'the'), 3164)\n",
      "(('<start>', 'the'), 3060)\n",
      "(('a', 'UNK'), 2594)\n",
      "\n",
      "\n",
      "Trigram's 20 most common words:\n",
      "(('.', '<end>', '<end>'), 46309)\n",
      "(('<start>', '<start>', 'and'), 8003)\n",
      "(('?', '<end>', '<end>'), 5950)\n",
      "(('!', '<end>', '<end>'), 4959)\n",
      "(('UNK', '.', '<end>'), 4956)\n",
      "(('<start>', '<start>', 'i'), 3532)\n",
      "(('<start>', '<start>', 'the'), 3060)\n",
      "(('<start>', '<start>', 'but'), 2394)\n",
      "(('<start>', '<start>', 'he'), 2090)\n",
      "(('UNK', 'and', 'UNK'), 1717)\n",
      "(('the', 'UNK', 'of'), 1446)\n",
      "(('<start>', '<start>', 'UNK'), 1382)\n",
      "(('<start>', '<start>', 'for'), 1349)\n",
      "(('<start>', '<start>', 'it'), 1315)\n",
      "(('<start>', 'and', 'the'), 1242)\n",
      "(('<start>', '<start>', 'then'), 1121)\n",
      "(('of', 'the', 'lord'), 1036)\n",
      "(('UNK', 'of', 'the'), 993)\n",
      "(('<start>', '<start>', 'she'), 992)\n",
      "(('him', '.', '<end>'), 982)\n"
     ]
    }
   ],
   "source": [
    "vocab_len = len(vocabulary)\n",
    "print (\"Vocabulary length: \", vocab_len)\n",
    "\n",
    "print(\"Unigram's 20 most common words:\")\n",
    "unigram_top_20 = _calc_ngrams(new_corpus, 1).most_common(20)\n",
    "for gram in unigram_top_20:\n",
    "    print(gram)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Bigram's 20 most common words:\")\n",
    "bigram_top_20 = _calc_ngrams(new_corpus,2).most_common(20)\n",
    "for gram in bigram_top_20:\n",
    "    print(gram)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Trigram's 20 most common words:\")\n",
    "trigram_top_20 = _calc_ngrams(new_corpus,3).most_common(20)\n",
    "for gram in trigram_top_20:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361a4ed-2f57-4f4b-a61f-6227992bacf8",
   "metadata": {},
   "source": [
    "(ii). First step: Tune Î± (alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f8740d-4f21-49fc-abca-319b07e97926",
   "metadata": {},
   "source": [
    "##  Calculate bi-gram probability\n",
    "\n",
    "### $ P(w_2|w_1) = \\frac{C(w_1,w_2) + \\alpha}{C(w_1) + \\alpha \\cdot|V|} $\n",
    "\n",
    "* $ C(w_1,w_2) $ : bigram count\n",
    "* $ C(w_1) $ : unigram count\n",
    "* $ 0 \\leq\\alpha \\leq1 $ :  smoothing hyper-parameter\n",
    "* |V|: vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aeb667ae-c698-44d6-9e4b-f8748d46c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Bigram and trigram probabilities using Laplace and a-smoothing\n",
    "uni_counter = _calc_ngrams(training_set, 1)                     #Unigram counter\n",
    "bi_counter = _calc_ngrams(training_set, 2)                      #Bigram counter\n",
    "tri_counter = _calc_ngrams(training_set, 3)                     #Trigram counter\n",
    "def prob_bigram_model(w0,w1,alpha,vocabulary, uni_counter, bi_counter): \n",
    "    c_w = bi_counter[w0,w1]\n",
    "    c = uni_counter[w0]\n",
    "    prob = (c_w + alpha) / (c + alpha * len(vocabulary))                  #probability of bigram \n",
    "    return prob\n",
    "\n",
    "def prob_trigram_model(w0,w1,w2,alpha,vocabulary, bi_counter, tri_counter):\n",
    "    c_w = tri_counter[w0,w1,w2]\n",
    "    c = bi_counter[w0,w1]\n",
    "    prob = (c_w + alpha) / (c + alpha * len(vocabulary))                  #probability of trigram\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c2fbe4-0a4b-4a7d-bb1b-5daad8b369c6",
   "metadata": {},
   "source": [
    "## Bi-gram LM Cross entropy & perplexity\n",
    "\n",
    "* $ CrossEntropy = -\\frac{1}{N}\\sum^{bigrams}{log_2(P(w_2|w_1))} $\n",
    " * N: Number of bigrams\n",
    "* $ Perplexity = 2^{H(p)} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab10c7fb-84d1-4305-a432-91ccca503e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha after tuning:  0.001\n",
      "Cross Entropy: 8.737\n",
      "perplexity: 426.562\n"
     ]
    }
   ],
   "source": [
    "HC = []\n",
    "perplexity = []\n",
    "\n",
    "min_entropy = 10000\n",
    "min_index = 0\n",
    "alpha_values = np.linspace(0.001, 1)\n",
    "best_alpha_bigram = 0\n",
    "for i, alpha in enumerate(alpha_values):\n",
    "    sum_prob = 0\n",
    "    bi_count = 0\n",
    "    for sentence in development_set:\n",
    "        sentence = ['<s>']  + sentence + ['<e>']\n",
    "        for i in range(1, len(sentence)):\n",
    "            bi_prob = prob_bigram_model(sentence[i - 1] , sentence[i], alpha, vocabulary, uni_counter, bi_counter)\n",
    "            sum_prob += math.log2(bi_prob)\n",
    "            bi_count +=1\n",
    "        \n",
    "    HC.append(-sum_prob / bi_count)\n",
    "    perplexity.append(math.pow(2, -sum_prob / bi_count))\n",
    "    if ((-sum_prob / bi_count) < min_entropy ):\n",
    "        min_entropy = -sum_prob / bi_count\n",
    "        min_index = i\n",
    "        best_alpha_bigram = alpha\n",
    "\n",
    "print(\"Best alpha after tuning: \", best_alpha_bigram)\n",
    "print(\"Cross Entropy: {0:.3f}\".format(HC[min_index]))\n",
    "print(\"perplexity: {0:.3f}\".format(perplexity[min_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ef72c-dbfc-4cec-bc1a-a77d0fe832cd",
   "metadata": {},
   "source": [
    "## Tri-gram LM Cross entropy & perplexity\n",
    "\n",
    "### $ P(w_3|w_1,w_2) = \\frac{C(w_1,w_2,w_3) + \\alpha}{C(w_1,w_2) + \\alpha \\cdot |V|} $\n",
    "\n",
    "* $ C(w_1,w_2,w_3) $ : trigram count\n",
    "* $ C(w_1,w_2) $ : bigram count\n",
    "* $ 0 \\leq\\alpha \\leq1 $ :  smoothing hyper-parameter\n",
    "* |V|: vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c799679-4840-4743-98a5-c99740090a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha after tuning:  0.042\n",
      "Cross Entropy: 12.318\n",
      "perplexity: 5104.465\n"
     ]
    }
   ],
   "source": [
    "HC = []\n",
    "perplexity = []\n",
    "\n",
    "min_entropy = 10000\n",
    "min_index = 0\n",
    "alpha_values = np.linspace(0.001, 1)\n",
    "best_alpha_trigram = 0\n",
    "for i, alpha in enumerate(alpha_values):\n",
    "    sum_prob = 0\n",
    "    tri_count = 0\n",
    "    for sentence in development_set:\n",
    "        sentence = ['<s>']  + sentence + ['<e>']\n",
    "        for i in range(2, len(sentence)):\n",
    "            tri_prob = prob_trigram_model(sentence[i - 2] , sentence[i - 1], sentence[i], alpha, vocabulary, bi_counter, tri_counter)\n",
    "            sum_prob += math.log2(tri_prob)\n",
    "            tri_count +=1\n",
    "        \n",
    "    HC.append(-sum_prob / tri_count)\n",
    "    perplexity.append(math.pow(2, -sum_prob / tri_count))\n",
    "    if ((-sum_prob / tri_count) < min_entropy ):\n",
    "        min_entropy = -sum_prob / tri_count\n",
    "        min_index = i\n",
    "        best_alpha_trigram = alpha\n",
    "\n",
    "print(\"Best alpha after tuning: \", round(best_alpha_trigram,3))\n",
    "print(\"Cross Entropy: {0:.3f}\".format(HC[min_index]))\n",
    "print(\"perplexity: {0:.3f}\".format(perplexity[min_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f62e2d9-1ea3-4935-9d0b-735e7c37c21e",
   "metadata": {},
   "source": [
    "Now, let's test the performance in the test set, after having defined the optimal alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61544d78-1a77-4e8d-a0c8-a441fe4b667f",
   "metadata": {},
   "source": [
    "1. Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3b5b2f4-89b6-4df2-9fcd-6ed26d6abc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Cross Entropy: 2.726\n",
      "Language perplexity: 6.617\n"
     ]
    }
   ],
   "source": [
    "lang_sequence = []\n",
    "for sentence in test_set:\n",
    "    lang_sequence += ['<s>']  + sentence + ['<e>']\n",
    "sum_prob = 0\n",
    "bi_count = 0\n",
    "for i in range(1, len(lang_sequence)):\n",
    "    if lang_sequence[i] != '<s>' :\n",
    "        bi_prob = prob_bigram_model(lang_sequence[i - 1] , lang_sequence[i], best_alpha_bigram, vocabulary, uni_counter, bi_counter)\n",
    "        sum_prob += math.log2(bi_prob)\n",
    "        bi_count +=1\n",
    "            \n",
    "HC = -sum_prob / bi_count\n",
    "perplexity = math.pow(2, -sum_prob / bi_count)\n",
    "    \n",
    "print(\"Language Cross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"Language perplexity: {0:.3f}\".format(perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913cbd2d-9423-4fe0-8848-2df57147c0bf",
   "metadata": {},
   "source": [
    "2. Trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac162230-bb55-4039-a474-5f7bba182f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Cross Entropy: 10.558\n",
      "Language perplexity: 1507.716\n"
     ]
    }
   ],
   "source": [
    "lang_sequence = []\n",
    "for sentence in test_set:\n",
    "    lang_sequence += ['<s>']  + sentence + ['<e>']\n",
    "sum_prob = 0\n",
    "tri_count = 0\n",
    "for i in range(2, len(lang_sequence)):\n",
    "    if lang_sequence[i] != '<s>' :\n",
    "        tri_prob = prob_trigram_model(lang_sequence[i - 2] ,lang_sequence[i - 1], lang_sequence[i], best_alpha_trigram, vocabulary, bi_counter, tri_counter)\n",
    "        sum_prob += math.log2(tri_prob)\n",
    "        tri_count +=1\n",
    "            \n",
    "HC = -sum_prob / tri_count\n",
    "perplexity = math.pow(2, -sum_prob / tri_count)\n",
    "    \n",
    "print(\"Language Cross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"Language perplexity: {0:.3f}\".format(perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75603bb8-21ee-469b-bf12-85665e9094b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 5\n",
    "beam_width = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a8c72-e1a8-44dc-8338-266bbf419ee2",
   "metadata": {},
   "source": [
    "v. Create a fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c7a057e-c101-4158-a412-39c85dd6ba66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: she was the youngest of the two daughters of a most affectionate  indulgent father  and had  in consequence of her sister's marriage  been mistress of his house from a very early period.\n",
      "Corrupted: sne was the youngest of the tvo daugnterc of a most affectionate  indulgent fatnet  amd had  in consequence of her sister's merriage  been mictress of his houce trom a very eatiy period.\n",
      "\n",
      "---\n",
      "\n",
      "Original: her mother had died too long ago for her to have more than an indistinct remembrance of her caresses  and her place had been supplied by an excellent woman as governess  who had fallen little short of a mother in affection.\n",
      "Corrupted: her mother had died tuo long ago for her to nave more than am indlctinst remembrance of het cerasses  and har blaca hab baen supplied py an excellant woman es joverness  who hab falien liftle short of a mothar in affection.\n",
      "\n",
      "---\n",
      "\n",
      "Original: sixteen years had miss taylor been in mr. woodhouse's family  less as a governess than a friend  very fond of both daughters  but particularly of emma.\n",
      "Corrupted: sixteem years had miss tayior baem in mr. woodnouse's familv  less as a govarness than a friend  vety fonb of both daughters  put particularly of emna.\n",
      "\n",
      "---\n",
      "\n",
      "Original: between _them_ it was more the intimacy of sisters.\n",
      "Corrupted: between _then_ it was more the intimacy of sisters.\n",
      "\n",
      "---\n",
      "\n",
      "Original: even before miss taylor had ceased to hold the nominal office of governess  the mildness of her temper had hardly allowed her to impose any restraint  and the shadow of authority being now long passed away  they had been living together as friend and friend very mutually attached  and emma doing just what she liked  highly esteeming miss taylor's judgment  but directed chiefly by her own.\n",
      "Corrupted: evem before miss taylot had ceased to hold the nominal office of governess  the milbmess uf her femper had nardly aliowab har to impose amy restraint  and the snadow of aufhotify beinj now iong passad away  thev hed been living together as friend amb friand vary mutualiy attached  and emma doing just what she liked  highly esteeming miss taylor's judgment  but ditested chieflv by her own.\n",
      "\n",
      "---\n",
      "\n",
      "Original: the real evils  indeed  of emma's situation were the power of having rather too much her own way  and a disposition to think a little too well of herself  these were the disadvantages which threatened alloy to her many enjoyments.\n",
      "Corrupted: the real evils  lndeed  ot emma's sifuation were the powet of havlng rather too much her own way  and a disbosition to thimx a little too well of hercelt  these were the disadwanfages wnich threatemed alloy to net mamy enjoyments.\n",
      "\n",
      "---\n",
      "\n",
      "Original: the danger  however  was at present so unperceived  that they did not by any means rank as misfortunes with her.\n",
      "Corrupted: tha dangar  however  was et present so unperceived  thet they did not by any means rank as misfortunes witn her.\n",
      "\n",
      "---\n",
      "\n",
      "Original: sorrow came  a gentle sorrow  but not at all in the shape of any disagreeable consciousness.\n",
      "Corrupted: sortow ceme  a jentle surrow  put not at ail in the shepe of any disegreeable sonsclousness.\n",
      "\n",
      "---\n",
      "\n",
      "Original: miss taylor married.\n",
      "Corrupted: miss taylor married.\n",
      "\n",
      "---\n",
      "\n",
      "Original: it was miss taylor's loss which first brought grief.\n",
      "Corrupted: it was miss taylor's loss which first broughf griaf.\n",
      "\n",
      "---\n",
      "\n",
      "Original: it was on the wedding day of this beloved friend that emma first sat in mournful thought of any continuance.\n",
      "Corrupted: it was on tne vebding dey ot thic belovab friend that emma first sat in mournful thought uf amy continuance.\n",
      "\n",
      "---\n",
      "\n",
      "Original: the wedding over  and the bride people gone  her father and herself were left to dine together  with no prospect of a third to cheer a long evening.\n",
      "Corrupted: the wedbing over  and the btide paople gune  her father and herself were left tu dima together  with no pruspest of e third to cheet a iong evening.\n",
      "\n",
      "---\n",
      "\n",
      "Original: her father composed himself to sleep after dinner  as usual  and she had then only to sit and think of what she had lost.\n",
      "Corrupted: her fether composed himself tu sleep after dinner  as ucual  and sna hed then oniv to sit and thlnx of what she hab locf.\n",
      "\n",
      "---\n",
      "\n",
      "Original: the event had every promise of happiness for her friend.\n",
      "Corrupted: the ewent had every prumise of happinesc for her triend.\n",
      "\n",
      "---\n",
      "\n",
      "Original: mr. weston was a man of unexceptionable character  easy fortune  suitable age  and pleasant manners  and there was some satisfaction in considering with what self denying  generous friendship she had always wished and promoted the match  but it was a black morning's work for her.\n",
      "Corrupted: mr. vestun was a man of unexceptioneble character  easy fortune  suitable age  end pieasant manmers  amd there was sone safisfaction in sonsidering with what self danying  generouc ftiendship che hed always vished end prumofeb the match  but it was a black morning's work for her.\n",
      "\n",
      "---\n",
      "\n",
      "Original: the want of miss taylor would be felt every hour of every day.\n",
      "Corrupted: the want of miss faylor woulb be felt every hour ot every day.\n",
      "\n",
      "---\n",
      "\n",
      "Original: she recalled her past kindness  the kindness  the affection of sixteen years  how she had taught and how she had played with her from five years old  how she had devoted all her powers to attach and amuse her in health  and how nursed her through the various illnesses of childhood.\n",
      "Corrupted: she recailab het past kindness  the kindness  the affection of sixteen vears  how che had faughf and how she had playad with her from flve years old  how she had devoted all her powerc to attech and emuse her in healfh  anb how nutsed her fhrough the variouc illnesses of cnildhood.\n",
      "\n",
      "---\n",
      "\n",
      "Original: a large debt of gratitude was owing here  but the intercourse of the last seven years  the equal footing and perfect unreserve which had soon followed isabella's marriage  on their being left to each other  was yet a dearer  tenderer recollection.\n",
      "Corrupted: a large debf of jratitude wes owing hare  but fhe lntercoutse of the last seven years  the equal fuoting and perfect unreserve vhich had coon tollowed isabella's marriege  om their being left to each uther  was yet a dearar  tendaret recollection.\n",
      "\n",
      "---\n",
      "\n",
      "Original: she had been a friend and companion such as few possessed  intelligent  well informed  useful  gentle  knowing all the ways of the family  interested in all its concerns  and peculiarly interested in herself  in every pleasure  every scheme of hers  one to whom she could speak every thought as it arose  and who had such an affection for her as could never find fault.\n",
      "Corrupted: she had baen a ftiend and conpanium such as few possesseb  intelligant  weil informed  useful  gantia  kmowimg ali the wavs of fhe family  inferestad in all its concerns  and peculiarlv interested in herself  im ewery bleacure  every ccneme ot nars  one to whom sha could speak every thought as it arose  and vho had cuch an affecfiun for her as coulb never fimd fault.\n",
      "\n",
      "---\n",
      "\n",
      "Original: how was she to bear the change?\n",
      "Corrupted: how was cne fo bear the snanje?\n",
      "\n",
      "---\n",
      "\n",
      "Original: it was true that her friend was going only half a mile from them  but emma was aware that great must be the difference between a mrs. weston  only half a mile from them  and a miss taylor in the house  and with all her advantages  natural and domestic  she was now in great danger of suffering from intellectual solitude.\n",
      "Corrupted: it was true that her triend was goimg only haif e mile trom them  but emma was aware that great must be fhe diffarence between a nrs. westom  only half a mile ftom them  and a mlss faylor in the house  and vith ail her advantages  natural and domesfic  she was nuw in great banger ot suffering from intellectual soiitude.\n",
      "\n",
      "---\n",
      "\n",
      "Original: she dearly loved her father  but he was no companion for her.\n",
      "Corrupted: she dearly loveb her father  but he was nu companion for nat.\n",
      "\n",
      "---\n",
      "\n",
      "Original: he could not meet her in conversation  rational or playful.\n",
      "Corrupted: he could not neef har in conwarsation  tatlonal ot playful.\n",
      "\n",
      "---\n",
      "\n",
      "Original: the evil of the actual disparity in their ages  and mr. woodhouse had not married early  was much increased by his constitution and habits  for having been a valetudinarian all his life  without activity of mind or body  he was a much older man in ways than in years  and though everywhere beloved for the friendliness of his heart and his amiable temper  his talents could not have recommended him at any time.\n",
      "Corrupted: the evll of the astual disparity in theit ages  and mr. voodhouse had nof marrieb eariy  was much incteased by hls sonctitufion amd habits  for havinj been a valetudinarian ell his iife  without ecfivity ot mind or body  ne was a nuch older men in ways than in years  end though everywhere beioved for tne friendlinecs of his heart amb hls amiabla tamper  his talenfs could not hawe recomnended hin at any time.\n",
      "\n",
      "---\n",
      "\n",
      "Original: her sister  though comparatively but little removed by matrimony  being settled in london  only sixteen miles off  was much beyond her daily reach  and many a long october and november evening must be struggled through at hartfield  before christmas brought the next visit from isabella and her husband  and their little children  to fill the house  and give her pleasant society again.\n",
      "Corrupted: het sister  though comperatively buf littla removed bv matrimony  being setflad im london  onlv cixfeen miles oft  was much bavond her daily reech  amb mamy a long october and nowember evening must be strugglab throogh at nartfield  before chtistmas brooght tha mext visit from isabelia and her husband  and fneir iitfle children  to fill the house  and give her pleasant sosiety again.\n",
      "\n",
      "---\n",
      "\n",
      "Original: highbury  the large and populous village  almost amounting to a town  to which hartfield  in spite of its separate lawn  and shrubberies  and name  did really belong  afforded her no equals.\n",
      "Corrupted: highbury  tne larja and populous village  aimust amountlnj to a tovn  to vhich hartfield  im spite of its separafe lawn  anb shrobbarles  and name  did really belong  afforded her no equals.\n",
      "\n",
      "---\n",
      "\n",
      "Original: the woodhouses were first in consequence there.\n",
      "Corrupted: the woudnouses were first in consaquence there.\n",
      "\n",
      "---\n",
      "\n",
      "Original: all looked up to them.\n",
      "Corrupted: ell louked up fu them.\n",
      "\n",
      "---\n",
      "\n",
      "Original: she had many acquaintance in the place  for her father was universally civil  but not one among them who could be accepted in lieu of miss taylor for even half a day.\n",
      "Corrupted: she hed many acquaintamce in the place  for her father wac oniversally clvil  but not one among them who could ba accepted in lleu uf niss taylor tur even helf a day.\n",
      "\n",
      "---\n",
      "\n",
      "Original: it was a melancholy change  and emma could not but sigh over it  and wish for impossible things  till her father awoke  and made it necessary to be cheerful.\n",
      "Corrupted: it was a melamchoiy chanje  and amma could not bot sigh ovet it  amd wisn for lnposslble thimgs  till her fathat awoke  and mede it necassarv to be cheerful.\n",
      "\n",
      "---\n",
      "\n",
      "Original: his spirits required support.\n",
      "Corrupted: his cbirifs requireb supbort.\n",
      "\n",
      "---\n",
      "\n",
      "Original: he was a nervous man  easily depressed  fond of every body that he was used to  and hating to part with them  hating change of every kind.\n",
      "Corrupted: he was a nervous mam  eesily depressed  fonb of every budy that he was used to  amd hatlnj to parf with then  hatlmg change uf every xinb.\n",
      "\n",
      "---\n",
      "\n",
      "Original: matrimony  as the origin of change  was always disagreeable  and he was by no means yet reconciled to his own daughter's marrying  nor could ever speak of her but with compassion  though it had been entirely a match of affection  when he was now obliged to part with miss taylor too  and from his habits of gentle selfishness  and of being never able to suppose that other people could feel differently from himself  he was very much disposed to think miss taylor had done as sad a thing for herself as for them  and would have been a great deal happier if she had spent all the rest of her life at hartfield.\n",
      "Corrupted: matrimony  as the origin of chamge  was alvays dlsagreeable  anb he was py no means yet reconciled to his own daughter's marrying  mot could evat speek uf her but with compasciom  though it had peen emtltely a match of affecfion  when ha was nov obliged to patt with mlsc taylor too  and from his habits of gantle selfishness  and of baing never aple tu soppose tnat othet people could teel dlfferanfly frum himself  ne was vety mush disposed to think miss tayior had dune as sad a fhing for nersalf as tor them  and would have been a great deal happler it she had cpent all tha resf of her life et hartfiald.\n",
      "\n",
      "---\n",
      "\n",
      "Original: emma smiled and chatted as cheerfully as she could  to keep him from such thoughts  but when tea came  it was impossible for him not to say exactly as he had said at dinner    poor miss taylor!\n",
      "Corrupted: emma sniled and chetteb as sheerfully as she could  to xeeb him ttom such thoughts  but vnen tea came  it was imposciple for him not to sav exactly as ha had said af dinner    poot miss tavlur!\n",
      "\n",
      "---\n",
      "\n",
      "Original: i wish she were here again.\n",
      "Corrupted: i wlsh she were hera again.\n",
      "\n",
      "---\n",
      "\n",
      "Original: what a pity it is that mr. weston ever thought of her!\n",
      "Corrupted: what a pify it is that mr. weston awer thought of het!\n",
      "\n",
      "---\n",
      "\n",
      "Original: i cannot agree with you  papa  you know i cannot.\n",
      "Corrupted: i cannot ajree wlth you  pepa  you knuw i cannot.\n",
      "\n",
      "---\n",
      "\n",
      "Original: mr. weston is such a good humoured  pleasant  excellent man  that he thoroughly deserves a good wife   and you would not have had miss taylor live with us for ever  and bear all my odd humours  when she might have a house of her own?\n",
      "Corrupted: mr. wectun is such a good humoured  pleasanf  ekcellent mam  that ne thoroughly deserves a jood wife   and yoo would not have had niss fayior liva with us tor evar  and bear all ny odb humourc  vhen she night have a house of har uvn?\n",
      "\n",
      "---\n",
      "\n",
      "Original: a house of her own!\n",
      "Corrupted: a house ut het uwn!\n",
      "\n",
      "---\n",
      "\n",
      "Original: but where is the advantage of a house of her own?\n",
      "Corrupted: but where is the abventage of a huuse of her own?\n",
      "\n",
      "---\n",
      "\n",
      "Original: this is three times as large.\n",
      "Corrupted: this is three tlmec as large.\n",
      "\n",
      "---\n",
      "\n",
      "Original: and you have never any odd humours  my dear.\n",
      "Corrupted: and you have never any odd humours  my daar.\n",
      "\n",
      "---\n",
      "\n",
      "Original: how often we shall be going to see them  and they coming to see us!\n",
      "Corrupted: how often we shall be going fo sea them  and they comlng fo see us!\n",
      "\n",
      "---\n",
      "\n",
      "Original: we shall be always meeting!\n",
      "Corrupted: we shall pe elwayc meeting!\n",
      "\n",
      "---\n",
      "\n",
      "Original: _we_ must begin  we must go and pay wedding visit very soon.\n",
      "Corrupted: _we_ must begin  we nost jo and bay veddlng visit very soon.\n",
      "\n",
      "---\n",
      "\n",
      "Original: my dear  how am i to get so far?\n",
      "Corrupted: my dear  how am i fo get so fer?\n",
      "\n",
      "---\n",
      "\n",
      "Original: randalls is such a distance.\n",
      "Corrupted: randalls is cush a distenca.\n",
      "\n",
      "---\n",
      "\n",
      "Original: i could not walk half so far.\n",
      "Corrupted: i could not waik helt so far.\n",
      "\n",
      "---\n",
      "\n",
      "Original: no  papa  nobody thought of your walking.\n",
      "Corrupted: no  paba  nobody thought of your walking.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def corrupt_sentence(sentence, probability):\n",
    "    corrupted_sentence = \"\"\n",
    "    for char in sentence:\n",
    "        if char != ' ' and random.random() < probability:\n",
    "            \n",
    "            corrupted_sentence += get_similar_char(char)                 #replace with a similar character\n",
    "        else:\n",
    "            corrupted_sentence += char\n",
    "    return corrupted_sentence\n",
    "\n",
    "def get_similar_char(char):\n",
    "    \n",
    "    similar_chars = {\n",
    "        'a': 'e',\n",
    "        'b': 'p',\n",
    "        'c': 's',\n",
    "        'd': 'b',\n",
    "        'e': 'a',\n",
    "        'f': 't',\n",
    "        'g': 'j',\n",
    "        'h': 'n',\n",
    "        'i': 'l',\n",
    "        'j': 'g',\n",
    "        'k': 'x',\n",
    "        'l': 'i',\n",
    "        'm': 'n',\n",
    "        'n': 'm',\n",
    "        'o': 'u',\n",
    "        'p': 'b',\n",
    "        'q': 'g',\n",
    "        'r': 't',\n",
    "        's': 'c',\n",
    "        't': 'f',\n",
    "        'u': 'o',\n",
    "        'v': 'w',\n",
    "        'w': 'v',\n",
    "        'x': 'k',\n",
    "        'y': 'v',\n",
    "        'z': 's',\n",
    "    }\n",
    "\n",
    "    \n",
    "    return similar_chars.get(char, char)                            #return a randomly chosen character\n",
    "\n",
    "'''\n",
    "test_corpus = [\"he plays football\",\n",
    "               \"he plais footbal\",\n",
    "               \"she enjoys good football\",\n",
    "               \"she plays good music\",\n",
    "               \"he prays to god\",\n",
    "               \"please buy me the other ball\",\n",
    "               \"he pleases the other players by playing good football\",\n",
    "               \"he plys god ftball\"]\n",
    "\n",
    "'''\n",
    "probability = 0.1                                         #probability of character replacement\n",
    "\n",
    "\n",
    "corrupted_corpus = [corrupt_sentence(sentence, probability) for sentence in sentences[1:50]]          #generate the corrupted corpus\n",
    "\n",
    "\n",
    "for original, corrupted in zip(sentences[1:50], corrupted_corpus):\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Corrupted: {corrupted}\")\n",
    "    print(\"\\n---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97003c75-b55e-43ec-8867-ff6b28ab6e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['but', 'i', 'say', 'unto', 'you', 'that', 'elias', 'is', 'indeed', 'come', 'and', 'they', 'have', 'done', 'unto', 'him', 'whatsoever', 'they', 'UNK', 'as', 'it', 'is', 'written', 'of', 'him', '.'], ['but', 'soon', 'the', 'fear', 'of', 'being', 'too', 'late', 'at', 'the', 'UNK', 'meeting', 'began', 'to', 'balance', 'the', 'dread', 'of', 'appearing', 'in', 'his', 'stained', 'UNK', 'and', 'he', 'now', 'as', 'anxiously', 'repeated', 'whilst', 'the', 'woman', 'held', 'the', 'wet', 'coat', 'to', 'the', 'fire', 'oh', 'i', 'shall', 'be', 'too', 'late', 'indeed', 'i', 'shall', 'be', 'too', 'late', 'make', 'haste', 'it', 'will', 'never', 'dry', 'hold', 'it', 'nearer', 'nearer', 'to', 'the', 'fire', '.'], ['burs', '.'], ['to', 'do', 'justice', 'and', 'judgment', 'is', 'more', 'acceptable', 'to', 'the', 'lord', 'than', 'sacrifice', '.'], ['the', 'pequod', \"'s\", 'UNK', 'were', 'pointed', 'and', 'breaking', 'up', 'the', 'UNK', 'circle', 'she', 'UNK', 'parted', 'the', 'white', 'whale', 'from', 'his', 'UNK', '.'], ['and', 'as', 'we', 'have', 'borne', 'the', 'image', 'of', 'the', 'UNK', 'we', 'shall', 'also', 'bear', 'the', 'image', 'of', 'the', 'heavenly', '.'], ['UNK', '.'], [\"'\", 'i', 'did', \"n't\", 'know', 'it', 'was', 'your', 'table', \"'\", 'said', 'alice', \"'it\", \"'s\", 'laid', 'for', 'a', 'great', 'many', 'more', 'than', 'three', '.', \"'\"], ['if', 'the', 'men', 'of', 'my', 'tabernacle', 'said', 'not', 'oh', 'that', 'we', 'had', 'of', 'his', 'flesh', '!'], ['UNK', '!']]\n"
     ]
    }
   ],
   "source": [
    "print(new_corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04032172-854e-4616-85e8-b9be2bf51a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_UNK_sentences = [sent_tokenize(' '.join(sentence)) for sentence in new_corpus]       #get the sentences that include UNK values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a7427fe-087f-48eb-8441-d4d506ef53ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['but soon the fear of being too late at the UNK meeting began to balance the dread of appearing in his stained UNK and he now as anxiously repeated whilst the woman held the wet coat to the fire oh i shall be too late indeed i shall be too late make haste it will never dry hold it nearer nearer to the fire .'],\n",
       " ['burs .'],\n",
       " ['to do justice and judgment is more acceptable to the lord than sacrifice .'],\n",
       " [\"the pequod 's UNK were pointed and breaking up the UNK circle she UNK parted the white whale from his UNK .\"]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_UNK_sentences[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74fc0e10-f718-42e4-9f04-86bcf1fcec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "tweet_wt = TweetTokenizer()\n",
    "tokenized = [tweet_wt.tokenize(' '.join(sentence)) for sentence in tokenized_UNK_sentences[:50]]  # Get the first 50 sentences\n",
    "model = BigramModel(alpha=0.01)\n",
    "model.fit(tokenized)                                # model is fitted with the correct and tokenized words\n",
    "\n",
    "corrupted_tokenized = [tweet_wt.tokenize(sentence) for sentence in corrupted_corpus]             # tokenize the corrupted sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6779be16-f60e-4fbf-a336-c0e2dad91458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentences: [\"she was the youngest of the two daughters of a most affectionate  indulgent father  and had  in consequence of her sister's marriage  been mistress of his house from a very early period.\", 'her mother had died too long ago for her to have more than an indistinct remembrance of her caresses  and her place had been supplied by an excellent woman as governess  who had fallen little short of a mother in affection.', \"sixteen years had miss taylor been in mr. woodhouse's family  less as a governess than a friend  very fond of both daughters  but particularly of emma.\"]\n",
      "\n",
      "\n",
      "Corrupted(wrong) sentences: [\"sne was the youngest of the tvo daugnterc of a most affectionate  indulgent fatnet  amd had  in consequence of her sister's merriage  been mictress of his houce trom a very eatiy period.\", 'her mother had died tuo long ago for her to nave more than am indlctinst remembrance of het cerasses  and har blaca hab baen supplied py an excellant woman es joverness  who hab falien liftle short of a mothar in affection.', \"sixteem years had miss tayior baem in mr. woodnouse's familv  less as a govarness than a friend  vety fonb of both daughters  put particularly of emna.\"]\n",
      "\n",
      "\n",
      "Final result (corrected sentences): [['<start>', 'but', 'i', 'say', 'unto', 'you'], ['<start>', 'but', 'i', 'say', 'unto', 'you'], ['<start>', 'but', 'i', 'say', 'unto', 'you']]\n"
     ]
    }
   ],
   "source": [
    "corrected = []\n",
    "corrector = BigramSpellCorrector(model, lamda1=0.5, lamda2=-0.5)\n",
    "for sent in corrupted_tokenized:\n",
    "  output_seq = corrector.spell_correct(original_tokenized_sentence=sent, max_depth = 5, beam_width = 3)  #give the corrupt sentences to spell correct\n",
    "  corrected.append(output_seq)\n",
    "    \n",
    "print('Original sentences:', sentences[1:4])\n",
    "print('\\n')\n",
    "print('Corrupted(wrong) sentences:', corrupted_corpus[:3])\n",
    "print('\\n')\n",
    "print('Final result (corrected sentences):', corrected[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0df89467-dcab-483e-92eb-05aafc129ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence ['but', 'i', 'say', 'unto', 'you', 'that', 'elias', 'is', 'indeed', 'come', 'and', 'they', 'have', 'done', 'unto', 'him', 'whatsoever', 'they', 'UNK', 'as', 'it', 'is', 'written', 'of', 'him', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['but', 'soon', 'the', 'fear', 'of', 'being', 'too', 'late', 'at', 'the', 'UNK', 'meeting', 'began', 'to', 'balance', 'the', 'dread', 'of', 'appearing', 'in', 'his', 'stained', 'UNK', 'and', 'he', 'now', 'as', 'anxiously', 'repeated', 'whilst', 'the', 'woman', 'held', 'the', 'wet', 'coat', 'to', 'the', 'fire', 'oh', 'i', 'shall', 'be', 'too', 'late', 'indeed', 'i', 'shall', 'be', 'too', 'late', 'make', 'haste', 'it', 'will', 'never', 'dry', 'hold', 'it', 'nearer', 'nearer', 'to', 'the', 'fire', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['burs', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['to', 'do', 'justice', 'and', 'judgment', 'is', 'more', 'acceptable', 'to', 'the', 'lord', 'than', 'sacrifice', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['the', 'pequod', \"'\", 's', 'UNK', 'were', 'pointed', 'and', 'breaking', 'up', 'the', 'UNK', 'circle', 'she', 'UNK', 'parted', 'the', 'white', 'whale', 'from', 'his', 'UNK', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['and', 'as', 'we', 'have', 'borne', 'the', 'image', 'of', 'the', 'UNK', 'we', 'shall', 'also', 'bear', 'the', 'image', 'of', 'the', 'heavenly', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['UNK', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence [\"'\", 'i', 'did', \"n't\", 'know', 'it', 'was', 'your', 'table', \"'\", 'said', 'alice', \"'\", 'it', \"'\", 's', 'laid', 'for', 'a', 'great', 'many', 'more', 'than', 'three', '.', \"'\"]\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['if', 'the', 'men', 'of', 'my', 'tabernacle', 'said', 'not', 'oh', 'that', 'we', 'had', 'of', 'his', 'flesh', '!']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['UNK', '!']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['if', 'you', 'knew', 'how', 'much', 'i', 'love', 'every', 'thing', 'that', 'is', 'decided', 'and', 'open', '!']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['book', 'i', '.', 'UNK', 'chapter', 'iii', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['your', 'farm', 'profits', 'crops', 'to', 'think', 'how', 'UNK', \"'\", 'd', 'you', 'are', 'to', 'think', 'there', 'will', 'still', 'be', 'farms', 'profits', 'crops', 'yet', 'for', 'you', 'of', 'what', 'avail', '?']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['and', 'what', 'will', 'make', 'it', 'worth', 'your', 'while', 'to', 'let', 'it', 'alone', '?']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['ay', 'that', 'i', 'will', 'with', 'a', 'deal', 'of', 'pleasure', 'for', 'you', 'be', 'a', 'civil', 'spoken', 'young', 'gentleman', 'and', 'besides', 'i', 'do', \"n't\", 'think', 'the', 'worse', 'on', 'you', 'for', 'being', 'UNK', 'a', 'little', 'about', 'your', 'mother', 'being', 'what', 'i', 'might', 'ha', \"'\", 'been', 'at', 'your', 'age', 'myself', 'for', 'i', 'had', 'a', 'mother', 'myself', 'once', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['margery', 'could', 'hardly', 'wait', 'to', 'open', 'it', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['replied', 'her', 'mother', 'with', 'a', 'smile', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['archer', 'immediately', 'began', 'to', 'talk', 'of', 'the', 'bill', 'and', 'throwing', 'down', 'a', 'guinea', 'and', 'a', 'half', 'the', 'UNK', 'carpenter', 'UNK', 'the', 'money', 'directly', 'and', 'made', 'his', 'bow', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['but', 'i', 'as', 'a', 'deaf', 'man', 'heard', 'not', 'and', 'i', 'was', 'as', 'a', 'dumb', 'man', 'that', 'openeth', 'not', 'his', 'mouth', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['your', 'UNK', 'of', 'UNK', 'that', 'were', 'wont', 'to', 'set', 'the', 'table', 'on', 'a', 'UNK', '?']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['because', 'if', 'you', 'are', 'i', 'shall', 'have', 'no', 'scruple', 'in', 'asking', 'you', 'to', 'take', 'my', 'place', 'and', 'give', 'anne', 'your', 'arm', 'to', 'her', 'father', \"'\", 's', 'door', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['and', 'as', 'moses', 'lifted', 'up', 'the', 'serpent', 'in', 'the', 'wilderness', 'even', 'so', 'must', 'the', 'son', 'of', 'man', 'be', 'lifted', 'up', 'that', 'whosoever', 'believeth', 'in', 'him', 'should', 'not', 'perish', 'but', 'have', 'eternal', 'life', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['nothing', 'was', 'done', 'and', 'nothing', 'seemed', 'capable', 'of', 'being', 'done', 'those', 'on', 'deck', 'rushed', 'towards', 'the', 'bows', 'and', 'stood', 'eyeing', 'the', 'UNK', 'as', 'if', 'it', 'were', 'the', 'lower', 'jaw', 'of', 'an', 'UNK', 'whale', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['to', 'see', 'me', 'do', 'what', '?']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['are', 'there', 'not', 'UNK', 'with', 'me', '?']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['then', 'UNK', 'the', 'king', 'of', 'tyre', 'answered', 'in', 'writing', 'which', 'he', 'sent', 'to', 'solomon', 'because', 'the', 'lord', 'hath', 'loved', 'his', 'people', 'he', 'hath', 'made', 'thee', 'king', 'over', 'them', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['i', 'took', 'his', 'advice', 'and', 'have', 'never', 'regretted', 'it', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['violet', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['and', 'without', 'love', 'i', 'am', 'sure', 'i', 'should', 'be', 'a', 'fool', 'to', 'change', 'such', 'a', 'situation', 'as', 'mine', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['he', 'should', 'have', 'UNK', 'even', 'unreasonable', 'scruples', 'had', 'there', 'been', 'such', 'but', 'hers', 'were', 'all', 'reasonable', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['i', 'have', 'a', 'very', 'sincere', 'interest', 'in', 'emma', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['she', 'spoke', 'but', 'no', 'beef', 'appeared', 'till', 'franklin', 'with', 'a', 'look', 'of', 'sudden', 'recollection', 'cried', 'did', 'not', 'i', 'see', 'something', 'like', 'a', 'piece', 'of', 'beef', 'in', 'a', 'basket', 'in', 'the', 'UNK', '?']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['for', 'as', 'the', 'father', 'hath', 'life', 'in', 'himself', 'so', 'hath', 'he', 'given', 'to', 'the', 'son', 'to', 'have', 'life', 'in', 'himself', 'and', 'hath', 'given', 'him', 'authority', 'to', 'execute', 'judgment', 'also', 'because', 'he', 'is', 'the', 'son', 'of', 'man', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['the', 'UNK', 'of', 'the', 'valley', 'shall', 'be', 'sweet', 'unto', 'him', 'and', 'every', 'man', 'shall', 'draw', 'after', 'him', 'as', 'there', 'are', 'innumerable', 'before', 'him', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['to', 'be', 'sure', 'cried', 'emma', 'it', 'is', 'always', 'UNK', 'to', 'a', 'man', 'that', 'a', 'woman', 'should', 'ever', 'refuse', 'an', 'offer', 'of', 'marriage', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['and', 'they', 'took', 'away', 'their', 'cattle', 'of', 'their', 'camels', 'fifty', 'thousand', 'and', 'of', 'sheep', 'two', 'hundred', 'and', 'fifty', 'thousand', 'and', 'of', 'asses', 'two', 'thousand', 'and', 'of', 'men', 'an', 'hundred', 'thousand', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['but', 'listen', 'not', 'to', 'his', 'temptations', 'warn', 'thy', 'UNK', 'let', 'it', 'profit', 'thee', 'to', 'have', 'heard', 'by', 'terrible', 'example', 'the', 'reward', 'of', 'disobedience', 'firm', 'they', 'might', 'have', 'stood', 'yet', 'fell', 'remember', 'and', 'fear', 'to', 'transgress', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['and', 'elisha', 'died', 'and', 'they', 'buried', 'him', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['and', 'the', 'eyes', 'of', 'them', 'that', 'see', 'shall', 'not', 'be', 'dim', 'and', 'the', 'ears', 'of', 'them', 'that', 'hear', 'shall', 'hearken', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['and', 'joram', 'said', 'make', 'ready', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['i', \"'\", 'm', 'convinced', 'mr', '.', 'and', 'mrs', '.', 'montague', 'will', 'find', 'themselves', 'obliged', 'to', 'stay', 'out', 'another', 'day', 'and', 'i', 'so', 'long', 'to', 'show', 'you', 'off', 'to', 'her', 'ladyship', 'and', 'your', 'doctor', 'carbuncle', 'and', 'your', 'counsellor', 'puff', 'and', 'your', 'miss', 'UNK', 'and', 'all', 'your', 'charming', 'characters', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['UNK', 'as', 'i', 'chant', 'lo', '!']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['and', 'yesterday', 'i', 'talked', 'the', 'same', 'to', 'starbuck', 'there', 'concerning', 'my', 'broken', 'boat', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['the', 'UNK', 'men', 'women', 'and', 'children', 'who', 'had', 'come', 'with', 'their', 'UNK', 'to', 'draw', 'water', 'at', 'this', 'well', 'were', 'held', 'at', 'bay', 'by', 'the', 'UNK', 'female', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['and', 'champion', 'wanted', 'to', 'be', 'UNK', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['be', 'not', 'ye', 'therefore', 'partakers', 'with', 'them', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['the', 'UNK', 'room', 'of', 'all', 'was', 'like', 'the', 'inside', 'of', 'a', 'UNK', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['and', 'upon', 'the', 'table', 'of', 'shewbread', 'they', 'shall', 'spread', 'a', 'cloth', 'of', 'blue', 'and', 'put', 'thereon', 'the', 'UNK', 'and', 'the', 'spoons', 'and', 'the', 'bowls', 'and', 'UNK', 'to', 'cover', 'withal', 'and', 'the', 'continual', 'bread', 'shall', 'be', 'thereon', 'and', 'they', 'shall', 'spread', 'upon', 'them', 'a', 'cloth', 'of', 'scarlet', 'and', 'cover', 'the', 'same', 'with', 'a', 'covering', 'of', 'badgers', \"'\", 'skins', 'and', 'shall', 'put', 'in', 'the', 'staves', 'thereof', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['UNK', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n",
      "Original sentence ['elinor', 'was', 'not', 'inclined', 'after', 'a', 'little', 'observation', 'to', 'give', 'him', 'credit', 'for', 'being', 'so', 'UNK', 'and', 'UNK', 'ill', 'natured', 'or', 'ill', 'bred', 'as', 'he', 'wished', 'to', 'appear', '.']\n",
      "\n",
      "\n",
      "Autocomplete: say\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check how the auto correction works\n",
    "autocompleted = []\n",
    "for sent in tokenized:\n",
    "    print(\"Original sentence\", sent)\n",
    "    autocomplete = model.predict(tokenized_sentence = sent)\n",
    "    print(\"\\n\")\n",
    "    print(\"Autocomplete:\", autocomplete)\n",
    "    print(\"\\n\")\n",
    "    autocompleted.append(autocomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e399e286-72a6-4202-9b68-a5ccd53ec5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import cer, wer\n",
    "\n",
    "\n",
    "count = 0\n",
    "sum_cer = 0\n",
    "sum_wer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef2059fe-2d2d-4be3-a6f9-8c8c722a7eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg cer = 0.8055555555555561\n",
      "Avg wer = 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "for corrected_sentence in corrected:\n",
    "    for index in range(len(corrected_sentence)):\n",
    "        token_k = START_TOKEN if index == 0 else tokenized[i][index - 1]\n",
    "        token_j = corrected_sentence[index]\n",
    "        sum_cer += cer(token_k, token_j)\n",
    "        sum_wer += wer(token_k, token_j)\n",
    "        count += 1\n",
    "        \n",
    "avg_cer = sum_cer/count\n",
    "avg_wer = sum_wer/count\n",
    "\n",
    "print(f'Avg cer = {avg_cer}')\n",
    "print(f'Avg wer = {avg_wer}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
