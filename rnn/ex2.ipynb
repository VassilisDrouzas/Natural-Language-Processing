{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks import preprocessing, util\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "INPUT_DIR = \"input/UD_English-EWT\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "INTERMEDIATE_DIR = \"intermediate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Acquiring and preprocessing our data with the goal of eventually acquiring a sufficient representation of our text is the most difficult and time-consuming task. We thus split it in distinct phases:\n",
    "\n",
    "* Original dataset acquisition and parsing\n",
    "* Qualitative analysis and preprocessing\n",
    "* Transformation for the NLP task\n",
    "\n",
    "Note that due to the relative custom code complexity, most of the code used in this section was developed and imported from python source files located in the `tasks` module. In-depth documentation and implementation details can be found in these files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading training dataset...\")\n",
    "train_df = preprocessing.conllu_to_pd(os.path.join(INPUT_DIR, \"en_ewt-ud-train.conllu\"))\n",
    "print(\"Loading validation dataset...\")\n",
    "val_df = preprocessing.conllu_to_pd(os.path.join(INPUT_DIR, \"en_ewt-ud-dev.conllu\"))\n",
    "print(\"Loading test dataset...\")\n",
    "test_df = preprocessing.conllu_to_pd(os.path.join(INPUT_DIR, \"en_ewt-ud-test.conllu\"))\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\\nValidation data shape: {val_df.shape}\"\n",
    "      \"\\nTest data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see a preview of our parsed training dataset. Our preprocessing exploits pandas's ordering scheme in order to make sure the words are inserted in the order they appear in the sentence. This ordering will prove important later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, our dataset features words connected with punctuation such as \"don't\". These are normally treated as two words, with the first being their intuitive POS tag (\"do\" - AUX) and the second as part of the first (\"n't\" - PART).\n",
    "\n",
    "This dataset contains both the full words and their split versions, with only the latter featuring valid POS tags. The former are instead marked by a pseudo-tag (here \"_\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_idx = train_df.pos == \"_\"\n",
    "train_df[invalid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(train_df[invalid_idx].words.unique()[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see an example of a word being contained both times in the dataset, one in full with the pseudo-tag, and the other as split words with valid POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[176:179]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus remove the full words including the pseudo-tag from our datasets, ensuring that all target POS tags will be compliant with the UPOS scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[~invalid_idx]\n",
    "val_df = val_df[val_df.pos != \"_\"]\n",
    "test_df = test_df[test_df.pos != \"_\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Analysis\n",
    "\n",
    "We analyze our dataset in two granualities: sentences and individual words. We begin by analyzing how many words are in each sentence, which will give us an idea on the size of context available for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_sentences(df: pd.DataFrame) -> float:\n",
    "    lengths = df.groupby([\"sent_id\"]).agg(lambda x: len(x))\n",
    "    return lengths.words\n",
    "\n",
    "\n",
    "train_length = length_sentences(train_df)\n",
    "val_length = length_sentences(val_df)\n",
    "test_length = length_sentences(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "stats_df = pd.DataFrame({\"words\": pd.concat([train_length, val_length, test_length], ignore_index=True),\n",
    "                        \"type\": len(train_length)*[\"train\"] +\n",
    "                         len(val_length)*[\"validation\"] + \n",
    "                         len(test_length)*[\"test\"]})\n",
    "\n",
    "sns.histplot(x=\"words\", \n",
    "             hue=\"type\", \n",
    "             data=stats_df, \n",
    "             multiple=\"stack\")\n",
    "\n",
    "plt.title(\"Number of sentences by word count\")\n",
    "util.save_plot(\"ex_2_dataset_stats.png\", OUTPUT_DIR)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(set(train_df.words))\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total word count:\\nTraining: {train_df.shape[0]}\"\n",
    "      f\"\\nValidation: {val_df.shape[0]}\"\n",
    "      f\"\\nTesting: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total sentence count:\\nTraining: {len(set(train_df.sent_id))}\"\n",
    "      f\"\\nValidation: {len(set(val_df.sent_id))}\"\n",
    "      f\"\\nTesting: {len(set(test_df.sent_id))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Embedding, Bidirectional, GRU,\\\n",
    "                                    TextVectorization, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "y_train = lb.fit_transform(train_df.pos)\n",
    "y_valid = lb.transform(val_df.pos)\n",
    "y_test = lb.transform(test_df.pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = int(np.quantile(train_length, 0.95))\n",
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = vocab_size\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(max_tokens=MAX_WORDS, output_mode='int', ngrams=1, \n",
    "              output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Setting up vectorizer...\")\n",
    "vectorizer.adapt(train_df.words.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Executing with \", gpus[0].name if len(gpus) != 0 else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and unzip only if the download and unzipped files do not exist \n",
    "!wget -nc -P input/fasttext https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "\n",
    "![ -f \"input/fasttext/cc.en.300.bin\" ] && echo \"Skipping model file\" || gzip --decompress --keep --force \"input/fasttext/cc.en.300.bin.gz\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext \n",
    "\n",
    "\n",
    "print(\"Loading embedding model...\")\n",
    "fasttext_model = fasttext.load_model('input/fasttext/cc.en.300.bin')\n",
    "embedding_matrix = np.zeros(shape=(MAX_WORDS, 300))\n",
    "\n",
    "print(\"Computing word embeddings...\")\n",
    "for w2idx, _word in tqdm(enumerate(vectorizer.get_vocabulary()), \n",
    "                          total = len(vectorizer.get_vocabulary())):\n",
    "    # Skip PAD and UNK tokens\n",
    "    if w2idx < 2:\n",
    "      continue\n",
    "    embedding_matrix[w2idx] = fasttext_model.get_word_vector(_word)\n",
    "\n",
    "# reclaim memory\n",
    "del fasttext_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(https://arxiv.org/pdf/1801.05134.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_SIZE = 300\n",
    "DENSE = 1000\n",
    "\n",
    "\n",
    "# create empty sequential model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vectorizer)\n",
    "\n",
    "model.add(Embedding(MAX_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                    input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False))\n",
    "\n",
    "model.add(Bidirectional(GRU(GRU_SIZE, return_sequences=True)))\n",
    "model.add(LayerNormalization())\n",
    "\n",
    "model.add(Bidirectional(GRU(GRU_SIZE, return_sequences=False)))\n",
    "model.add(LayerNormalization())\n",
    "\n",
    "# add an MLP with 1 hidden layer\n",
    "model.add(Dense(units=DENSE, activation='tanh' ))\n",
    "model.add(Dropout(0.33))\n",
    "model.add(Dense(len(np.unique(train_df.pos)), activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "786/786 [==============================] - 66s 70ms/step\n",
      " — val_f1: 0.827580 — val_precision: 0.849112 — val_recall: 0.835003\n",
      "800/800 [==============================] - 514s 619ms/step - loss: 0.3578 - categorical_accuracy: 0.8838 - val_loss: 0.6857 - val_categorical_accuracy: 0.8350 - val_f1: 0.8276 - val_recall: 0.8350 - val_precision: 0.8491\n",
      "Epoch 2/10\n",
      "786/786 [==============================] - 37s 47ms/step\n",
      " — val_f1: 0.834724 — val_precision: 0.874597 — val_recall: 0.828364\n",
      "800/800 [==============================] - 384s 480ms/step - loss: 0.3101 - categorical_accuracy: 0.8922 - val_loss: 0.5498 - val_categorical_accuracy: 0.8284 - val_f1: 0.8347 - val_recall: 0.8284 - val_precision: 0.8746\n",
      "Epoch 3/10\n",
      "786/786 [==============================] - 37s 47ms/step\n",
      " — val_f1: 0.849330 — val_precision: 0.866722 — val_recall: 0.856711\n",
      "800/800 [==============================] - 377s 471ms/step - loss: 0.2958 - categorical_accuracy: 0.8952 - val_loss: 0.4547 - val_categorical_accuracy: 0.8567 - val_f1: 0.8493 - val_recall: 0.8567 - val_precision: 0.8667\n",
      "Epoch 4/10\n",
      "786/786 [==============================] - 28s 36ms/step\n",
      " — val_f1: 0.818269 — val_precision: 0.836999 — val_recall: 0.823950\n",
      "800/800 [==============================] - 364s 455ms/step - loss: 0.2875 - categorical_accuracy: 0.8969 - val_loss: 0.6493 - val_categorical_accuracy: 0.8240 - val_f1: 0.8183 - val_recall: 0.8240 - val_precision: 0.8370\n",
      "Epoch 5/10\n",
      "786/786 [==============================] - 28s 35ms/step\n",
      " — val_f1: 0.840201 — val_precision: 0.871193 — val_recall: 0.831345\n",
      "800/800 [==============================] - 363s 454ms/step - loss: 0.2813 - categorical_accuracy: 0.8975 - val_loss: 0.5508 - val_categorical_accuracy: 0.8313 - val_f1: 0.8402 - val_recall: 0.8313 - val_precision: 0.8712\n",
      "Epoch 6/10\n",
      "786/786 [==============================] - 28s 35ms/step\n",
      " — val_f1: 0.830842 — val_precision: 0.849783 — val_recall: 0.836434\n",
      "800/800 [==============================] - 366s 457ms/step - loss: 0.2776 - categorical_accuracy: 0.8986 - val_loss: 0.5151 - val_categorical_accuracy: 0.8364 - val_f1: 0.8308 - val_recall: 0.8364 - val_precision: 0.8498\n",
      "Epoch 7/10\n",
      "786/786 [==============================] - 28s 35ms/step\n",
      " — val_f1: 0.835939 — val_precision: 0.868446 — val_recall: 0.827290\n",
      "800/800 [==============================] - 362s 452ms/step - loss: 0.2735 - categorical_accuracy: 0.8996 - val_loss: 0.5206 - val_categorical_accuracy: 0.8273 - val_f1: 0.8359 - val_recall: 0.8273 - val_precision: 0.8684\n",
      "Epoch 8/10\n",
      "786/786 [==============================] - 28s 35ms/step\n",
      " — val_f1: 0.837204 — val_precision: 0.868404 — val_recall: 0.826853\n",
      "800/800 [==============================] - 366s 457ms/step - loss: 0.2710 - categorical_accuracy: 0.8997 - val_loss: 0.6110 - val_categorical_accuracy: 0.8269 - val_f1: 0.8372 - val_recall: 0.8269 - val_precision: 0.8684\n",
      "Epoch 9/10\n",
      "786/786 [==============================] - 28s 35ms/step\n",
      " — val_f1: 0.837643 — val_precision: 0.875884 — val_recall: 0.829119\n",
      "800/800 [==============================] - 365s 456ms/step - loss: 0.2682 - categorical_accuracy: 0.9007 - val_loss: 0.5540 - val_categorical_accuracy: 0.8291 - val_f1: 0.8376 - val_recall: 0.8291 - val_precision: 0.8759\n",
      "Epoch 10/10\n",
      "786/786 [==============================] - 28s 35ms/step\n",
      " — val_f1: 0.842884 — val_precision: 0.879063 — val_recall: 0.835321\n",
      "800/800 [==============================] - 362s 453ms/step - loss: 0.2671 - categorical_accuracy: 0.9000 - val_loss: 0.5482 - val_categorical_accuracy: 0.8353 - val_f1: 0.8429 - val_recall: 0.8353 - val_precision: 0.8791\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "train_data = train_df.words.values\n",
    "val_data = val_df.words.values\n",
    "\n",
    "history = model.fit(train_data, y_train,\n",
    "              validation_data=(val_data, y_valid),\n",
    "              batch_size=256,\n",
    "              epochs=10,\n",
    "              shuffle=True,\n",
    "              callbacks=[util.Metrics(valid_data=(val_data, y_valid))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
