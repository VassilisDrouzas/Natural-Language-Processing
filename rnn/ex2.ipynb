{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks import preprocessing, util\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "INPUT_DIR = \"input/UD_English-EWT\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "INTERMEDIATE_DIR = \"intermediate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Acquiring and preprocessing our data with the goal of eventually acquiring a sufficient representation of our text is the most difficult and time-consuming task. We thus split it in distinct phases:\n",
    "\n",
    "* Original dataset acquisition and parsing\n",
    "* Qualitative analysis and preprocessing\n",
    "* Transformation for the NLP task\n",
    "\n",
    "Note that due to the relative custom code complexity, most of the code used in this section was developed and imported from python source files located in the `tasks` module. In-depth documentation and implementation details can be found in these files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading training dataset...\")\n",
    "train_df = preprocessing.conllu_to_pd(os.path.join(INPUT_DIR, \"en_ewt-ud-train.conllu\"))\n",
    "print(\"Loading validation dataset...\")\n",
    "val_df = preprocessing.conllu_to_pd(os.path.join(INPUT_DIR, \"en_ewt-ud-dev.conllu\"))\n",
    "print(\"Loading test dataset...\")\n",
    "test_df = preprocessing.conllu_to_pd(os.path.join(INPUT_DIR, \"en_ewt-ud-test.conllu\"))\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\\nValidation data shape: {val_df.shape}\"\n",
    "      \"\\nTest data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see a preview of our parsed training dataset. Our preprocessing exploits pandas's ordering scheme in order to make sure the words are inserted in the order they appear in the sentence. This ordering will prove important later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, our dataset features words connected with punctuation such as \"don't\". These are normally treated as two words, with the first being their intuitive POS tag (\"do\" - AUX) and the second as part of the first (\"n't\" - PART).\n",
    "\n",
    "This dataset contains both the full words and their split versions, with only the latter featuring valid POS tags. The former are instead marked by a pseudo-tag (here \"_\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_idx = train_df.pos == \"_\"\n",
    "train_df[invalid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(train_df[invalid_idx].words.unique()[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see an example of a word being contained both times in the dataset, one in full with the pseudo-tag, and the other as split words with valid POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[176:179]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus remove the full words including the pseudo-tag from our datasets, ensuring that all target POS tags will be compliant with the UPOS scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[~invalid_idx]\n",
    "val_df = val_df[val_df.pos != \"_\"]\n",
    "test_df = test_df[test_df.pos != \"_\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Analysis\n",
    "\n",
    "We analyze our dataset in two granualities: sentences and individual words. We begin by analyzing how many words are in each sentence, which will give us an idea on the size of context available for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_sentences(df: pd.DataFrame) -> float:\n",
    "    lengths = df.groupby([\"sent_id\"]).agg(lambda x: len(x))\n",
    "    return lengths.words\n",
    "\n",
    "\n",
    "train_length = length_sentences(train_df)\n",
    "val_length = length_sentences(val_df)\n",
    "test_length = length_sentences(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "stats_df = pd.DataFrame({\"words\": pd.concat([train_length, val_length, test_length], ignore_index=True),\n",
    "                        \"type\": len(train_length)*[\"train\"] +\n",
    "                         len(val_length)*[\"validation\"] + \n",
    "                         len(test_length)*[\"test\"]})\n",
    "\n",
    "sns.histplot(x=\"words\", \n",
    "             hue=\"type\", \n",
    "             data=stats_df, \n",
    "             multiple=\"stack\")\n",
    "\n",
    "plt.title(\"Number of sentences by word count\")\n",
    "util.save_plot(\"ex_2_dataset_stats.png\", OUTPUT_DIR)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(set(train_df.words))\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total word count:\\nTraining: {train_df.shape[0]}\"\n",
    "      f\"\\nValidation: {val_df.shape[0]}\"\n",
    "      f\"\\nTesting: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total sentence count:\\nTraining: {len(set(train_df.sent_id))}\"\n",
    "      f\"\\nValidation: {len(set(val_df.sent_id))}\"\n",
    "      f\"\\nTesting: {len(set(test_df.sent_id))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaselineLabelClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaselineLabelClassifier</label><div class=\"sk-toggleable__content\"><pre>BaselineLabelClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaselineLabelClassifier()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tasks.models import BaselineLabelClassifier\n",
    "\n",
    "\n",
    "x_base_train = train_df.words\n",
    "x_base_valid = val_df.words\n",
    "x_base_test = test_df.words\n",
    "\n",
    "y_base_train = train_df.pos\n",
    "y_base_valid = val_df.pos\n",
    "y_base_test = test_df.pos\n",
    "\n",
    "base_cls = BaselineLabelClassifier()\n",
    "base_cls.fit(X=x_base_train, y=y_base_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.88      0.89      0.89     13152\n",
      "         ADP       0.87      0.67      0.76     17795\n",
      "         ADV       0.61      0.83      0.71     10113\n",
      "         AUX       0.88      0.79      0.83     12828\n",
      "       CCONJ       0.98      0.99      0.99      6688\n",
      "         DET       0.96      0.95      0.95     16299\n",
      "        INTJ       0.68      0.87      0.76       693\n",
      "        NOUN       0.88      0.90      0.89     34813\n",
      "         NUM       0.99      0.89      0.94      4126\n",
      "        PART       0.71      0.89      0.79      5748\n",
      "        PRON       0.90      0.95      0.93     18642\n",
      "       PROPN       0.90      0.84      0.87     12325\n",
      "       PUNCT       0.99      0.99      0.99     23597\n",
      "       SCONJ       0.64      0.41      0.50      3839\n",
      "         SYM       0.88      0.83      0.85       721\n",
      "        VERB       0.83      0.89      0.86     22594\n",
      "           X       0.86      0.64      0.74       641\n",
      "\n",
      "    accuracy                           0.87    204614\n",
      "   macro avg       0.85      0.84      0.84    204614\n",
      "weighted avg       0.88      0.87      0.87    204614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "training_preds = base_cls.predict(x_base_train)\n",
    "print(classification_report(y_base_train, training_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.88      0.83      0.85      1787\n",
      "         ADP       0.89      0.67      0.76      2033\n",
      "         ADV       0.62      0.83      0.71      1178\n",
      "         AUX       0.89      0.78      0.83      1543\n",
      "       CCONJ       0.99      0.99      0.99       737\n",
      "         DET       0.96      0.95      0.95      1897\n",
      "        INTJ       0.70      0.72      0.71       120\n",
      "        NOUN       0.66      0.89      0.76      4137\n",
      "         NUM       0.96      0.54      0.69       542\n",
      "        PART       0.70      0.90      0.79       649\n",
      "        PRON       0.91      0.95      0.93      2162\n",
      "       PROPN       0.88      0.48      0.62      1980\n",
      "       PUNCT       1.00      0.98      0.99      3096\n",
      "       SCONJ       0.60      0.44      0.51       384\n",
      "         SYM       0.81      0.83      0.82       109\n",
      "        VERB       0.80      0.83      0.81      2606\n",
      "           X       0.00      0.00      0.00       136\n",
      "\n",
      "    accuracy                           0.82     25096\n",
      "   macro avg       0.78      0.74      0.75     25096\n",
      "weighted avg       0.84      0.82      0.82     25096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_preds = base_cls.predict(x_base_test)\n",
    "print(classification_report(y_base_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Embedding, Bidirectional, GRU,\\\n",
    "                                    TextVectorization, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "y_train = lb.fit_transform(train_df.pos)\n",
    "y_valid = lb.transform(val_df.pos)\n",
    "y_test = lb.transform(test_df.pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = int(np.quantile(train_length, 0.95))\n",
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = vocab_size\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(max_tokens=MAX_WORDS, output_mode='int', ngrams=1, \n",
    "              output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Setting up vectorizer...\")\n",
    "vectorizer.adapt(train_df.words.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Executing with \", gpus[0].name if len(gpus) != 0 else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and unzip only if the download and unzipped files do not exist \n",
    "!wget -nc -P input/fasttext https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "\n",
    "![ -f \"input/fasttext/cc.en.300.bin\" ] && echo \"Skipping model file\" || gzip --decompress --keep --force \"input/fasttext/cc.en.300.bin.gz\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext \n",
    "\n",
    "\n",
    "print(\"Loading embedding model...\")\n",
    "fasttext_model = fasttext.load_model('input/fasttext/cc.en.300.bin')\n",
    "embedding_matrix = np.zeros(shape=(MAX_WORDS, 300))\n",
    "\n",
    "print(\"Computing word embeddings...\")\n",
    "for w2idx, _word in tqdm(enumerate(vectorizer.get_vocabulary()), \n",
    "                          total = len(vectorizer.get_vocabulary())):\n",
    "    # Skip PAD and UNK tokens\n",
    "    if w2idx < 2:\n",
    "      continue\n",
    "    embedding_matrix[w2idx] = fasttext_model.get_word_vector(_word)\n",
    "\n",
    "# reclaim memory\n",
    "del fasttext_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(https://arxiv.org/pdf/1801.05134.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_SIZE = 300\n",
    "DENSE = 1000\n",
    "\n",
    "\n",
    "# create empty sequential model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vectorizer)\n",
    "\n",
    "model.add(Embedding(MAX_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                    input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False))\n",
    "\n",
    "model.add(Bidirectional(GRU(GRU_SIZE, return_sequences=True)))\n",
    "model.add(LayerNormalization())\n",
    "\n",
    "model.add(Bidirectional(GRU(GRU_SIZE, return_sequences=False)))\n",
    "model.add(LayerNormalization())\n",
    "\n",
    "# add an MLP with 1 hidden layer\n",
    "model.add(Dense(units=DENSE, activation='tanh' ))\n",
    "model.add(Dropout(0.33))\n",
    "model.add(Dense(len(np.unique(train_df.pos)), activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "786/786 [==============================] - 66s 70ms/step\n",
      " — val_f1: 0.827580 — val_precision: 0.849112 — val_recall: 0.835003\n",
      "800/800 [==============================] - 514s 619ms/step - loss: 0.3578 - categorical_accuracy: 0.8838 - val_loss: 0.6857 - val_categorical_accuracy: 0.8350 - val_f1: 0.8276 - val_recall: 0.8350 - val_precision: 0.8491\n",
      "Epoch 2/10\n",
      "786/786 [==============================] - 37s 47ms/step\n",
      " — val_f1: 0.834724 — val_precision: 0.874597 — val_recall: 0.828364\n",
      "800/800 [==============================] - 384s 480ms/step - loss: 0.3101 - categorical_accuracy: 0.8922 - val_loss: 0.5498 - val_categorical_accuracy: 0.8284 - val_f1: 0.8347 - val_recall: 0.8284 - val_precision: 0.8746\n",
      "Epoch 3/10\n",
      "786/786 [==============================] - 37s 47ms/step\n",
      " — val_f1: 0.849330 — val_precision: 0.866722 — val_recall: 0.856711\n",
      "800/800 [==============================] - 377s 471ms/step - loss: 0.2958 - categorical_accuracy: 0.8952 - val_loss: 0.4547 - val_categorical_accuracy: 0.8567 - val_f1: 0.8493 - val_recall: 0.8567 - val_precision: 0.8667\n",
      "Epoch 4/10\n",
      "786/786 [==============================] - 28s 36ms/step\n",
      " — val_f1: 0.818269 — val_precision: 0.836999 — val_recall: 0.823950\n",
      "800/800 [==============================] - 364s 455ms/step - loss: 0.2875 - categorical_accuracy: 0.8969 - val_loss: 0.6493 - val_categorical_accuracy: 0.8240 - val_f1: 0.8183 - val_recall: 0.8240 - val_precision: 0.8370\n",
      "Epoch 5/10\n",
      "786/786 [==============================] - 28s 35ms/step\n",
      " — val_f1: 0.840201 — val_precision: 0.871193 — val_recall: 0.831345\n",
      "800/800 [==============================] - 363s 454ms/step - loss: 0.2813 - categorical_accuracy: 0.8975 - val_loss: 0.5508 - val_categorical_accuracy: 0.8313 - val_f1: 0.8402 - val_recall: 0.8313 - val_precision: 0.8712\n",
      "Epoch 6/10\n",
      "786/786 [==============================] - 28s 35ms/step\n",
      " — val_f1: 0.830842 — val_precision: 0.849783 — val_recall: 0.836434\n",
      "800/800 [==============================] - 366s 457ms/step - loss: 0.2776 - categorical_accuracy: 0.8986 - val_loss: 0.5151 - val_categorical_accuracy: 0.8364 - val_f1: 0.8308 - val_recall: 0.8364 - val_precision: 0.8498\n",
      "Epoch 7/10\n",
      "786/786 [==============================] - 28s 35ms/step\n",
      " — val_f1: 0.835939 — val_precision: 0.868446 — val_recall: 0.827290\n",
      "800/800 [==============================] - 362s 452ms/step - loss: 0.2735 - categorical_accuracy: 0.8996 - val_loss: 0.5206 - val_categorical_accuracy: 0.8273 - val_f1: 0.8359 - val_recall: 0.8273 - val_precision: 0.8684\n",
      "Epoch 8/10\n",
      "786/786 [==============================] - 28s 35ms/step\n",
      " — val_f1: 0.837204 — val_precision: 0.868404 — val_recall: 0.826853\n",
      "800/800 [==============================] - 366s 457ms/step - loss: 0.2710 - categorical_accuracy: 0.8997 - val_loss: 0.6110 - val_categorical_accuracy: 0.8269 - val_f1: 0.8372 - val_recall: 0.8269 - val_precision: 0.8684\n",
      "Epoch 9/10\n",
      "786/786 [==============================] - 28s 35ms/step\n",
      " — val_f1: 0.837643 — val_precision: 0.875884 — val_recall: 0.829119\n",
      "800/800 [==============================] - 365s 456ms/step - loss: 0.2682 - categorical_accuracy: 0.9007 - val_loss: 0.5540 - val_categorical_accuracy: 0.8291 - val_f1: 0.8376 - val_recall: 0.8291 - val_precision: 0.8759\n",
      "Epoch 10/10\n",
      "786/786 [==============================] - 28s 35ms/step\n",
      " — val_f1: 0.842884 — val_precision: 0.879063 — val_recall: 0.835321\n",
      "800/800 [==============================] - 362s 453ms/step - loss: 0.2671 - categorical_accuracy: 0.9000 - val_loss: 0.5482 - val_categorical_accuracy: 0.8353 - val_f1: 0.8429 - val_recall: 0.8353 - val_precision: 0.8791\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "train_data = train_df.words.values\n",
    "val_data = val_df.words.values\n",
    "\n",
    "history = model.fit(train_data, y_train,\n",
    "              validation_data=(val_data, y_valid),\n",
    "              batch_size=256,\n",
    "              epochs=10,\n",
    "              shuffle=True,\n",
    "              callbacks=[util.Metrics(valid_data=(val_data, y_valid))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
