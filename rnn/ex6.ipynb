{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# set random seed for deterministic execution\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/dimits/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to /home/dimits/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/dimits/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_corpus = nltk.corpus.movie_reviews.fileids()                                 #Get all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg_corpus = nltk.corpus.gutenberg.fileids()                                 #Get all the files\n",
    "gutenberg_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_gutenberg_corpus = [                        #We wish to keep only these from gutenberg corpus\n",
    "    'austen-emma.txt', \n",
    "    'austen-persuasion.txt', \n",
    "    'austen-sense.txt', \n",
    "    'burgess-busterbrown.txt', \n",
    "    'carroll-alice.txt', \n",
    "    'chesterton-ball.txt',\n",
    "    'chesterton-brown.txt' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gutenberg corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bd1996e9c44a48abe1a94342bab6c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Movie Review corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c471e573748b4bf09714425ad1e51b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died t\n"
     ]
    }
   ],
   "source": [
    "combined_text = \"\"   \n",
    "\n",
    "print(\"Loading Gutenberg corpus...\")\n",
    "for file_id in tqdm(filtered_gutenberg_corpus):                                        # Combine the text from all files\n",
    "    combined_text += nltk.corpus.gutenberg.raw(file_id)\n",
    "\n",
    "print(\"Loading Movie Review corpus...\")\n",
    "for file_id in tqdm(movie_corpus):\n",
    "    combined_text += nltk.corpus.movie_reviews.raw(file_id)\n",
    "    \n",
    "print(combined_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_TOKEN = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    text = text.replace('[', '')\n",
    "    text = text.replace(']', '')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'[^a-zA-z.?!\\']', ' ', text)                     #Remove these characters   \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Emma by Jane Austen       VOLUME I  CHAPTER I   Emma Woodhouse  handsome  clever  and rich  with a comfortable home and happy disposition  seemed to unite some of the best blessings of existence  and had lived nearly twenty one years in the world with very little to distress or vex her.  She was the youngest of the two daughters of a most affectionate  indulgent father  and had  in consequence of her sister's marriage  been mistress of his house from a very early period.  Her mother had died too\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_text = remove_special_chars(combined_text)\n",
    "combined_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(''.join(text))                  #Get the sentences\n",
    "    return sentences     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99864\n",
      "Her mother had died too long ago for her to have more than an indistinct remembrance of her caresses  and her place had been supplied by an excellent woman as governess  who had fallen little short of a mother in affection.\n",
      "certain moments are so preposterous that i nearly herniated myself in an attempt to stifle laughter .\n"
     ]
    }
   ],
   "source": [
    "sentences = tokenize_sentences(combined_text) \n",
    "print(len(sentences))    \n",
    "print(sentences[2])  \n",
    "print(sentences[57649])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002260\n",
      "Austen\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "words = tokenize_words(combined_text)\n",
    "print(len(words))\n",
    "print(words[3])\n",
    "print(words[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting words from source texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad142cb243c472a9f1c5dba10a3b8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99864 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_list = []                                    #list of all the words of sentences\n",
    "print(\"Getting words from source texts...\")\n",
    "for f in tqdm(sentences):\n",
    "    words_list.append(tokenize_words(f))                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "random.shuffle(words_list)\n",
    "train_len = math.floor(0.6 * len(words_list))                      #Training set length(60%)\n",
    "dev_len = math.floor(0.2 * len(words_list))                        #Development set length (20%)\n",
    "test_len = math.floor(0.2 * len(words_list))                       #Test set length (20%)\n",
    "\n",
    "training_set = []\n",
    "development_set = []\n",
    "test_set = []\n",
    "\n",
    "for content in words_list[0:train_len]:\n",
    "    training_set.append(content)\n",
    "    \n",
    "for content in words_list[train_len: train_len + dev_len]:\n",
    "    development_set.append(content)\n",
    "\n",
    "for content in words_list[train_len + dev_len:]:\n",
    "    test_set.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def count_unigrams(all_corpus):\n",
    "    unigram_counter = Counter()\n",
    "    for sentence in all_corpus:\n",
    "            grams = [gram for gram in ngrams(sentence, 1)]\n",
    "            unigram_counter.update(grams)\n",
    "    return unigram_counter\n",
    "\n",
    "\n",
    "def replace_OOV_words_train(all_corpus, unk_token):\n",
    "    unigram_counter = count_unigrams(all_corpus)\n",
    "    OOV_words = {}\n",
    "\n",
    "    for k, v in unigram_counter.items():\n",
    "        if v < 10:\n",
    "            key = k[0]\n",
    "            #Using the symbol UNKOWN_TOKEN else the model will think it's a word\n",
    "            OOV_words[key] = unk_token                 #set the word to \"UNK\"\n",
    "\n",
    "    replaced_corpus = []                          #the original corpus having the OOV words replaced by 'UNK'\n",
    "    for sentence in all_corpus:\n",
    "        clean_sentence = []\n",
    "    \n",
    "        for word in sentence:\n",
    "            clean_sentence.append(OOV_words.get(word, word))\n",
    "    \n",
    "        replaced_corpus.append(clean_sentence)\n",
    "\n",
    "\n",
    "    vocabulary = []\n",
    "\n",
    "    for key in unigram_counter.keys():        #Iterate the unigram counter\n",
    "        word = key[0]                         #get the word\n",
    "        if word not in OOV_words:\n",
    "            vocabulary.append(word)\n",
    "\n",
    "    vocabulary = set(vocabulary)              #Keep unique words\n",
    "    return vocabulary, replaced_corpus, OOV_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_OOV_words_test(all_corpus, vocabulary, oov_words, unk_token):\n",
    "    \n",
    "    replaced_corpus = []\n",
    "    for sentence in all_corpus:\n",
    "        updated_sent = []\n",
    "\n",
    "        for word in sentence:\n",
    "            if (word not in vocabulary) or (word in oov_words):\n",
    "                updated_sent.append(unk_token)\n",
    "            else:\n",
    "                updated_sent.append(word)\n",
    "                \n",
    "    replaced_corpus.append(updated_sent)\n",
    "    return replaced_corpus   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, train_corpus, OOV_words = replace_OOV_words_train(training_set, UNKNOWN_TOKEN)\n",
    "development_set = replace_OOV_words_test(development_set, vocabulary, OOV_words, UNKNOWN_TOKEN)\n",
    "test_set = replace_OOV_words_test(test_set, vocabulary, OOV_words, UNKNOWN_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, TweetTokenizer\n",
    "\n",
    "\n",
    "UNK_sentences = [sent_tokenize(' '.join(sentence)) for sentence in train_corpus]       #get the sentences that include UNK values\n",
    "tweet_wt = TweetTokenizer()\n",
    "tokenized = [tweet_wt.tokenize(' '.join(sentence)) for sentence in UNK_sentences[:50]]  # Get the first 50 sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
