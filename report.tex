\documentclass[11pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[greek, english]{babel}
\usepackage{alphabeta}
\usepackage{libertine}
\usepackage{graphicx}
\usepackage{biblatex}[sorting=nty] % sort alphabetically
\usepackage[table]{xcolor}
\usepackage{mathptmx} % Times New Roman
\usepackage{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}

\pagenumbering{arabic}
\graphicspath{ {./resources/} }
\addbibresource{refs.bib}

\def\code#1{\texttt{#1}}

\title{\Huge Text Analytics: 1st Assignment}

\author{\LARGE Tsirmpas Dimitris\\\LARGE Drouzas Vasilis}


\begin{document}
	
	\begin{titlepage}
		\maketitle
		\begin{center}
			
			\large Athens University of Economics and Business
			
			\large MSc in Data Science
			
		\end{center}
		
	\end{titlepage}
	
	\tableofcontents
	\newpage	
	
	\section{Introduction}
	
	This report will briefly discuss the theoretical background, implementation details and decisions taken for the construction of bi-gram and tri-gram models.
	
 
	The full code can be found at \url{https://github.com/dimits-exe/text_analytics}. Note that the notebook does not contain the models, which are imported from python source code files in the \texttt{src} directory.

    In terms of how we collaborated, Dimitris Tsirmpas constructed the models and their corresponding functions (e.g. auto-complete, the spelling corrector) while Vasilis Drouzas handled data preprocessing (e.g. OOV words) and the evaluation of the models (creating an artificial dataset, calculating CER/WER scores). Both authors collaborated in the process of demonstrating the models' scores (cross entropy, perplexity). 
	
	\section{Datasets}
	
	
	\subsection{Original Dataset}
	
	For the needs of this assignment, we picked the movie reviews corpus from the NLTK data repository, as well as a hand-picked selection of files from the Gutenberg dataset. 
	
	We followed the following Data preprocessing steps:
	
	\begin{itemize}
		\item We converted the text to lowercase letters.
		\item We used tokenization in terms of both sentences and words.
		\item We divided the dataset in 3 sets, the training set (60\%), development set (20\%) and test set (20\%). We used the development set in order to get the optimal alpha value which would be used to find the bigram and trigram probabilities.
		\item We removed some special characters, such as [ ] ? !
		
	\end{itemize}

	
	We used a function to get the counters of unigrams, bigrams and trigrams.  Regarding the OOV words, in the training dataset, we checked for words that appear less than 10 times. These words were filtered and their value was set to \texttt{'UNK'}. (OOV words).
	
	We initialized a new corpus, called ‘replaced\_corpus’ , where  OOV words are replaced with \texttt{'UNK'}. It iterates through each sentence in the original corpus (‘all\_corpus’) and replaces words with their corresponding "UNK" value if they are OOV.
	
	To find the vocabulary, we simply iterated the word counter and added all the words that were not OOV. To make sure we did not include duplicates, we converted the vocabulary to a set.
	
	The same process was applied for the development and test sets, except that now we kept the same vocabulary. We updated the sentences with the \texttt{‘UNK’} value when necessary. Finally, we calculated the 20 most frequent words of unigrams, bigrams and trigrams and the vocabulary length, which can be found in Figure 1 and in the notebook.

 \begin{figure}[H]
    \centering
    \includegraphics[width=6cm]{"common_words1".png}
    
    \label{fig:galaxy}
\end{figure}

 \begin{figure}[H]
    \centering
    \includegraphics[width=6cm]{"common_words2".png}
    
    \label{fig:galaxy}
\end{figure}


\begin{centering}



\renewcommand{\caption}{Figure 1: }
\caption{The most common words for the unigram, bigram and trigram models.}

\end{centering}
	
	\subsection{Corrupted Dataset}
	
	In order to test the spell checking models, a new dataset needed to be created. We decided to use a manually corrupted version of the combined dataset mentioned above.
	
	Thus, we created a function \texttt{get\_similar\_char()} to define replacements from original characters. For example, a would be replaced by e, c would be replaced by s etc. This function returns a randomly chosen character from those defined.
	
	The function was subsequently used by \texttt{corrupt\_sentence()}, which takes as input a sentence and returns a new corrupted one with a probability for every character of 0.1 (user-defined parameter). Below we give an example:
 
  \begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{"corrupted".png}
    
    \label{fig:galaxy}
\end{figure}

 \begin{centering}



\renewcommand{\caption}{Figure 2: }
\caption{Testing the sentence corruption.}

\end{centering}
	
	\section{Language Modeling}
	
	\subsection{Defining the models}
	
	Two models were primarily used in the language modeling task, those being the Bi-gram and Tri-gram models. We also include a Linear Interpolation model using the former models for the sake of comparison. The source code for the models can be found at \texttt{src/ngram\_models.py}.
	
	During fitting, the models simply memorize the different uni-grams and bi-grams, or bi-grams and tri-grams for the bi-gram and tri-gram models respectively, and their occurrences in the training corpus. 
	
	During inference, the models predict the next token based on the algorithm shown in Algorithm \ref{al::next_token}, where \texttt{candidates(sentence)} and \texttt{ngram\_probability(sentence, word)} are defined according to the model. 
	
	The \texttt{is not UNK} condition makes it impossible to output UNKNOWN tokens without disrupting the distribution of the words as found in the original text (which would have been the case had we, for instance, removed ngrams containing UNK during fitting). Our model thus essentially selects the next best option when UNK would mathematically be the best guess.
	
	The \texttt{candidates(sentence)} function is a look-up of the last or the two last words in the sentences in the model's respective bi-grams and tri-grams.
	
	The \texttt{ngram\_probability(sentence, word)} function for the bi-gram model would be $ P(w_2|w_1) = \frac{C(w_1,w_2) + \alpha}{C(w_1) + \alpha \cdot|V|} $, where  $w1$ is the last word of the sentence, $w2$ is the word under consideration, $ C(w_1,w_2) $ is the bigram count, $ C(w_1) $ is the unigram count, $ 0 \leq\alpha \leq1 $ is the smoothing hyper-parameter and |V| the vocabulary size.
	
	Similarly, the \texttt{ngram\_probability(sentence, word)} function for the tri-gram model would be $ P(w_3|w_1,w_2) = \frac{C(w_1,w_2,w_3) + \alpha}{C(w_1,w_2) + \alpha \cdot |V|} $ where  $w1 and w2$ the last words of the sentence, $w3$ the word under consideration, $ C(w_1,w_2,w_3) $ the trigram count, $ C(w_1,w_2) $ is the bigram count, $ 0 \leq\alpha \leq1 $ is the smoothing hyper-parameter and |V| the vocabulary size.
	
	The reason we only need to calculate the last ngram in order to predict the next token is simple. Let $t_1$, $t_2$ be the tokens under consideration and $w_1 \cdots w_k$ the words of the sentence thus far. For $t_1$ to be selected over $t_2$ the total probability of the sentence which includes $t_1$ must exceed the one which includes $t_2$. For the bigram model, this is expressed as in Equation \ref{eq::bigram_sentence_probs}. Similarly, the trigram model case is explored in Equation \ref{eq::trigram_sentence_probs}. 
	
	\begin{equation}
		\label{eq::bigram_sentence_probs}
		\begin{aligned}
			& P(w^{k+1}_1) > P(w^{k+1}_1) \iff \\
			& P(w_1|<start>) P(w_2|w_1) \cdots P(t_1|w_k) > \\
			& P(w_1|<start>) P(w_2|w_1) \cdots P(t_2|w_k) \iff \\
			& P(t_1|w_k) > P(t_2|w_k)
		\end{aligned}
	\end{equation}
	
	\begin{equation}
		\label{eq::trigram_sentence_probs}
		\begin{aligned}
			& P(w^{k+1}_1) > P(w^{k+1}_1) \iff \\
			& P(w_1|<start>, <start>) P(w_2|w_1, <start>) \cdots P(t_1|w_k, w_{k-1}) > \\ & P(w_1|<start>, <start>) P(w_2|w_1, <start>) \cdots P(t_2|w_k, w_{k-1})) \iff \\
			& P(t_1|w_k, w_{k-1}) > P(t_2|w_k, w_{k-1}))
		\end{aligned}
	\end{equation}
	
	
	
	\begin{algorithm}
		\caption{N-Gram model next-token prediction} 
		\label{al::next_token}
		
		\hspace*{\algorithmicindent} \textbf{Input} sentence: a list of strings\\
		\hspace*{\algorithmicindent} \textbf{Output} max\_token: the most probable string
		\begin{algorithmic}[1]
			
			\State max\_prob = $-\infty$
			
			\For {token in candidates(sentence)}
			
				\If{token is not UNK}
					\State prob = ngram\_probability(sentence, word)
					
					\If{prob \textgreater  max\_prob}
						\State max\_prob = prob
						\State max\_token = token
					\EndIf
				
				\EndIf
			
			\EndFor
			
			\State \Return max\_token
		\end{algorithmic} 
	\end{algorithm}
	
	
	Meta tags such as \textless START \textgreater  and \textless END \textgreater  are appropriately automatically inserted depending on the model. The tri-gram model uses the same tag for its two starting tokens, because of restrictions of the \texttt{nltk} library which is used to produce ngrams. Because of this decision, during the probability estimation we ignore $P(word1 | <start>, <start> )$.
	
	
	
	
	
	
	
	\subsection{Comparing the language models}
	
	To find the cross-entropy and perplexity, we used the models defined in the .py files with the simple formulas of cross entropy and perplexity in the corresponding functions. 
	
	What we needed next was to find the optimal alpha for the probability formulas, as we stated earlier. In ngram\_model\_alpha\_search() we initialize a numpy array to store the entropy values. Iterating the alpha values, we calculate the cross entropy for each alpha. Finally, we keep the index with the best alpha (the one with the smallest cross entropy value). We searched for $1000$ alpha values taken from an exponential sequence in the range of $[10^{-10}, 1]$.

 \begin{figure}[H]
    \centering
    \includegraphics[width=11cm]{"alpha_entropy".png}
    
    \label{fig:galaxy}
\end{figure}


\begin{centering}



\renewcommand{\caption}{Figure 3: }
\caption{Finding the best alpha (bigram model).}

\end{centering} 

\

\

The cross-entropy and perplexity scores of the two models can be found below:

\begin{figure}[H]
    \centering
    \includegraphics[width=6cm]{"scores_bigram".png}
    
    \label{fig:galaxy}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=6cm]{"scores_trigram".png}
    
    \label{fig:galaxy}
\end{figure}

\begin{centering}



\renewcommand{\caption}{Figure 4: }
\caption{Cross entropy and perplexity scores for bigram and trigram models.}

\end{centering} 
	\newpage
	\section{Spell Checking}
	Regarding the spell correction, we cite an example of our bi-gram model corrector:

 \begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{"bigram_evaluation".png}
    
    \label{fig:galaxy}
\end{figure}


\begin{centering}



\renewcommand{\caption}{Figure 5: }
\caption{Testing the spell corrector (bi-gram model).}

\end{centering} 

\ 

\

 
	In order to obtain the WER and CER scores of a sentence, we imported the \texttt{jiwer} package, from which we used the \texttt{wer()} and \texttt{cer()} functions to calculate the corresponding scores. Then, we just took the average of these scores. The final results can be found below.

 \begin{figure}[H]
    \centering
    \includegraphics[width=11cm]{"cer_bigram".png}
    
    \label{fig:galaxy}
\end{figure}


 \begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{"cer_trigram".png}
    
    \label{fig:galaxy}
\end{figure}


\begin{centering}



\renewcommand{\caption}{Figure 6: }
\caption{WER and CER scores for the two models.}

\end{centering} 
	
	
\end{document}