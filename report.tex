\documentclass[11pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[greek, english]{babel}
\usepackage{alphabeta}
\usepackage{libertine}
\usepackage{graphicx}
\usepackage{biblatex}[sorting=nty] % sort alphabetically
\usepackage[table]{xcolor}
\usepackage{mathptmx} % Times New Roman
\usepackage{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{subcaption} %for subfigures
\usepackage{hyperref}


\pagenumbering{arabic}
\graphicspath{ {./output/}{./images/} }
\addbibresource{refs.bib}

\def\code#1{\texttt{#1}}

\title{\Huge Text Analytics: 1st Assignment}

\author{\LARGE Tsirmpas Dimitris\\\LARGE Drouzas Vasilis}


\begin{document}
	
	\begin{titlepage}
		\maketitle
		\begin{center}
			
			\large Athens University of Economics and Business
			
			\large MSc in Data Science
			
		\end{center}
		
	\end{titlepage}
	
	\tableofcontents
	\newpage	
	
	\section{Introduction}
	
	This report will briefly discuss the theoretical background, implementation details and decisions taken for the construction of bi-gram and tri-gram models.

    In terms of how we collaborated, Dimitris Tsirmpas constructed the models and their corresponding functions (e.g. auto-complete, the spelling corrector) while Vasilis Drouzas handled data preprocessing (e.g. OOV words) and the evaluation of the models (creating an artificial dataset, calculating CER/WER scores). Both authors collaborated in the process of demonstrating the models' scores (cross entropy, perplexity). 
   
	
	\section{Datasets}
	
	
	\subsection{Original Dataset}
	
	For the needs of this assignment, we picked the movie reviews corpus from the NLTK data repository, as well as a hand-picked selection of files from the Gutenberg dataset. 
	
	We followed the following Data preprocessing steps:
	
	\begin{itemize}
		\item We converted the text to lowercase letters.
		\item We used tokenization in terms of both sentences and words.
		\item We divided the dataset in 3 sets, the training set (60\%), development set (20\%) and test set (20\%). We used the development set in order to get the optimal alpha value which would be used to find the bigram and trigram probabilities.
		\item We removed some special characters, such as [ ] ? !
		
	\end{itemize}

	
	We used a function to get the counters of unigrams, bigrams and trigrams.  Regarding the OOV words, in the training dataset, we checked for words that appear less than 10 times. These words were filtered and their value was set to \texttt{'UNK'}. (OOV words).
	
	We initialized a new corpus, called ‘replaced\_corpus’ , where  OOV words are replaced with \texttt{'UNK'}. It iterates through each sentence in the original corpus (‘all\_corpus’) and replaces words with their corresponding "UNK" value if they are OOV.
	
	To find the vocabulary, we simply iterated the word counter and added all the words that were not OOV. To make sure we did not include duplicates, we converted the vocabulary to a set.
	
	The same process was applied for the development and test sets, except that now we kept the same vocabulary. We updated the sentences with the \texttt{‘UNK’} value when necessary. Finally, we calculated the 20 most frequent words of unigrams, bigrams and trigrams and the vocabulary length, which can be found in Figure \ref{fig::common_words} and in the notebook.
	

	 \begin{figure}
	    \centering
	    \includegraphics[width=12cm]{"common_words".jpg}
	    \caption{Most common uni-grams, bi-grams and tri-grams in the training corpus.}
	    \label{fig::common_words}
	\end{figure}
	
	
	
	

	\subsection{Corrupted Dataset}
	
	In order to test the spell checking models, a new dataset needed to be created. We decided to use a manually corrupted version of the combined dataset mentioned above.
	
	Thus, we created a function \texttt{get\_similar\_char()} to define replacements from original characters. For example, a would be replaced by e, c would be replaced by s etc. This function returns a randomly chosen character from those defined.
	
	The function was subsequently used by \texttt{corrupt\_sentence()}, which takes as input a sentence and returns a new corrupted one with a probability for every character of 0.1 (user-defined parameter). An example can be found in Figure \ref{fig::corruption}.
 
	\begin{figure}
	    \centering
	    \includegraphics[width=15cm]{"corrupted".png}
	    \caption{An example of a "corrupted" sentence compared to its original.}
    \label{fig::corruption}
	\end{figure}
		
	
	\section{Language Modeling}
	
	\subsection{Defining the models}
	
	Two models were primarily used in the language modeling task, those being the Bi-gram and Tri-gram models. We also include a Linear Interpolation model using the former models for the sake of comparison. The source code for the models can be found at \texttt{src/ngram\_models.py}.
	
	During fitting, the models simply memorize the different uni-grams and bi-grams, or bi-grams and tri-grams for the bi-gram and tri-gram models respectively, and their occurrences in the training corpus. 
	
	During inference, the models predict the next token based on the algorithm shown in Algorithm \ref{al::next_token}, where \texttt{candidates(sentence)} and \texttt{ngram\_probability(sentence, word)} are defined according to the model. 
	
	The \texttt{is not UNK} condition makes it impossible to output UNKNOWN tokens without disrupting the distribution of the words as found in the original text (which would have been the case had we, for instance, removed ngrams containing UNK during fitting). Our model thus essentially selects the next best option when UNK would mathematically be the best guess.
	
	The \texttt{candidates(sentence)} function is a look-up of the last or the two last words in the sentences in the model's respective bi-grams and tri-grams.
	
	The \texttt{ngram\_probability(sentence, word)} function for the bi-gram model would be $ P(w_2|w_1) = \frac{C(w_1,w_2) + \alpha}{C(w_1) + \alpha \cdot|V|} $, where  $w1$ is the last word of the sentence, $w2$ is the word under consideration, $ C(w_1,w_2) $ is the bigram count, $ C(w_1) $ is the unigram count, $ 0 \leq\alpha \leq1 $ is the smoothing hyper-parameter and $\lvert V \rvert$ the vocabulary size.
	
	Similarly, the \texttt{ngram\_probability(sentence, word)} function for the tri-gram model would be $ P(w_3|w_1,w_2) = \frac{C(w_1,w_2,w_3) + \alpha}{C(w_1,w_2) + \alpha \cdot |V|} $ where  $w1$ and $w2$ the last words of the sentence, $w3$ the word under consideration, $ C(w_1,w_2,w_3) $ the trigram count, $ C(w_1,w_2) $ is the bigram count, $ 0 \leq\alpha \leq1 $ is the smoothing hyper-parameter and $\lvert V \rvert$ the vocabulary size.
	
	The reason we only need to calculate the last ngram in order to predict the next token is simple. Let $t_1$, $t_2$ be the tokens under consideration and $w_1 \cdots w_k$ the words of the sentence thus far. For $t_1$ to be selected over $t_2$ the total probability of the sentence which includes $t_1$ must exceed the one which includes $t_2$. For the bigram model, this is expressed as in Equation \ref{eq::bigram_sentence_probs}. Similarly, the trigram case is explored in Equation \ref{eq::trigram_sentence_probs}. 
	
	\begin{equation}
		\label{eq::bigram_sentence_probs}
		\begin{aligned}
			& P(w^{k}_1 | t^k+1) > P(w^{k+1}_1 | t^k+1) \iff \\
			& log(P(w_1|<start>)) + log(P(w_2|w_1)) + \cdots + log(P(t_1|w_k)) > \\
			& log(P(w_1|<start>)) + log(P(w_2|w_1)) + \cdots + log(P(t_2|w_k)) \iff \\
			& log(P(t_1|w_k)) > log(P(t_2|w_k))
		\end{aligned}
	\end{equation}
	
	\begin{equation}
		\label{eq::trigram_sentence_probs}
		\begin{aligned}
			&  P(w^{k}_1 | t^k+1) > P(w^{k+1}_1 | t^k+1) \iff \\
			& log(P(w_1|<start>, <start>)) + log(P(w_2|w_1, <start>)) + \cdots + log(P(t_1|w_k, w_{k-1})) > \\ 
			& log(P(w_1|<start>, <start>)) + log(P(w_2|w_1, <start>)) + \cdots + log(P(t_2|w_k, w_{k-1})) \iff \\
			& log(P(t_1|w_k, w_{k-1})) > log(P(t_2|w_k, w_{k-1}))
		\end{aligned}
	\end{equation}
	
	
	
	\begin{algorithm}
		\caption{N-Gram model next-token prediction} 
		\label{al::next_token}
		
		\hspace*{\algorithmicindent} \textbf{Input} sentence: a list of strings\\
		\hspace*{\algorithmicindent} \textbf{Output} max\_token: the most probable string
		\begin{algorithmic}[1]
			
			\State max\_prob = $-\infty$
			
			\For {token in candidates(sentence)}
			
				\If{token is not UNK}
					\State prob = ngram\_probability(sentence, word)
					
					\If{prob \textgreater  max\_prob}
						\State max\_prob = prob
						\State max\_token = token
					\EndIf
				
				\EndIf
			
			\EndFor
			
			\State \Return max\_token
		\end{algorithmic} 
	\end{algorithm}
	
	
	Meta tags such as \textless START \textgreater  and \textless END \textgreater  are appropriately automatically inserted depending on the model. The tri-gram model uses the same tag for its two starting tokens, because of restrictions of the \texttt{nltk} library which is used to produce ngrams. Because of this decision, during the probability estimation we ignore $P(word1 | <start>, <start>)$.
	
	
	\subsection{Comparing the language models}
	
	To find the cross-entropy and perplexity, we used the models defined in the .py files with the simple formulas of cross entropy and perplexity in the corresponding functions. 
	
	What we needed next was to find the optimal alpha for the probability formulas, as we stated earlier. In \texttt{ngram\_model\_alpha\_search()} we initialize a numpy array to store the entropy values. Iterating the alpha values, we calculate the cross entropy for each alpha. Finally, we keep the index with the best alpha (the one with the smallest cross entropy value). We searched for $1000$ alpha values taken from an exponential sequence in the range of $[10^{-15}, 1]$. The perplexity scores were computed on a separate validation (development) set.
	
	The change in entropy depending on the $\alpha$ smoothing hyperparameter can be seen in Figures \ref{fig::bi_alpha_entropy}, \ref{fig::tri_alpha_entropy}, \ref{fig::li_alpha_entropy}. The Bi-gram model exhibits high variance depending on different $\alpha$ values, where for a very tight region close to 0 (but not too close) results in catastrophic loss of performance. The tri-gram and linear interpolation models on the other hand exhibit relative stability, with very small $\alpha$ values being slightly favored. Of note is also a slight improvement in the bi-grams complexity when $\alpha=1$, possibly indicating high uncertainty on the model.
	
	
	 \begin{figure}
	    \centering
	    \includegraphics[width=8cm]{"bi_alpha.png"}
	    \caption{Cross Entropy of the bi-gram language model depending on the values of the $\alpha$ smoothing hyperparameter.}
	    \label{fig::bi_alpha_entropy}
	\end{figure}

	\begin{figure}
		\centering
		\includegraphics[width=8cm]{"tri_alpha.png"}
		\caption{Cross Entropy of the trigram language model depending on the values of the $\alpha$ smoothing hyperparameter.}
		\label{fig::tri_alpha_entropy}
	\end{figure}
	
	
	\begin{figure}
		\centering
		\includegraphics[width=8cm]{"interpolation_alpha.png"}
		\caption{Cross Entropy of the linear interpolation language model depending on the values of the $\alpha$ smoothing hyperparameter.}
		\label{fig::li_alpha_entropy}
	\end{figure}


	The cross-entropy and perplexity scores of the two models on the \textit{test} corpus can be found in Figures \ref{fig::scores_bigram}, \ref{fig::scores_trigram}. The scores tend to vary greatly depending on the given seed, fluctuating between $40$ and $1,800$ perplexity. This indicates an unstable classifier, which is probably caused by the clashing domains of our two main datasets, as well as relative lack of relevant data which cause our model to underfit. This can be clearly seen by the training and test scores not being far apart, while both featuring high perplexity. For the sake of brevity we do not include the training scores in this report, although they are available in the notebook.

	\begin{figure}
	    \centering
	    \includegraphics[width=6cm]{"scores_bigram".png}
	    \caption{Cross entropy and perplexity scores for the bigram model.}
	    \label{fig::scores_bigram}
	\end{figure}
	
	\begin{figure}
	    \centering
	    \includegraphics[width=6cm]{"scores_trigram".png}
	    \caption{Cross entropy and perplexity scores for the trigram model.}
	    \label{fig::scores_trigram}
	\end{figure}


	
	\section{Spell Checking}
	
	
	\subsection{Defining the models}
	
	In order to design models capable of correcting spelling mistakes we need to adapt our previous language models to factor in the user-provided (and likely incorrectly spelled) sentence. Thus, we make the assumption that the generated sentence must have a length equal to the user-provided one. As a logical consequence, the model can never predict meta tokens (no START tokens by design, UNKNOWN tokens for the reasons detailed in the previous section, and no END tokens since the predicted sentence's size is known and constant).
	
	We use Beam Search (defined in the file \texttt{src/beam\_search.py}) to construct the best candidate sentence. This is a generalization of Algorithm \ref{al::next_token}, where for each new token, we take into consideration the $k$ most probable tokens, where $k$ the beam search width hyper-parameter. Large values of $k$ lead to increased computational complexity but also more reliable results.
	
	We define two models, one bi-gram and one tri-gram spell checking model, which internally use the respective language models defined in the section above. The \texttt{candidates(sentence)} are delegated to said internal models, while the \texttt{ngram\_probability(sentence, word)} function is defined as $P(w^{k}_1) =log(P(t^k_1)) + log(P(w^{k}_1 | t^k_1))$ where $P(t^k_1)$ is defined as in the language models and $ P(w^{k}_1 | t^k_1) = \sum^n_i \upsilon_i \frac{1}{Lev(w_i, t_i) + 1}$, where $n$ is the current search depth, $\upsilon_i = 0$ if the $i$-th word is the unknown token and 1 otherwise, and $Lev(w_i, k_i)$ is the levenshtein distance between the original word $w_i$ and the candidate token $t_i$. Thus UNKNOWN tokens are only handled by the language models, with no input from the distance score function.

	
	\subsection{Comparing the language models}
	
	Regarding the spell correction, we cite an example of our bi-gram and tr-gram model correctors in Figures \ref{fig::spell_check_example_bigram} \& \ref{fig::spell_check_example_trigram}.
	
	 \begin{figure}
	    \centering
	    \includegraphics[width=15cm]{"bigram_evaluation".png}
	    \caption{Testing the spell corrector (bi-gram model).}
	    \label{fig::spell_check_example_bigram}
	\end{figure}

	\begin{figure}
	    \centering
	    \includegraphics[width=15cm]{"trigram_evaluation".png}
	    \caption{Testing the spell corrector (tri-gram model).}
	    \label{fig::spell_check_example_trigram}
	\end{figure}


     Now, let's consider comparing our models using WER and CER scores. WER measures the percentage of words that are incorrectly predicted or recognized by a system compared to a reference (ground truth). It is calculated by the formula: 
      \[  WER = \frac{S + D + I}{N} \times 100 \]

    where:
    \begin{align*}
    & S \text{ is the number of substitutions (incorrectly predicted words)}, \\
    & D \text{ is the number of deletions (words in the reference but not predicted)}, \\
    & I \text{ is the number of insertions (extra words predicted but not in the reference)}, \\
    & N \text{ is the total number of words in the reference}.
    \end{align*}

    CER measures the percentage of characters that are incorrectly predicted or recognized by a system compared to a reference. It is calculated using the formula:

    \[ CER = \frac{S + D + I}{C} \times 100 \]

    where:

    \begin{align*}
    S & \text{ is the number of substitutions (incorrectly predicted characters),} \\
    D & \text{ is the number of deletions (characters in the reference but not predicted),} \\
    I & \text{ is the number of insertions (extra characters predicted but not in the reference),} \\
    C & \text{ is the total number of characters in the reference.}
    \end{align*}

    Like WER, CER is expressed as a percentage, and lower values indicate better performance. A CER of 0 means perfect character-level recognition.

    
	In order to obtain the WER and CER scores of a sentence, we imported the \texttt{jiwer} package, from which we used the \texttt{wer()} and \texttt{cer()} functions to calculate the corresponding scores. Then, we just took the average of these scores. The final results can be found in Figures \ref{fig::cer_bigram}, \ref{fig::cer_trigram}.
	

	 \begin{figure}
	    \centering
	    \includegraphics[width=11cm]{"cer_bigram".png}
	    \caption{WER and CER scores for the bi-gram spell checking model.}
	    \label{fig::cer_bigram}
	\end{figure}
	
	
	 \begin{figure}
	    \centering
	    \includegraphics[width=12cm]{"cer_trigram".png}
	    \caption{WER and CER scores for the tri-gram spell checking model.}
	    \label{fig::cer_trigram}
	\end{figure}
 
	The trigram model outperforms the bigram model in both WER and CER, indicating that incorporating more context (tri-grams instead of bi-grams) has led to better language model performance.
	
\end{document}