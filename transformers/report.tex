\documentclass[10pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[greek, english]{babel}
\usepackage{alphabeta}
\usepackage{libertine}
\usepackage{graphicx}
\usepackage[backend=biber]{biblatex}
\usepackage[table]{xcolor}
\usepackage{mathptmx} % Times New Roman
\usepackage{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{subcaption} %for subfigures
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{longtable}


\hypersetup{
	colorlinks=true,
	linkcolor=black,
	filecolor=magenta,      
	urlcolor=blue
}


\pagenumbering{arabic}
\graphicspath{ {./output/}{./images/} }
\addbibresource{refs.bib}

\def\code#1{\texttt{#1}}

\title{\Huge Text Analytics: 4th Assignment}

\author{\LARGE Tsirmpas Dimitris\\\LARGE Drouzas Vasilis}


\begin{document}
	
	\begin{titlepage}
		\maketitle
		\begin{center}
			
			\large Athens University of Economics and Business
			
			\large MSc in Data Science
			
		\end{center}
		
	\end{titlepage}
	
	\tableofcontents
	\newpage	
	
	\section{Introduction}
	
	This report will briefly discuss the theoretical background, implementation details and decisions taken for the construction of CNN models for sentiment analysis and POS tagging tasks.
	
	This report and its associated code, analysis and results were conducted by the two authors. Specifically, the sentiment analysis task was performed by Drouzas Vasilis, and the POS-tagging task by Tsirmpas Dimitris. This report was written by both authors.
	
	
	\section{POS Tagging}
	
	POS tagging is a language processing task where words in a given text are assigned specific grammatical categories, such as nouns, verbs, or adjectives. The objective is to analyze sentence structure. 
	
	In this section we describe how we can leverage pre-trained word embeddings to create a context-aware RNN classifier.
	
	
	\subsection{Dataset}
	
	Acquiring and preprocessing our data with the goal of eventually acquiring a sufficient representation of our text is the most difficult and time-consuming task. We thus split it in distinct phases:
	
	\begin{itemize}
		\item Original dataset acquisition and parsing
		\item Qualitative analysis and preprocessing
		\item Transformation necessary for the NLP task
	\end{itemize}
	
	Each of these distinct steps are individually analyzed below.
	
	
	\subsubsection{Acquisition}
	
	We select the \href{https://universaldependencies.org/treebanks/en_ewt/index.html}{English EWT-UD} tree, which is the largest currently supported collection for POS tagging tasks for the English language.
	
	This corpus contains 16622 sentences, 251492 tokens and 254820 syntactic words, as well as 926 types of words that contain both letters and punctuation, such as 's, n't, e-mail, Mr., ’s, etc). This is markedly a much higher occurrence than its siblings, and therefore may lead to a slightly more difficult task.
	
	The dataset is made available in \texttt{conllu} format, which we parse using the recommended \texttt{conllu} python library. We create a dataframe for every word and its corresponding POS tag and link words belonging to the same sentences by a unique sentence ID. The data are already split to training, validation and test sets, thus our own sets correspond to the respective split files.
	
	We are interested in the UPOS (Universal Part of Speech) tags for English words.
	
	\subsubsection{Qualitative Analysis}
	
	Our training vocabulary is comprised of $16654$ words. We include qualitative statistics on the sentences of our dataset in Tables \ref{tab::ex-2-sent-stats} and \ref{tab::ex-2-stats}. The splits are explicitly mentioned separately because the splitting was performed by the dataset authors and not by random sampling. We would therefore like to confirm at a glance whether their data are similar.
	
	\begin{table}
		\begin{tabular}{|l|l|l|l|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{Set} & \cellcolor{blue!25}\textbf{Mean} & \cellcolor{blue!25}\textbf{Std} &
			\cellcolor{blue!25}\textbf{Min} & \cellcolor{blue!25}\textbf{25\%} & \cellcolor{blue!25}\textbf{50\%} &
			\cellcolor{blue!25}\textbf{75\%} &
			\cellcolor{blue!25}\textbf{Max} \\
			\hline
			Training & 18.96 & 11.78 & 5  & 10 & 16 & 24 & 159 \\\hline
			Validation  & 15.66 & 10.05 & 5 & 8 & 13 & 20 & 75  \\\hline
			Test & 12518 & 10.33  & 5 & 8 & 13 & 20 & 81
			\\\hline
		\end{tabular}
		\centering
		\caption{Summary and order statistics for the number of words in the sentences of each data split.}
		\label{tab::ex-2-sent-stats}
	\end{table}
	
	\begin{table}
		\begin{tabular}{|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{Set} & \cellcolor{blue!25}\textbf{Total Word Count} &
			\cellcolor{blue!25}\textbf{Total Sentence Count}\\
			\hline
			Training & 15967 & 10539 \\\hline
			Validation & 24005  & 1538 \\\hline
			Test & 23811 & 1535 \\\hline
		\end{tabular}
		\centering
		\caption{Total text volume of each data split.}
		\label{tab::ex-2-stats}
	\end{table}
	
	
	\subsubsection{Preprocessing}
	
	Given the nature of our task we can not implement preprocessing steps such as removing punctuation marks, stopwords or augmenting the dataset. Thus, the only meaningful preprocessing at this stage would be converting the words to lowercase. We believe that the context of each word will carry enough information to distinguish its POS tag regardless of case. 
	
	Another issue we need to address before continuing is that of words being part of (depending on) other words for their POS tag. Those would be words such as "don't", "couldn't" or "you're". In the standard UPOS schema these are defined as two or more separate words, where the first is represented by its standard POS tag, and the rest as part of that tag (UPOS tag "PART"). For instance, "don't" would be split into "do" and "n't" with "AUX" and "PART" tags respectively. In our dataset, these words are represented both in the manner described above followed by the full word ("don't") tagged with the pseudo-tag "\_". We remove the latter representation from the working dataset.
	
	For the word embeddings we originally used a Word2Vec variant implemented in the \texttt{spacy} library called \texttt{en\_core\_web\_md}. The model seemed suitable for our needs because of the similarities in domain (pre-trained on blogs, news and comments which fits our dataset). However, it proved extremely slow and thus constrained the amount of embeddings we could reasonably procure, limiting our classifier.
	
	Thus we use the fasttext \texttt{cc.en.300} model. This model has a total size of 7GB which may present OOM issues in some machines, but calculates embeddings extremely fast, while also allowing partial modeling of Out Of Vocabulary (OOV) words. The model is used to calculate the embedding matrix which is later attached to the RNN model.
	
	As we can see from Table \ref{tab::ex-2-sent-stats}, there is a sizable portion of our sentences that feature very few words. In order to make the RNN training more efficient, we choose to discard sentences with very few words. We also set a window size equal to the 90\% percentile of sentence word count, meaning tht 90\% of our windows will fully fit the training sentences. The rest will be automatically split into more sentences, and as such don't need to be excluded from the dataset.
	
	
	\subsection{Baseline Classifier}
	
	We create our own classifier which classifies each token by the majority label associated with it. The classifier is defined as a subclass of sklearn's classifier superclass and thus can seamlessly use it in most sklearn-provided functions such as \texttt{classification\_report()} and its implementation can be found in the \texttt{tasks.models} module.
	
	The results of the classifier can be found in Tables \ref{tab::ex_3_train}, \ref{tab::ex_3_valid} and \ref{tab::ex_3_test}. We note a high accuracy for most tags, which make intuitive sense, since most words in the English language can be classified in a single label, irrespective of context. For example, "is" will always be classified as "AUX", and all punctuation marks will be classified as "PUNCT".
	
	Thus, besides quantitative statistics such as categorical accuracy and f1-score, we should pay close attention to the precision and recall statistics for the more variable POS tags such as "NOUN" or "VERB" in order to properly evaluate our MLP classifier.
	
	
	\subsection{MLP Classifier}
	
	The model we use is the pre-trained optimal model used in the previous assignment. We follow the same preprocessing and caching steps as in the previous assignment. Since the model is not trained again, we use only a subset of the original training data (25,000 windows) in order to save on scare main-memory resources. We consider this a representative sample for comparison with other classifiers due to the sample size (law of large numbers). Results can be found in Tables \ref{tab::ex_3_train}, \ref{tab::ex_3_valid} and \ref{tab::ex_3_test}.
	
	
	\subsection{RNN Classifier}
	
	We use the time-distributed, Bi-GRU RNN model used in the previous assignment. Both this and the CNN model use the same window-based input, thus no more intervention is necessary to run the model. Results can be found in the tables outlined above.
	
	
	\subsection{CNN classifier}
	
	We use the pretrained stacked CNN model used in the previous assignment. As mentioned above, the input is the same as in the Bi-GRU RNN case. Results can be found in the tables outlined above.
	
	
	
	\subsection{BERT}
	
	We will be using the Transformers library in order to leverage already pretrained BERT models (implemented in PyTorch) for the POS tagging task. The model we will be using is the \href{https://huggingface.co/TweebankNLP/bertweet-tb2_ewt-pos-tagging}{Tweebank NLP Bert Model} which is pretrained on the POS tagging task on a similar dataset to ours.
	
	
	\subsubsection{Data Representation}
	
	We split our text into individual sentences and load them in a dataset dict (from the \code{datasets} library) in order to properly feed them to our model.
	
	Unfortunately, while the authors do provide their own tokenizer, it is a "slow" tokenizer, meaning that its API is restricted. Since we need parts of the API to properly align labels and tokens, we will be using another "fast" tokenizer. We pick the vanilla bert-base-uncased tokenizer since its unlikely to deviate significantly from the tweebank tokenizer, and we choose the uncased version since, for the reasons mentioned above, we have converted all words to lowercase.
	
	An important issue in token classification for BERT models, is that the underlying model does not operate on words as tokens, but to sub-word tokens. This effectively means that for each word we must predict pseudo-tokens followed by the real POS token. We modify the originally provided \code{compute\_metrics} function which maps each token to multiple pseudo-labels (with the value -100), while the last token is the actual POS tag from our dataset. We apply this function to all data.
	
	In order to extract different metrics for each evaluation period, we construct our own \code{compute\_metrics} function. This is necessary since a lot of custom processing is need for token classification tasks, unlike with binary classification tasks where the code is trivial.
	
	We specifically turn the predictions into a continuous array of one-hot vectors, which can be used by sklearn to compute the metrics. We represent the dummy class (-100) with an extra integer, which is used to create a mask. This mask is applied both to the true labels and predictions, leading to two matrices composed of one-hot-vectors of only valid POS tags. 
	
	
	\subsubsection{Hyperparameter tuning}
	
	We will be using the Trainer class from the transformers library to train models of various hyper-parameters.
	
	\begin{itemize}
		\item We exploit the gradient\_accumulation\_steps parameter to simulate a larger batch size, without incurring further GPU VRAM cost.
		
		\item We set up early stopping with respect to validation accuracy like with the models in previous assignments.
		
		\item We set the evaluation and save procedures to be executed each 100 steps, in order to strike a balance between the evaluation overhead and accurate statistics.
		
		\item We provide our0\code{compute\_metrics} function to the \code{Trainer} class in order to get detailed statistics in every evaluation period
	\end{itemize}
	
	
	The model features hundreds of millions of parameters making training very expensive. Given that we run our model locally, this places a very heavy restriction on the number of tuning runs we can afford and restraining us from choices we otherwise would like to explore. Namely:
	
	* We only consider one pretrained model. However, this model is a large BERT model pretrained on the same task and very similar dataset. It is therefore improbable that another model could outperform it.
	* We do not unfreeze any BERT layers since that would sharply increase the training time. However, as mentioned above, the specific model has been pretrained on the same family of datasets as ours. Thus it is unlikely that the underlying BERT model would "learn" anything new from our fine-tuning.
	* We restrict the hyperparameter runs to only 2. However, given the relative lack of hyperparameters to tune, this should not significantly restrict the final performance of our model.
	
	Given the above limitations we define our search space as:
	\begin{itemize}
		\item The learning rate.
		\item The weight decay.
		\item The warmup steps.
	\end{itemize}
	
	Unlike \code{keras\_tuner}, unfortunately we \href{https://discuss.huggingface.co/t/oserror-unable-to-load-weights-from-pytorch-checkpoint-file/3406}{cannot access automatically the best run}. Thus, we choose the best model by hand from our local files.
	
	\subsubsection{Results}
	
	We can adapt the provided \code{get\_prediction} function in order to compare the true and predicted POS tags for each sentence in our dataset. 
	
	However, a significant issue becomes evident once we attempt to apply this to our data; our data are tokenized twice by the tokenizer (as explained more in-depth in the class forum). While post-processing allows some sentences to successfully get parsed into aligned POS tags, any sentences including tokens such as apostrophes force the algorithm to fail. This seems to be a restriction of the tokenizer API which we can not solve at the current moment in time. If the method worked correctly, we could feed the aligned true and prediction POS tags as seen above to the \code{tasks.util.stats\_by\_label} function which would automatically generate label-wise accuracy, precision, recall, f1 and PR-AUC scores.
	
	Nonetheless, since we are able to compute aggregated results, we make them available in Table \ref{tab::bert_results}. The fine-tuned BERT model seems to out-perform all baselines except from the MLP baseline by a very narrow margin, which aligns with our expectations. We hypothesize that with more resources, enabling further hyper-parameter tuning and more datasets, this model could outperform the MLP classifier.
	
	
	\begin{table}
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{Cross Entropy Loss} & 
			\cellcolor{blue!25}\textbf{Accuracy} &
			\cellcolor{blue!25}\textbf{Precision} &
			\cellcolor{blue!25}\textbf{Recall}  &
			\cellcolor{blue!25}\textbf{F1-score} \\
			\hline
			0.3773 & 0.8852 & 0.7394 & 0.716 & 0.7256 \\
			\hline			
		\end{tabular}
		\centering
		\caption{Macro-averaged evaluation metrics for the fine-tuned BERT POS tagger.}
		\label{tab::bert_results}
	\end{table}
	
	
	\subsection{LLM Classifier}
	
	For the final POS tagging task, we will be using ChatGPT to automatically produce POS tags from our dataset. The input and output is inserted/extracted manually using the ChatGPT web API.
	
	We utilize a few-shot (demonstrator) prompting approach with a role prompt, detailed instructions and definitions. The model is prompted to answer via standard Input: Output: indicators. The prompt used and a breakdown of its structure can be found in Figure \ref{fig::prompt.png}.
	
	Unfortunately, our approach suffers from the same issue encountered in the Transformers Result section. Since we can not reliably tokenize the words into sub-token strings, the LLM produces misaligned POS labels.
	
	\begin{figure}
		\centering
		\includegraphics[height=16cm]{prompt.png}
		\caption{The prompt used to elicit POS tagging classifications. Used on Chat-GPT.}
		\label{fig::prompt.png}
	\end{figure}
	
		
	\input{output/ex_3_train.tex}
	
	\input{output/ex_3_valid.tex}
	
	\input{output/ex_3_test.tex}
	
	
	\section{Sentiment Analysis}
	
	Sentiment analysis, also known as opinion mining, is the process of analyzing text to determine the sentiment or emotional tone expressed within it. The goal of sentiment analysis is to understand the attitudes, opinions, and emotions conveyed by the text. 
	
	
	\subsection{Dataset}
	Here we will be working with the \href{http://www.cs.cornell.edu/people/pabo/movie-review-data/}{Cornell Movie Review dataset}, which consists of 2000 movie reviews, split equally in 1000 positive and 1000 negative ones. The goal here will be to develop classifiers that will effectively understand whether a review is a positive or negative one, based on the data it has been trained on.We begin by taking a brief look into our dataset.
	
	
	\subsubsection{Average Document Length }
	
	The average document length in words and characters is:
	\begin{itemize}
		\item Average number of words: 746.3405
		\item Average number of characters: 3893.002
	\end{itemize}


	\subsubsection{Pre-processing}
	
	For demonstration reasons, we start by printing the 20 most frequent words in the text, in Figure \ref{fig::20_common}.
	
	\begin{figure}
		\centering
		\includegraphics[width=4cm]{"20\_most\_common_words".png}
		\caption{The 20 most common words in the text, along with their occurences.}
		\label{fig::20_common}
	\end{figure}
	
	
	
	Most of these words are actually stop words. As in most text classification problems, we would typically need to remove the stop words of the text.
	
	The english stopwords is a package of 179 words that in general, would not help in a sentiment analysis problem. But, since they include terms that are negative, removing them could prove harmful for our case, since we are dealing with a sentiment analysis problem.
	
	e.g. imagine the phrase "I didn't like the film" to end up "like film". Disastrous, right?
	
	So, the plan is to remove all the stop words that include negative meaning before the preprocessing.
	The stop words that we decided to keep in the text are shown in Figure \ref{fig::to_keep}.
	
	\begin{figure}
		\centering
		\includegraphics[width=4cm]{"to_keep_words".png}
		\caption{The 'important' words we decided to keep for this sentiment analysis problem.}
		\label{fig::to_keep}
	\end{figure}
	
	Moving on to the pre-processing task, the steps performed are the following:
	\begin{itemize}
		\item{ Combination to a single document.}
		\item{ Convertion to lowercase.}
		\item{Lemmatization and stop words extraction.}
		\item{ Punctuation removal.}
		\item{ Number removal.}
		\item{Single characters removal.}
		\item{ Converting multiple spaces to single ones.}
	\end{itemize}
 
        \subsubsection{Data Augmentation}
        Our dataset consists of 2000 reviews, as we stated earlier. The size can be considered rather small and classification algorithms may be led to overfitting. 
        
        One measure we may take to face this is augmenting the text data. This involves generating new data points by making minor modifications on the existing ones. Here we will use the technique of synonym replacement. Iterating the original data, the function \texttt{synonym\_replacement()} replaces words in each sentence with synonyms and the labels are also updated accordingly.
        
        How the \texttt{synonym\_replacement()} works: We split the input sentence into words. For each word, we retrieve synonyms from the 'synsets' function of WordNet. If we find synonyms, we randomly select one and we extract the canonical form of it (lemmatization). Finally, we replace the original word with the lemma if they are different.

        After applying data augmentation, we manage to double the size of the dataset, from 2000 to 4000 movie reviews. 

        
	
	\subsubsection{Splitting the dataset}
	We decided to split the (processed) dataset into the training set (70\%), development set (15\%) and test set (15\%). The sizes of each set are shown in Table \ref{tab::ex-9-stats}.
	
	\begin{table}
		\begin{tabular}{|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{Set} & \cellcolor{blue!25}\textbf{Total Word Count} &
			\cellcolor{blue!25}\textbf{Total Document Count}\\
			\hline
			Training & 46103 & 2800 \\\hline
			Development & 25136  & 600 \\\hline
			Test & 25410 & 600 \\\hline
		\end{tabular}
		\centering
		\caption{Total text volume of each data split.}
		\label{tab::ex-9-stats}
	\end{table}
	
	
	\subsubsection{SpaCy}
	As an additional step to our pre-processing function, we also used SpaCy in order to proceed to the sentence splitting and the tokenization, in the same manner as we discussed in the lab. In the training dataset, we find out that the average word length dropped from 2587.9 (before tokenization) to 314.12 (after tokenization). More statistics about the mean and the standard deviation of the sequence length on the training, development and test sets can be found in Table \ref{tab::ex-1-stats} .
	
	\begin{table}
		\begin{tabular}{|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{Set} & \cellcolor{blue!25}\textbf{Mean of sequence length} &
			\cellcolor{blue!25}\textbf{Standard deviation of sequence length}\\
			\hline
			Training & 314.12 & 135.9 \\\hline
			Development & 313.7  & 133.6 \\\hline
			Test & 321.4 & 138.5 \\\hline
		\end{tabular}
		\centering
		\caption{Mean and standard deviation of the sequence length in training,development and test sets.}
		\label{tab::ex-1-stats}
	\end{table}
	
	
	
	\subsubsection{Padding the sequences}
	After that, we used the Tokenizer module from keras preprocessing with maximum number of words to 100000 (so we kept all words actually) and we replaced all rare words with UNK values. We keep a word index (a dictionary where the keys are words (tokens) and the values are their corresponding indices in the tokenizer's vocabulary). Eventually we find out that the number of unique words in the index is 36637.
	
	Next steps involve converting the tokenized sets to sequences and padding these sequences.
	
	\subsubsection{Embedding matrix}
	We downloaded the fasttext binary model that includes pretrained word embeddings. The procedure to create the embedding matrix was the following: We iterated over the word\_index dictionary, and for each word we check whether the index is within the limit of MAX\_WORDS. If so, we retrieve the word vector from the fasttext model and we assign it to the corresponding word row in the embedding matrix.
	
	
	\subsection{Classifiers}
	\subsubsection{DummyClassifier}
	DummyClassifier makes predictions that ignore the input features. This classifier serves as a simple baseline to compare against other more complex classifiers.The strategy to generate predictions was set to ‘most\_frequent’,  meaning that the predict method always returns the most frequent class label. The results of this classifier are demonstrated in Tables  \ref{tab::ex-1-report-train-dummy}, \ref{tab::ex-1-report-dev-dummy}, \ref{tab::ex-1-report-test-dummy}.
	
	\begin{table}
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{} & \cellcolor{blue!25}\textbf{Precision} &
			\cellcolor{blue!25}\textbf{Recall}  &
			\cellcolor{blue!25}\textbf{f1-score}  &
			\cellcolor{blue!25}\textbf{support}\\
			\hline
			neg & 0.00 & 0.00 & 0.00 & 690 \\\hline
			pos & 0.51  & 1.00 & 0.67 & 710 \\\hline
			accuracy & & & \textbf{0.51} & 1400 \\\hline
			macro avg & 0.25 & 0.5 & 0.34 & 1400 \\\hline
			weighted avg & 0.26 & 0.51 & 0.34 & 1400 \\\hline
			
		\end{tabular}
		\centering
		\caption{Classification report on the training set (Dummy Classifier).}
		\label{tab::ex-1-report-train-dummy}
	\end{table}
	

 	\begin{table}
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{} & \cellcolor{blue!25}\textbf{Precision} &
			\cellcolor{blue!25}\textbf{Recall}  &
			\cellcolor{blue!25}\textbf{f1-score}  &
			\cellcolor{blue!25}\textbf{support}\\
			\hline
			neg & 0.00 & 0.00 & 0.00 & 157 \\\hline
			pos & 0.48  & 1.00 & 0.65 & 143 \\\hline
			accuracy & & & \textbf{0.48} & 300 \\\hline
			macro avg & 0.24 & 0.5 & 0.32 & 300 \\\hline
			weighted avg & 0.23 & 0.48 & 0.31 & 300 \\\hline
			
		\end{tabular}
		\centering
		\caption{Classification report on the development set (Dummy Classifier).}
		\label{tab::ex-1-report-dev-dummy}
	\end{table}
	
	\begin{table}
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{} & \cellcolor{blue!25}\textbf{Precision} &
			\cellcolor{blue!25}\textbf{Recall}  &
			\cellcolor{blue!25}\textbf{f1-score}  &
			\cellcolor{blue!25}\textbf{support}\\
			\hline
			neg & 0.00 & 0.00 & 0.00 & 153 \\\hline
			pos & 0.49  & 1.00 & 0.66 & 147 \\\hline
			accuracy & & & \textbf{0.49} & 300 \\\hline
			macro avg & 0.24 & 0.5 & 0.33 & 300 \\\hline
			weighted avg & 0.24 & 0.49 & 0.32 & 300 \\\hline
			
		\end{tabular}
		\centering
		\caption{Classification report on the test set (Dummy Classifier).}
		\label{tab::ex-1-report-test-dummy}
	\end{table}
	
	
	As expected, the results are poor since the decision of the classifier depends exclusively only the majority class.
	
	\subsubsection{Logistic Regression}
	Logistic Regression is a statistical method used for binary classification tasks, where the output variable takes only two possible outcomes.
	Before applying Logistic Regression, we will perform a grid search to find the optimal parameters to run the classifier. The parameters we tried are the following:
	\begin{itemize}
		\item{Solver: We tested ‘liblinear’ and ‘saga’ solvers}
		\item{Penalty: We tested ‘l1’, ‘l2’ reguralization penalties}
		\item{C: We tested values of 0.001, 0.01, 0.1, 1 and 10 (inverse of regularization strength)}
	\end{itemize}
	
	The best hyperparameters were the following:
	C= 1, penalty= ‘l2’, solver = ‘liblinear’.
	
	Now, it is time to fit the Logistic Regression using these parameters. The results we got are shown in Tables \ref{tab::ex-1-report-train-lr}, \ref{tab::ex-1-report-dev-lr}, \ref{tab::ex-1-report-test-lr}.
	
	\begin{table}
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{} & \cellcolor{blue!25}\textbf{Precision} &
			\cellcolor{blue!25}\textbf{Recall}  &
			\cellcolor{blue!25}\textbf{f1-score}  &
			\cellcolor{blue!25}\textbf{support}\\
			\hline
			neg & 0.94 & 0.92 & 0.93 & 690 \\\hline
			pos & 0.92  & 0.94 & 0.93 & 710 \\\hline
			accuracy & & & \textbf{0.93} & 1400 \\\hline
			macro avg & 0.93 & 0.93 & 0.93 & 1400 \\\hline
			weighted avg & 0.93 & 0.93 & 0.93 & 1400 \\\hline
			
		\end{tabular}
		\centering
		\caption{Classification report on the training set (Logistic Regression).}
		\label{tab::ex-1-report-train-lr}
	\end{table}
	

 	\begin{table}
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{} & \cellcolor{blue!25}\textbf{Precision} &
			\cellcolor{blue!25}\textbf{Recall}  &
			\cellcolor{blue!25}\textbf{f1-score}  &
			\cellcolor{blue!25}\textbf{support}\\
			\hline
			neg & 0.90 & 0.82 & 0.85 & 157 \\\hline
			pos & 0.82  & 0.9 & 0.85 & 143 \\\hline
			accuracy & & & \textbf{0.85} & 300 \\\hline
			macro avg & 0.86 & 0.86 & 0.85 & 300 \\\hline
			weighted avg & 0.86 & 0.85 & 0.85 & 300 \\\hline
			
		\end{tabular}
		\centering
		\caption{Classification report on the development set (Logistic Regression).}
		\label{tab::ex-1-report-dev-lr}
	\end{table}
	
	\begin{table}
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{} & \cellcolor{blue!25}\textbf{Precision} &
			\cellcolor{blue!25}\textbf{Recall}  &
			\cellcolor{blue!25}\textbf{f1-score}  &
			\cellcolor{blue!25}\textbf{support}\\
			\hline
			neg & 0.88 & 0.81 & 0.84 & 153 \\\hline
			pos & 0.82  & 0.88 & 0.85 & 147 \\\hline
			accuracy & & & \textbf{0.85} & 300 \\\hline
			macro avg & 0.85 & 0.85 & 0.85 & 300 \\\hline
			weighted avg & 0.85 & 0.85 & 0.85 & 300 \\\hline
			
		\end{tabular}
		\centering
		\caption{Classification report on the test set (Logistic Regression).}
		\label{tab::ex-1-report-test-lr}
	\end{table}
	 .
	
	
	
	\subsubsection{Our custom MLP classifier}
	
	First of all, we define the y\_train\_1\_hot and y\_dev\_1\_hot vectors using the LabelBinarizer and applying \texttt{fit\_transform()} and \texttt{transform()} to the training and development 1-hot vectors respectively.
	
	Now, it’s time to define our MLP model. We used the SGD algorithm since for this case it provided better results than Adam. The number of epochs was set to 50 and early stopping was used. We experimented with a variety of different hyperparameter combinations (Table \ref{tab::ex-9-hyper}).
	
	
	
	
	\begin{table}
		\centering
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\rowcolor{blue!25}\textbf{Learning rate} & \cellcolor{blue!25}\textbf{\#Hidden layers} & \cellcolor{blue!25}\textbf{Hidden layers size} & \cellcolor{blue!25}\textbf{Dropout probability} & \cellcolor{blue!25}\textbf{Batch size}\\
			\hline
			0.001 & 1 & 64 & 0.4 & 1\\
			\hline
			0.01 & 2 & 128 & 0.5 & 64\\
			\hline
			0.1 &  &  & & 128 \\
			
			
			
			\hline
		\end{tabular}
		\caption{Hyperpameters tested in the development set (MLP classifier).}
		\label{tab::ex-9-hyper}
	\end{table}
	
	The process to decide the hyperparameters is simple: We defined a list of the possible hyperparameter combinations and for each one we ran the model. After that, we evaluated on the development set and we kept the model with the best development accuracy.
	
	
	The optimal model consisted of the following hyperparameters:
	\begin{itemize}
		\item Learning rate: 0.1
		\item Number of hidden layers: 1
		\item Hidden layers' size: 64
		\item Dropout probability: 0.4
		\item Batch size: 64
	\end{itemize}
	
	
	Next, we provide the metrics (Precision,Recall, F1 score and the AUC scores) for training, development and test subsets in Figure \ref{fig::mlp_metrics}.
	
	\begin{figure}
		\centering
		\includegraphics[width=11cm]{"MLP_metrics".png}
		\caption{Metrics for the MLP classifier for both classes for the training, development and test sets.}
		\label{fig::mlp_metrics}
	\end{figure}
	
	Finally,  the Macro-averaged metrics (averaging the corresponding scores of the previous bullet over the classes) for the training, development and test subsets, are shown in Figure \ref{fig::mlp_macro_metrics}.
	
	\begin{figure}
		\centering
		\includegraphics[width=11cm]{"MLP_Macro_metrics".png}
		\caption{Macro-Metrics for the MLP classifier for both classes for the training, development and test sets.}
		\label{fig::mlp_macro_metrics}
	\end{figure}
	
	\subsubsection{Our custom RNN classifier}
	We start by creating a Self Attention class, which builds a sequential model as we discussed in the lab. We create the one-hot vectors we will need and now we are ready to construct our RNN model. The RNN model we create is a Sequential one, with:
	\begin{itemize}
		\item An embedding layer, which produces dense vector of fixed size. It utilizes the embedding matrix and sets the pre-trained word embeddings to non-trainable.
		\item Bidirectional GRU layers (processing the input)
		\item The self attention layer on the MLP.
		\item Dense layers, with 'relu' as the activation function.
		\item Dropout, output layers and the compilation part (using Adam this time).
	\end{itemize}
	
	The hyperparameters we will use here are summarized in Table \ref{tab::ex-1-hyper}.
	
	\begin{table}
		\centering
		\begin{tabular}{|l|l|l|l|l|l|}
			\hline
			\rowcolor{blue!25}\textbf{Learning rate} & \cellcolor{blue!25}\textbf{\#Hidden layers} & \cellcolor{blue!25}\textbf{Hidden layers size} & \cellcolor{blue!25}\textbf{Dropout probability} & \cellcolor{blue!25}\textbf{GRU size}  & \cellcolor{blue!25}\textbf{MLP Units}\\
			\hline
			0.001 & 1 & 64 & 0.2 & 100 & 64\\
			\hline
			0.01 & 2 & 128 & 0.25 & 150 & 128\\
			\hline
			0.1 & 3 & 256 & 0.3 & 200 & 256 \\
			\hline 
			& & & 0.35 & 250 & \\
			\hline 
			& & & 0.4 & 300 &\\
			\hline 
			& & & 0.45 & 350 &\\
			\hline 
			& & & 0.5 & 400 &\\
			\hline 
			& & &  & 450 &\\
			\hline 
			& & & & 500 & \\
			
			
			
			\hline
		\end{tabular}
		\caption{Hyperparameters tested in the development set (RNN classifier).}
		\label{tab::ex-1-hyper}
	\end{table}
	
	\ 
	
	
	\
	
	We utilize Keras Tuner in order to find the optimal hyperparameters. The best ones are the following:
	\begin{itemize}
		\item GRU Size: 250
		\item Dropout rate: 0.3
		\item MLP layers: 1
		\item MLP Units: 64
		\item MLP hidden layer size: 256
		\item Learning Rate: 0.01
	\end{itemize}
	
	
	
	Finally, we provide the classification report for training, development and test subsets in Tables \ref{tab::ex-1-report-train} , \ref{tab::ex-1-report-dev}, \ref{tab::ex-1-report-test} and the AUC scores in table \ref{tab::ex-1-stats-auc} .
	
	\begin{table}
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{} & \cellcolor{blue!25}\textbf{Precision} &
			\cellcolor{blue!25}\textbf{Recall}  &
			\cellcolor{blue!25}\textbf{f1-score}  &
			\cellcolor{blue!25}\textbf{support}\\
			\hline
			neg & 0.87 & 0.95 & 0.91 & 690 \\\hline
			pos & 0.94  & 0.86 & 0.9 & 710 \\\hline
			accuracy & & & \textbf{0.9} & 1400 \\\hline
			macro avg & 0.91 & 0.9 & 0.9 & 1400 \\\hline
			weighted avg & 0.91 & 0.9 & 0.9 & 1400 \\\hline
			
		\end{tabular}
		\centering
		\caption{Classification report on the training set (RNN classifier).}
		\label{tab::ex-1-report-train}
	\end{table}
	
	\begin{table}
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{} & \cellcolor{blue!25}\textbf{Precision} &
			\cellcolor{blue!25}\textbf{Recall}  &
			\cellcolor{blue!25}\textbf{f1-score}  &
			\cellcolor{blue!25}\textbf{support}\\
			\hline
			neg & 0.82 & 0.9 & 0.86 & 157 \\\hline
			pos & 0.88  & 0.79 & 0.83 & 143 \\\hline
			accuracy & & & \textbf{0.85} & 300 \\\hline
			macro avg & 0.85 & 0.84 & 0.85 & 300 \\\hline
			weighted avg & 0.85 & 0.85 & 0.85 & 300 \\\hline
			
		\end{tabular}
		\centering
		\caption{Classification report on the development set (RNN classifier).}
		\label{tab::ex-1-report-dev}
	\end{table}
	
	\begin{table}
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{} & \cellcolor{blue!25}\textbf{Precision} &
			\cellcolor{blue!25}\textbf{Recall}  &
			\cellcolor{blue!25}\textbf{f1-score}  &
			\cellcolor{blue!25}\textbf{support}\\
			\hline
			neg & 0.82 & 0.85 & 0.84 & 153 \\\hline
			pos & 0.84  & 0.81 & 0.82 & 147 \\\hline
			accuracy & & & \textbf{0.83} & 300 \\\hline
			macro avg & 0.83 & 0.83 & 0.83 & 300 \\\hline
			weighted avg & 0.83 & 0.83 & 0.83 & 300 \\\hline
			
		\end{tabular}
		\centering
		\caption{Classification report on the test set (RNN classifier).}
		\label{tab::ex-1-report-test}
	\end{table}
	
	\begin{table}
		\begin{tabular}{|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{Class} & \cellcolor{blue!25}\textbf{Training} & \cellcolor{blue!25}\textbf{Development} & \cellcolor{blue!25}\textbf{Test}\\
			\hline
			neg & 0.96419 & 0.91706 & 0.90574 \\\hline
			pos & 0.96408 & 0.91737 & 0.90565 \\\hline
			
		\end{tabular}
		\centering
		\caption{AUC stats for training, development and test sets (RNN classifier).}
		\label{tab::ex-1-stats-auc}
	\end{table}
	\ 
	
	
	
	\
	
	The MACRO AUC scores were found to be the following:
	\begin{itemize}
		\item Training set: 0.9641
		\item Development set: 0.9172
		\item Test set: 0.9057
	\end{itemize}

    \subsubsection{Our custom CNN classifier.}
    We start by defining functions for recall, precision, f1 and accuracy. These functions will be used as metrics when compiling our model. The model we create is a Sequential one, with:
    \begin{itemize}
            \item An input layer, which takes sequences of integers as input.
		\item An embedding layer, which converts input integers into fixed size dense vectors ('EMBEDDING\_DIM.' dimensional vectors).
		\item A dropout layer.
		\item Convolutional layers: They apply 1D convolution operation to the input sequence.
		\item Global Max Pooling layer, which performs downsampling by taking the max value over the time dimension.
            \item A dropout layer.
		\item A dense layer, with 'relu' activation function.
            \item A dense layer which has 2 units with sigmoid activation (since our problem is binary).
            \item The compilation part, using Adam.
	\end{itemize}

 The hyperparameters we will use here are summarized in Table \ref{tab::ex-2-hyper}.
	
	\begin{table}
		\centering
		\begin{tabular}{|l|l|l|l|}
			\hline
			\rowcolor{blue!25}\textbf{Learning rate} & \cellcolor{blue!25}\textbf{Kernel size} & \cellcolor{blue!25}\textbf{Number of convolutional layers} & \cellcolor{blue!25}\textbf{Dropout probability} \\
			\hline
			0.001 & 1 & 1 & 0.2 \\
			\hline
			0.01 & 2 & 2 & 0.25 \\
			\hline
			0.1 & 3 & 3 & 0.3  \\
			\hline 
			& 4 & 4 & 0.35  \\
			\hline 
			& & 5 & 0.4 \\
			\hline 
			& & & 0.45 \\
			\hline 
			& & & 0.5 \\
			
			
			
			
			\hline
		\end{tabular}
		\caption{Hyperparameters tested in the development set (CNN classifier).}
		\label{tab::ex-2-hyper}
	\end{table}
	
	\ 
	
	
	\
	
	We utilize Keras Tuner in order to find the optimal hyperparameters. The best ones are the following:
	\begin{itemize}
		\item Kernel size: 1
		\item Dropout rate: 0.35
		\item Number of convolution layers: 1
		\item Learning Rate: 0.001
	\end{itemize}
 
 
 Finally, we provide the classification report for training, development and test subsets in Tables \ref{tab::ex-2-report-train} , \ref{tab::ex-2-report-dev}, \ref{tab::ex-2-report-test} and the AUC scores in table \ref{tab::ex-2-stats-auc} .
	
	\begin{table}
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{} & \cellcolor{blue!25}\textbf{Precision} &
			\cellcolor{blue!25}\textbf{Recall}  &
			\cellcolor{blue!25}\textbf{f1-score}  &
			\cellcolor{blue!25}\textbf{support}\\
			\hline
			neg & 0.92 & 0.95 & 0.93 & 1376 \\\hline
			pos & 0.95  & 0.92 & 0.93 & 1424 \\\hline
			accuracy & & & \textbf{0.93} & 2800 \\\hline
			macro avg & 0.93 & 0.93 & 0.93 & 2800 \\\hline
			weighted avg & 0.93 & 0.93 & 0.93 & 2800 \\\hline
			
		\end{tabular}
		\centering
		\caption{Classification report on the training set (CNN classifier).}
		\label{tab::ex-2-report-train}
	\end{table}
	
	\begin{table}
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{} & \cellcolor{blue!25}\textbf{Precision} &
			\cellcolor{blue!25}\textbf{Recall}  &
			\cellcolor{blue!25}\textbf{f1-score}  &
			\cellcolor{blue!25}\textbf{support}\\
			\hline
			neg & 0.84 & 0.87 & 0.86 & 305 \\\hline
			pos & 0.86  & 0.83 & 0.85 & 295 \\\hline
			accuracy & & & \textbf{0.85} & 600 \\\hline
			macro avg & 0.85 & 0.85 & 0.85 & 600 \\\hline
			weighted avg & 0.85 & 0.85 & 0.85 & 600 \\\hline
			
		\end{tabular}
		\centering
		\caption{Classification report on the development set (CNN classifier).}
		\label{tab::ex-2-report-dev}
	\end{table}
	
	\begin{table}
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{} & \cellcolor{blue!25}\textbf{Precision} &
			\cellcolor{blue!25}\textbf{Recall}  &
			\cellcolor{blue!25}\textbf{f1-score}  &
			\cellcolor{blue!25}\textbf{support}\\
			\hline
			neg & 0.86 & 0.87 & 0.86 & 319 \\\hline
			pos & 0.85  & 0.84 & 0.84 & 281 \\\hline
			accuracy & & & \textbf{0.85} & 600 \\\hline
			macro avg & 0.85 & 0.85 & 0.85 & 600 \\\hline
			weighted avg & 0.85 & 0.85 & 0.85 & 600 \\\hline
			
		\end{tabular}
		\centering
		\caption{Classification report on the test set (CNN classifier).}
		\label{tab::ex-2-report-test}
	\end{table}
	
	\begin{table}
		\begin{tabular}{|l|l|l|l|}
			\hline
			\cellcolor{blue!25}\textbf{Class} & \cellcolor{blue!25}\textbf{Training} & \cellcolor{blue!25}\textbf{Development} & \cellcolor{blue!25}\textbf{Test}\\
			\hline
			neg & 0.9822 & 0.9182 & 0.9188 \\\hline
			pos & 0.9827 & 0.9185 & 0.9195 \\\hline
			
		\end{tabular}
		\centering
		\caption{AUC stats for training, development and test sets (CNN classifier).}
		\label{tab::ex-2-stats-auc}
	\end{table}
	\
	
	The MACRO AUC scores were found to be the following:
	\begin{itemize}
		\item Training set: 0.9824
		\item Development set: 0.9183
		\item Test set: 0.9190
	\end{itemize}

        \subsubsection{Our custom DistilBERT}
        
        Here we will be using a pre-trained DistilBERT model. At first, we will be creating the reviews dataset using the \texttt{datasets} library. Taking the DistilBERT base uncased model, we perform Byter Pair Encoding Tokenization with a pre-trained tokenizer from the \texttt{transformers} library and get tokenized sequences that have the maximum allowed length by the tokenizer. After that, we initialize data\_collator, which is used later during training to collate batches of tokenized data, padding sequences as necessary to ensure uniform length within each batch. We define the validation \textbf{f1} as our metric and perform hyperparameter search to get our best model. 
        \ 
        
        \
 
    The hyperparameters we will use here are:
    \begin{itemize}
        \item Learning Rate: Sample in (1e-5, 1e-3), uniform distribution.
        \item Number of training epochs: Sample in (1,3), uniform distribution.
        \item Per device train batch size: 8, 16, 32.
        
    \end{itemize}

    \ 
        
    \
    
    Our best model consists of the following hyperparameters:
    \begin{itemize}
        \item Learning Rate: 1.82e-05
        \item Number of training epochs: 3
        \item Train batch size per device: 8
        
    \end{itemize}

    This model achieves an F1 score of \textbf{0.889}, outperforming both CNN and RNN classifiers.
    \

    \

    Finally, we present some experimental results by prompting an LLM (ChatGPT in our case). The process was the following: We randomly selected 20 examples from our dataset, and we took their labels. We used the first 10 as demonstrators to instruct ChatGPT (few-shot learning) and the rest 10 are requested for classification by the language model. It turns out that ChatGPT predicts correct 8 out of 10 examples, with an F1 score of 0.83. You can check the full prompt \href{https://chat.openai.com/share/dba55d12-a18b-4ffa-9fc9-87c46a313ebc}{here}.
	
	\printbibliography
	
\end{document}
